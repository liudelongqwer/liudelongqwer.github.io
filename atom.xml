<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>拒绝再玩</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-29T06:40:40.930Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>duoyu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>K8s-监控-kube-prometheus</title>
    <link href="http://yoursite.com/2020/04/29/K8s-%E7%9B%91%E6%8E%A7-kube-prometheus/"/>
    <id>http://yoursite.com/2020/04/29/K8s-%E7%9B%91%E6%8E%A7-kube-prometheus/</id>
    <published>2020-04-29T06:36:32.000Z</published>
    <updated>2020-04-29T06:40:40.930Z</updated>
    
    <content type="html"><![CDATA[<p>prometheus 的监控项目 kube-prometheus</p> <a id="more"></a> <h1 id="kube-prometheus"><a href="#kube-prometheus" class="headerlink" title="kube-prometheus"></a>kube-prometheus</h1><p>基于最新<a href="https://github.com/coreos/kube-prometheus开发，支持prometheus" target="_blank" rel="noopener">https://github.com/coreos/kube-prometheus开发，支持prometheus</a> 2.15.2版本，支持kubeadm方式安装的k8s，二进制方式安装的未测试</p><h2 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h2><p>1.支持数据持久化</p><p>2.支持kube-controller监控</p><p>3.支持kube-scheduse监控</p><p>4.支持kube-etcd监控</p><p>5.支持NodePort访问</p><p>6.支持ingress访问</p><p>7.支持离线安装</p><p>8.支持重复安装</p><p>9.支持一键卸载</p><h2 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/chinaboy007/kube-prometheus.git</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> kube-prometheus/manifests</span><br><span class="line"></span><br><span class="line">sh install.sh</span><br></pre></td></tr></table></figure><h2 id="默认启用nfs-client数据持久化"><a href="#默认启用nfs-client数据持久化" class="headerlink" title="默认启用nfs-client数据持久化"></a>默认启用nfs-client数据持久化</h2><h2 id="一键卸载"><a href="#一键卸载" class="headerlink" title="一键卸载"></a>一键卸载</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd kube-prometheus&#x2F;manifests</span><br><span class="line">sh uninstall.sh</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;prometheus 的监控项目 kube-prometheus&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>配置Kubelet的垃圾回收</title>
    <link href="http://yoursite.com/2020/04/29/%E9%85%8D%E7%BD%AEKubelet%E7%9A%84%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
    <id>http://yoursite.com/2020/04/29/%E9%85%8D%E7%BD%AEKubelet%E7%9A%84%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/</id>
    <published>2020-04-29T06:32:22.000Z</published>
    <updated>2020-04-29T06:37:14.878Z</updated>
    
    <content type="html"><![CDATA[<p> Kubelet的垃圾回收功能</p><a id="more"></a> <h1 id="配置Kubelet的垃圾回收"><a href="#配置Kubelet的垃圾回收" class="headerlink" title="配置Kubelet的垃圾回收"></a>配置Kubelet的垃圾回收</h1><p>Kubelet的垃圾回收功能可以清理不再使用的容器和镜像，kubelet对容器进行垃圾回收的频率是每分钟一次，对镜像进行垃圾回收的频率是每五分钟一次。</p><p>不推荐使用外部的垃圾回收工具，因为这些工具有可能会删除 kubelet 仍然需要的容器或者镜像。</p><h2 id="镜像回收"><a href="#镜像回收" class="headerlink" title="镜像回收"></a>镜像回收</h2><p>Kubernetes 通过 imageManager 配合 cadvisor 管理所有镜像的生命周期。</p><p>镜像的垃圾回收策略主要考虑两方面因素： <code>HighThresholdPercent</code> 和 <code>LowThresholdPercent</code>。</p><ul><li>磁盘利用率超过 <code>high threshold</code> 将触发垃圾回收动作</li><li>垃圾回收功能将删除最近最少使用的镜像，直到磁盘利用率低于 <code>low threshold</code></li></ul><h2 id="容器回收"><a href="#容器回收" class="headerlink" title="容器回收"></a>容器回收</h2><p>容器的垃圾回收侧率主要考虑三个用户自定义的变量：</p><ul><li><code>MinAge</code>： 容器创建到现在的最小时长，低于此时长的不能被垃圾回收；如果设置为 0，则禁用该选项</li><li><code>MaxPerPodContainer</code>：以 <code>Pod UID</code> + <code>容器名</code> 作为组合键，<code>MaxPerPodContainer</code> 指定了同一个 <code>Pod UID</code> + <code>容器名</code> 组合键下可以包含的已停止容器的最大数量。如果设置为小于 0 的数值，则禁用该选项</li><li><code>MaxContainers</code>： 指定了最大的已停止容器的数量。如果设置为小于 0 的数值，则禁用该选项</li></ul><p>Kubelet 将对满足上述三个条件，且已经停止的容器执行垃圾回收的动作。通常，创建时间最长的容器将被最早移除。 <code>MaxPerPodContainer</code> 和 <code>MaxContainer</code> 这两个参数可能会相互冲突，例如， 如果要为每个 Pod 保存 <code>MaxPerPodContainer</code> 个已停止容器的话，可能最终总的已停止的容器的数量要超过 <code>MaxContainers</code> 的定义。 此时，优先保证 <code>MaxContainers</code> 的限定， <code>MaxPerPodContainer</code> 将被重新调整：最坏的情况下，kubelet 将 <code>MaxPerPodContainer</code> 的要求降低到 1，并删除创建时间最久的已停止的容器。此外，当 Pod 的已停止容器创建时长超过 <code>MinAge</code> 时，该容器将被即刻删除。</p><p>对于那些不是通过 kubelet 创建的容器，kubelet 不能对其进行垃圾回收操作。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>通过以下 kubelet 启动参数，可以调整镜像垃圾回收的变量：</p><ul><li><code>image-gc-high-threshold</code>，磁盘利用率高于此参数时，将触发镜像的垃圾回收。默认值为 85%</li><li><code>iamge-gc-low-threshold</code>，磁盘利用率低于此参数时，镜像的垃圾回收将停止。默认值为 80%</li></ul><p>通过以下 kubelet 启动参数，可以调整容器的垃圾回收的变量：</p><ul><li><code>minimum-container-ttl-duration</code>，容器创建到现在的最小时长，低于此时长的不能被垃圾回收。默认值为 0 分钟，即，每一个已停止的容器都可以被垃圾回收</li><li><code>maximum-dead-containers-per-container</code>，对于每个容器的旧实例，最多可以保留的个数。默认值为 1</li><li><code>maximum-dead-containers</code>，全局最大可以保留的已停止的容器数量。默认值为 -1，即，不做全局性限制</li></ul><p>容器在被垃圾回收时，也许仍然是有用的。例如，这些容器可能包含了对于问题诊断（trouble shooting）来说非常有用的日志和数据。强烈建议将 <code>maximum-dead-containers-per-container</code> 设置为足够大的数值，至少不能小于1，以便为每一个容器至少保留一个已停止的容器。同样的，也建议为 <code>maximum-dead-containers</code> 设置一个比较大的数值。</p><h2 id="Deprecation"><a href="#Deprecation" class="headerlink" title="Deprecation"></a>Deprecation</h2><p>此文档的某些特性已经不推荐使用，未来将被 kubelet eviction 替代，包括：</p><table><thead><tr><th>Existing Flag</th><th>New Flag</th><th>Rationale</th></tr></thead><tbody><tr><td><code>--image-gc-high-threshold</code></td><td><code>--eviction-hard</code> or <code>--eviction-soft</code></td><td>已有的 eviction 信号可以触发镜像的垃圾回收</td></tr><tr><td><code>--image-gc-low-threshold</code></td><td><code>--eviction-minimum-reclaim</code></td><td>eviction reclaims 可实现相同的效果</td></tr><tr><td><code>--maximum-dead-containers</code></td><td></td><td>如果日志被存储在容器外部，就不推荐使用此特性</td></tr><tr><td><code>--maximum-dead-containers-per-container</code></td><td></td><td>如果日志被存储在容器外部，就不推荐使用此特性</td></tr><tr><td><code>--minimum-container-ttl-duration</code></td><td></td><td>如果日志被存储在容器外部，就不推荐使用此特性</td></tr><tr><td><code>--low-diskspace-threshold-mb</code></td><td><code>--eviction-hard</code> or <code>eviction-soft</code></td><td>eviction 通过其他资源判断是否要垃圾回收，而不再通过磁盘利用率这个参数</td></tr><tr><td><code>--outofdisk-transition-frequency</code></td><td><code>--eviction-pressure-transition-period</code></td><td>eviction generalizes disk pressure transition to other resources</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Kubelet的垃圾回收功能&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-调度问题</title>
    <link href="http://yoursite.com/2020/04/29/K8s-%E8%B0%83%E5%BA%A6%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/04/29/K8s-%E8%B0%83%E5%BA%A6%E9%97%AE%E9%A2%98/</id>
    <published>2020-04-29T06:27:28.000Z</published>
    <updated>2020-04-29T06:30:58.985Z</updated>
    
    <content type="html"><![CDATA[<p> Kubernetes的调度和资源限制问题</p><a id="more"></a> <h1 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h1><p>在Kubernetes中，调度（Scheduling），指的是为 Pod 找到一个合适的节点，并由该节点上的 kubelet 运行 Pod。</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>每当集群中有新的 Pod 创建时，Kubernetes 调度器将负责为其找到最合适的节点去运行。调度器按照本文后面描述的原则执行执行调度工作。如果您想了解为什么 Pod 被分配到了具体的某一个节点，或者您打算自己实现一个定制化的调度器，本文可以帮助您更好的理解 Kubernetes 的调度工作</p><h2 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h2><p>kube-scheduler是 Kubernetes 中默认的调度器，并且运行在 Master 组件中。</p><p>kube-scheduler 虽然是默认的调度器，但是，在需要的时候，可以自定义调度器以替代 kube-scheduler。</p><p>对于每一个新创建的或者未调度的 Pod，kube-scheduler 为其选择一个合适的节点去运行。问题是，每一个 Pod 以及其中的每一个容器，都有不同的资源需求，在调度时，必须选择那些能够满足 Pod 的资源需求的节点才可以。</p><p>集群中能够满足某一个 Pod 的资源需求的节点，称为 <strong><em>可选节点</em></strong>（feasible node）。如果某一个 Pod 没有合适的 <strong><em>可选节点</em></strong>，则该 Pod 将一直停留在 <code>Pending</code> 状态，直到集群中出现了对于该 Pod 来说合适的 <strong><em>可选节点</em></strong>。</p><p>调度器在执行调度时，执行的步骤如下：</p><ol><li>找出该 Pod 的所有 <strong><em>可选节点</em></strong></li><li>按照某种方式对每一个 <strong><em>可选节点</em></strong> 评分</li><li>选择评分最高的 <strong><em>可选节点</em></strong></li><li>将最终选择结果通知 API Server，<code>这个过程，称为绑定（binding）</code></li></ol><p>在为 <strong><em>可选节点</em></strong> 评分时，需要考虑的因素有：</p><ul><li>单个 Pod 和所有 Pod 的资源需求</li><li>硬件、软件、策略（Policy，例如Limit Range、Resource Quota等）</li><li>亲和与反亲和（affinity and anti-affinity）</li><li>数据存储的位置</li><li>工作负载之间的相互影响，等。</li></ul><h2 id="使用kube-scheduler调度"><a href="#使用kube-scheduler调度" class="headerlink" title="使用kube-scheduler调度"></a>使用kube-scheduler调度</h2><p>kube-schduler在执行调度时，将上述过程分成两个阶段来执行：</p><ol><li>Filtering （筛选/过滤）</li><li>Scoring （评分）</li></ol><p>Filtering（筛选/过滤）阶段，kube-scheduler找出所有对待调度的 Pod 来说合适的 <strong><em>可选节点</em></strong>。例如，<code>PodFitsResources</code> 过滤器检查候选节点是否具备足够的资源可以满足 Pod 的资源需求。在筛选阶段结束后，通常可以找出多个 <strong><em>可选节点</em></strong>，如果没有找到，则 Pod 一直停留在 <code>Pending</code> 状态。</p><p>Scoring（评分）阶段，kube-scheduler 先按照当前可用的评分规则为每一个 <strong><em>可选节点</em></strong> 频分， 然后，按评分结果对所有的 <strong><em>可选节点</em></strong> 排序，以找出最适合 Pod 运行的节点。</p><p>最后，kube-scheduler 将 Pod 分配到评分最高的 <strong><em>可选节点</em></strong>。如果有多个节点评分一样且最高，kube-scheduler 将随机从中选择一个节点。</p><h3 id="Filtering"><a href="#Filtering" class="headerlink" title="Filtering"></a>Filtering</h3><p>Filtering（筛选/过滤）阶段，使用的过滤器有：</p><ul><li><strong>PodFitsHostPorts</strong>: 检查Pod需要的 <code>hostPort</code> 在该节点上是否可用</li><li><strong>PodFitsHost</strong>：检查 Pod 是否通过 hostname 指定了节点，</li><li><strong>PodFitsResource</strong>：检查节点是否满足 Pod 的资源需求（例如，CPU 和 Memory）</li><li><strong>PodMatchNodeSelector</strong>：检查 Pod 的节点选择器（nodeSelector）是否和节点的标签匹配</li><li><strong>NoVolumeZoneConflict</strong>：评估 Pod 所需要的 数据卷是否在节点上可用（数据卷的 failure zone restrictions）</li><li><strong>NoDiskConflict</strong>：评估Pod请求的数据卷是否和节点已经加载的数据卷冲突</li><li><strong>MaxCSIVolumeCount</strong>：计算节点可以挂载多少个 CSI（Container Storage Interface）数据卷，确保不会超出限定的数字</li><li><strong>CheckNodeMemoryPressure</strong>：检查节点是否有内存紧张的情况</li><li><strong>CheckNodePIDPressure</strong>：检查节点是否有 PID 短缺的情况</li><li><strong>CheckNodeDiskPressure</strong>：检查节点是否有存储空间吃紧的情况（文件系统已满，或者将要满）</li><li><strong>CheckNodeCondition</strong>：检查节点的 Condition 字段，该字段中包含关于 <code>文件系统已满</code>、<code>网络不可用</code>、<code>kubelet未就绪</code> 等相关的条件</li><li><strong>PodToleratesNodeTaints</strong>：检查 Pod 是否容忍 Pod 的污点</li><li><strong>CheckVolumeBinding</strong>：检查存储卷声明是否可绑定</li></ul><h3 id="Scoring"><a href="#Scoring" class="headerlink" title="Scoring"></a>Scoring</h3><ul><li><strong>SelectorSpreadPriority</strong>：将 Pod 分散到不同的节点，主要考虑同属于一个 Service、StatefulSet、Deployment的情况</li><li><strong>InterPodAffinityPriority</strong>：遍历 <code>weightedPodAffinityTerm</code> 并求和，找出结果最高的节点</li><li><strong>LeastRequestedPriority</strong>：已被消耗的资源最少的节点得分最高。如果节点上的 Pod 越多，被消耗的资源越多，则评分约低</li><li><strong>MostRequestedPriority</strong>：已被消耗的资源最多的节点得分最高。此策略会把 Pod 尽量集中到集群中的少数节点上</li><li><strong>RequestedToCapacityRatioPriority</strong>：按 requested / capacity 的百分比评分</li><li><strong>BalancedResourceAllocation</strong>：资源使用均衡的节点评分高</li><li><strong>NodePreferAvoidPodsPriority</strong>：根据节点的 annotation <code>scheduler.alpha.kubernetes.io/preferAvoidPods</code> 评分。可使用此 annotation 标识哪些 Pod 不能够运行在同一个节点上</li><li><strong>NodeAffinityPriority</strong>：基于 <code>PreferredDuringSchedulingIgnoredDuringExecution</code> 指定的 node affinity 偏好评分。</li><li><strong>TaintTolerationPriority</strong>： 根据节点上不可容忍的污点数评分</li><li><strong>ImageLocalityPriority</strong>：有限选择已经有该 Pod 所需容器镜像的节点</li><li><strong>ServiceSpreadingPriority</strong>：确保 Service 的所有 Pod 尽量分布在不同的节点上。</li><li><strong>CalculateAntiAffinityPriorityMap</strong>：anti-affinty</li><li><strong>EqualPriorityMap</strong>：为每个节点指定相同的权重</li></ul><h1 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h1><p>默认情况下，容器在 Kubernetes 集群上运行时，不受计算资源的限制。使用 Resource quota，集群管理员可以针对名称空间限定资源的使用情况。在名称空间内部，一个 Pod（或容器）的资源消耗不受限制。此时的顾虑在于，可能有一个 Pod（或容器）独占了名称空间的大部分资源。Limit Range 是一种用来限定名称空间内 Pod（或容器）可以消耗资源数量的策略（Policy）。 </p><p>Kubernetes <code>LimitRange</code> 对象可以：</p><ul><li>限制名称空间中每个 Pod 或容器的最小最大计算资源</li><li>限制名称空间中每个 PersistentVolumeClaim 可使用的最小最大存储空间</li><li>限制名称空间中计算资源请求request、限定limit之间的比例</li><li>设置名称空间中默认的计算资源的 request/limit，并在运行时自动注入到容器中</li></ul><h2 id="启用-Limit-Range"><a href="#启用-Limit-Range" class="headerlink" title="启用 Limit Range"></a>启用 Limit Range</h2><p>执行命令 <code>kubectl api-resources</code> 可查看集群是否支持 Limit Range（ 通常 LimitRange 是默认启用的 ），输出结果如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0426]<span class="comment"># kubectl api-resources | grep limit</span></span><br><span class="line">limitranges                       limits                                      <span class="literal">true</span>         LimitRange</span><br></pre></td></tr></table></figure><h2 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h2><ul><li>集群管理员在名称空间中创建一个 <code>LimitRange</code> 对象</li><li>用户在名称空间中创建工作负载等对象，例如 Pod、Container、PersistentVolumeClaim 等</li><li>针对那些没有设置计算资源请求 request 和限制 limit 的 Pod 和容器，<code>LimitRanger</code> 根据名称空间中的 <code>LimitRange</code> 对象为其设定默认的资源请求和响应，并确保 Pod 和容器对计算资源的实际消耗不会超过指定的值</li><li>如果创建或更新对象（Pod、Container、PersistentVolumeClaim）的请求与 Limit Range 的限定相冲突，apiserver 将返回 HTTP status 状态码 <code>403 FORBIDDEN</code>，以及相应的错误提示信息</li><li>如果名称空间中激活了 limit range 来限定 cpu 和内存等计算资源的使用，则用户创建 Pod、Container 时，必须指定 cpu 或内存的 <code>request</code> 和 <code>limit</code>，否则系统将拒绝创建 Pod</li><li>Kubernetes 只在 Pod 创建阶段检查 <code>LimitRange</code> 的限定，而不在 Pod 运行时执行任何检查</li></ul><h3 id="使用-LimitRange-的例子有："><a href="#使用-LimitRange-的例子有：" class="headerlink" title="使用 LimitRange 的例子有："></a>使用 LimitRange 的例子有：</h3><ul><li>在一个总容量为 8G内存 16核CPU 的 2 节点集群上，限定某个名称空间中的 Pod 使用 100m的CPU请求（request）且不超过 500m的CPU上限（limit），200Mi的内存请求（request）且不超过 600Mi的内存上线（limit）</li><li>为没有定义cpu和内存请求的容器，指定默认的 CPU 请求（request）和限制（limit）均为 150m，默认的内存请求为 300Mi</li></ul><p>当名称空间总的 limit 小于名称空间中 Pod/Container 的 limit 之和时，将发生资源争夺的现象，容器或者 Pod 将不能创建。</p><p>在资源争夺现象发生时，或者修改 limitrange 的时候，这两种情况都不会影响到已经创建的 Pod/Container。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Kubernetes的调度和资源限制问题&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-日志</title>
    <link href="http://yoursite.com/2020/04/27/K8s-%E6%97%A5%E5%BF%97/"/>
    <id>http://yoursite.com/2020/04/27/K8s-%E6%97%A5%E5%BF%97/</id>
    <published>2020-04-27T01:46:56.000Z</published>
    <updated>2020-04-27T06:25:05.343Z</updated>
    
    <content type="html"><![CDATA[<p> Kubernetes 日志处理</p><a id="more"></a> <h1 id="基本的日志"><a href="#基本的日志" class="headerlink" title="基本的日志"></a>基本的日志</h1><p>使用 <code>kubectl logs</code> 查看 pod 日志</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f <span class="variable">$&#123;POD_NAME&#125;</span></span><br></pre></td></tr></table></figure><p> 如果容器已经崩溃停止，仍然可以使用 <code>kubectl logs --previous</code> 获取该容器的日志，只不过需要添加参数 <code>--previous</code>。 如果 Pod 中包含多个容器，而想要看其中某一个容器的日志，那么请在命令的最后增加容器名字作为参数。 </p><p> 常用的日志命令示例如下： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追踪名称空间 nsA 下容器组 pod1 的日志</span></span><br><span class="line">kubectl logs -f pod1 -n nsA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 追踪名称空间 nsA 下容器组 pod1 中容器 container1 的日志</span></span><br><span class="line">kubectl logs -f pod1 -c container1 -n nsA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器组 nginx 下所有容器的日志</span></span><br><span class="line">kubectl logs nginx --all-containers=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看带有 app=nginx 标签的所有容器组所有容器的日志</span></span><br><span class="line">kubectl logs -lapp=nginx --all-containers=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器组 nginx 最近20行日志</span></span><br><span class="line">kubectl logs --tail=20 nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器组 nginx 过去1个小时的日志</span></span><br><span class="line">kubectl logs --since=1h nginx</span><br></pre></td></tr></table></figure><h1 id="节点级别的日志"><a href="#节点级别的日志" class="headerlink" title="节点级别的日志"></a>节点级别的日志</h1><h2 id="日志存储"><a href="#日志存储" class="headerlink" title="日志存储"></a>日志存储</h2><p><img src="/2020/04/27/K8s-%E6%97%A5%E5%BF%97/1.png" alt></p><p>容器化应用程序写入到 <code>stdout</code> 和 <code>stderr</code> 中的任何信息，都将被容器引擎重定向到某个地方。例如，Docker 容器引擎将 <code>stdout</code> 和 <code>stderr</code> 这两个输出流重定向到 logging driver，Kubernetes的默认配置中，最终 logging driver 最终把日志写入了一个 json 格式的文件 </p><p> 默认配置下，如果容器重启了，kubelet 只为该容器保留最后一份日志。如果 Pod 从节点上驱逐，当中所有的容器也将一并被驱逐，连同他们的日志也将被删除。 </p><h2 id="日志轮转"><a href="#日志轮转" class="headerlink" title="日志轮转"></a>日志轮转</h2><p>在节点级别的日志里，一个非常重要的考量是实现 log rotation（日志轮转），这样的话，日志文件就不会耗尽节点上的存储空间。</p><p>Kubernetes当前并不负责对日志进行轮转，而是期望由 kubernetes 安装工具来配置一个合适的解决方案。例如，在使用 <code>kube-up.sh</code> 脚本安装的 kubernetes 集群中，配置了 logrotage 工具，该工具每小时执行一次日志轮转的操作。</p><p>也可以配置容器引擎，使其自动轮转应用程序的日志，例如，使用 Docker 的 <code>log-opt</code>。在 <code>kube-up.sh</code> 脚本中，后者被用在了 GCP 的 COS 镜像中，而前者则用在了任何其他环境。两种情况下，默认的配置是，日志文件达到 10MB 时进行轮转。</p><p>如果Kubernetes的外部系统执行了文件的轮转，只有最新日志文件中的内容可以通过 <code>kubectl logs</code> 命令返回。例如，如果假设日志文件刚刚达到 10MB， <code>logrotate</code> 执行了日志轮转，将所有日志挪到一个带时间戳的日志存档文件中，并清空了当前日志文件，此时 <code>kubectl logs</code> 将返回空的日志信息（因为当前日志文件为空） </p><h2 id="系统组件的日志"><a href="#系统组件的日志" class="headerlink" title="系统组件的日志"></a>系统组件的日志</h2><p>Kubernetes中存在两种类型的系统组件：</p><ul><li>运行在容器中的系统组件</li><li>不运行在容器中的系统组件</li></ul><p>例如：</p><ul><li>kubenetes scheduler 和 kube-proxy 运行在容器中</li><li>kubelet 和容器引擎（例如 docker）不运行在容器中</li></ul><p>在带有 <code>systemd</code> 的机器上，kubelet 和容器引擎将日志写入 Linux 系统的 <code>journald</code> 中。如果没有 <code>systemd</code>，kubelet 和容器引擎将日志写入目录 <code>/var/log</code> 中的 <code>.log</code> 文件。运行在容器中的系统组件则使用将日志写入目录 <code>/var/log</code> 中（绕过了默认的日志机制–即将日志写入<code>stdout</code> 和 <code>stderr</code>）。</p><p>与容器化应用程序的日志相似，记录在 <code>/var/log</code> 目录下的系统组件的日志也应该进行轮转。在使用 <code>kube-up.sh</code> 脚本安装的集群中，这些日志文件将由 <code>logrotate</code> 工具进行轮转，轮转的机制是：每天或者每当日志文件超过 100MB 时。</p><h1 id="集群级别的日志"><a href="#集群级别的日志" class="headerlink" title="集群级别的日志"></a>集群级别的日志</h1><p>Kubernetes 中并不默认提供集群级别的日志，不过，有许多种途径可以和集群级别的日志整合。例如：</p><ul><li>在每个节点上配置日志代理</li><li>在应用程序的 Pod 中包含一个专门用于收集日志的 sidecar 容器</li><li>从应用程序中直接推送日志到日志存储端</li></ul><h2 id="在节点上配置日志代理"><a href="#在节点上配置日志代理" class="headerlink" title="在节点上配置日志代理"></a>在节点上配置日志代理</h2><p><img src="/2020/04/27/K8s-%E6%97%A5%E5%BF%97/2.png" alt></p><p>如上图所示，通过在每个节点上配置一个节点级别的 logging-agent，就可以实现集群级别的日志（cluster-level logging）。该 logging-agent 专门用来将节点上的日志文件中的日志推送到日志的后端存储。所有应用程序的标准输出都被使用 docker 的 logging driver存储到了节点的 <code>/var/log/containers</code> 目录下（K8S 的默认配置）。因此，节点级别的 logging-agent 应该能够将该目录下的日志文件发送到日志后端。<a href="https://kuboard.cn/learning/k8s-advanced/logs/node.html#系统组件的日志" target="_blank" rel="noopener">系统组件的日志</a> 可以考虑一下是否要推送到日志后端。</p><p>logging-agent 必须运行在每一个节点上，推荐使用 DaemonSet </p><p>对于 Kubernetes 集群来说，使用节点级别的 logging-agent 实现集群级别的日志（cluster-level logging）是使用最广泛也最为推荐的一种做法，因为这种做法只为每个节点创建了一个 logging-agent，且无需对节点上的容器化应用程序做任何修改。</p><p>当然，只有当应用程序使用 stdout 和 stderr 记录日志时，节点级别的 logging-agent 才能生效。Kubernetes 默认安装并不指定 logging-agent，但是Kubernetes默认打包了两种 logging-agent 以供选择：</p><ul><li>Stackdriver Logging配合 Google Cloud Platform</li><li><a href="https://kubernetes.io/docs/user-guide/logging/elasticsearch" target="_blank" rel="noopener">Elasticsearch</a> </li></ul><h2 id="在-sidecar-容器中配置-logging-agent"><a href="#在-sidecar-容器中配置-logging-agent" class="headerlink" title="在 sidecar 容器中配置 logging-agent"></a>在 sidecar 容器中配置 logging-agent</h2><p>可以按照以下方式使用 sidecar 容器，以收集日志：</p><ul><li>sidecar 容器跟踪应用程序的日志文件，并输出到 sidecar 容器自己的 stdout</li><li>sidecar 容器运行一个 logging-agent，追踪应用程序的日志文件，并发送到日志后端</li></ul><h3 id="sidecar输出到stdout"><a href="#sidecar输出到stdout" class="headerlink" title="sidecar输出到stdout"></a>sidecar输出到stdout</h3><p><img src="/2020/04/27/K8s-%E6%97%A5%E5%BF%97/3.png" alt></p><p>使用 sidecar 容器将应用程序的日志输出到 sidecar 容器自己的 <code>stdout</code> 和 <code>stderr</code> 之后，可以直接利用已经在集群上运行的 kubelet 以及节点级别的 logging-agent 进一步将日志发送到日志后端。此时 sidecar 容器可以从日志文件读取、socket读取或者 journald 读取日志内容，其输出则是 sidecar 容器自己的 <code>stdout</code> 和 <code>stderr</code>。</p><p>使用这种方法：</p><ul><li>只需要对那些不能将日志输出到 <code>stdout</code> 和 <code>stderr</code> 的应用程序进行特殊配置即可，那些已经可以将日志输出到 <code>stdout</code> 和 <code>stderr</code> 的应用程序，则可以直接利用节点级别的 logging-agent。</li><li>由于重定向日志的逻辑非常简单，这种做法所带来的系统开销也是非常小的。</li><li>此外，<code>stdout</code> 和 <code>stderr</code> 由 kubelet 处理，还可以直接使用支持 apiserver 的 kubernetes 管理工具来查看日志，例如 <code>kubectl logs</code>或者Kubernetes Dashboard等</li></ul><h2 id="直接从应用程序容器发送日志到后端"><a href="#直接从应用程序容器发送日志到后端" class="headerlink" title="直接从应用程序容器发送日志到后端"></a>直接从应用程序容器发送日志到后端</h2><p><img src="/2020/04/27/K8s-%E6%97%A5%E5%BF%97/4.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Kubernetes 日志处理&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>Internet如何访问K8s集群</title>
    <link href="http://yoursite.com/2020/04/26/Internet%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AEK8s%E9%9B%86%E7%BE%A4/"/>
    <id>http://yoursite.com/2020/04/26/Internet%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AEK8s%E9%9B%86%E7%BE%A4/</id>
    <published>2020-04-26T08:47:15.000Z</published>
    <updated>2020-04-27T01:44:58.280Z</updated>
    
    <content type="html"><![CDATA[<p>问题诊断-Deployment故障排除图解（待续…）</p> <a id="more"></a> <p>在Kubernetes中部署应用程序时，通常会定义三个组件：</p><ul><li><p>一个<strong>Deployment</strong> - 这是一份用于创建你的应用程序的Pod副本的”食谱”；</p></li><li><p>一个<strong>Service</strong> - 一个内部负载均衡器，用于将流量路由到内部的Pod上；</p></li><li><p>一个<strong>Ingress</strong> - 描述如何流量应该如何从集群外部流入到集群内部的你的服务上。</p><p><img src="https://kuboard.cn/statics/learning/ts/deployment/aa7dc4e26be246133054a6603aa07a77.svg" alt="img"></p></li></ul><ol><li>在Kubernetes中，应用程序通过两层负载均衡器暴露服务：内部的和外部的。</li><li>内部的负载均衡器称为Service，而外部的负载均衡器称为Ingress。</li><li>Pod不会直接部署。Deployment会负责创建Pod并管理它们。</li></ol><h2 id="一-连接Deployment和Service"><a href="#一-连接Deployment和Service" class="headerlink" title="一. 连接Deployment和Service"></a>一. 连接Deployment和Service</h2><p>其实，Service和Deployment之间根本没有连接。事实是：Service直接指向Pod，并完全跳过了Deployment。因此，应该注意的是Pod和Service之间的相互关系。</p><ul><li>Service selector应至少与Pod的一个标签匹配；</li><li>Service的<strong>targetPort</strong>应与Pod中容器的<strong>containerPort</strong>匹配；</li><li>Service的<strong>port</strong>可以是任何数字。多个Service可以使用同一端口号，因为它们被分配了不同的IP地址；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;问题诊断-Deployment故障排除图解（待续…）&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-应用诊断</title>
    <link href="http://yoursite.com/2020/04/26/K8s-%E5%BA%94%E7%94%A8%E8%AF%8A%E6%96%AD/"/>
    <id>http://yoursite.com/2020/04/26/K8s-%E5%BA%94%E7%94%A8%E8%AF%8A%E6%96%AD/</id>
    <published>2020-04-26T06:52:37.000Z</published>
    <updated>2020-04-26T08:13:18.984Z</updated>
    
    <content type="html"><![CDATA[<p> Kubernetes 出现问题排查</p><a id="more"></a> <h1 id="诊断应用程序"><a href="#诊断应用程序" class="headerlink" title="诊断应用程序"></a>诊断应用程序</h1><h2 id="Debugging-Pods"><a href="#Debugging-Pods" class="headerlink" title="Debugging Pods"></a>Debugging Pods</h2><p>查看 Pod 的完整描述 </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pods <span class="variable">$&#123;POD_NAME&#125;</span></span><br></pre></td></tr></table></figure><p>注意观察 <code>State</code> <code>Restart</code> <code>Count</code> <code>Conditions</code> <code>Events</code> 字段</p><h3 id="Pod一直是Pending"><a href="#Pod一直是Pending" class="headerlink" title="Pod一直是Pending"></a>Pod一直是Pending</h3><p>如果 Pod 一直停留在 <code>Pending</code>，意味着该 Pod 不能被调度到某一个节点上。通常，这是因为集群中缺乏足够的资源或者 <strong>合适</strong> 的资源。在上述 <code>kubectl describe...</code> 命令的输出中的 <code>Events</code> 字段，会有对应的事件描述为什么 Pod 不能调度到节点上。可能的原因有：</p><ul><li><p>资源不就绪：创建 Pod 时，有时候需要依赖于集群中的其他对象， ConfigMap（配置字典）、PVC（存储卷声明）等，例如</p><ul><li><p>可能该 Pod 需要的存储卷声明尚未与存储卷绑定，Events 信息如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Type     Reason            Age        From               Message</span><br><span class="line">----     ------            ----       ----               -------</span><br><span class="line">Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  pod has unbound immediate PersistentVolumeClaims (repated 2 <span class="built_in">times</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>缺乏足够的资源：可能集群中的CPU或内存都已经耗尽，此时，可以尝试：</p><ul><li>删除某些 Pod</li><li>调整Pod的资源请求</li><li>向集群中添加新的节点</li></ul></li><li><p><strong>该Pod使用<code>hostPort</code></strong>： 当Pod使用 <code>hostPort</code> 时，该Pod可以调度的地方就比较有限了。大多数情况下，是不需要使用 <code>hostPort</code> 的，可以尝试使用 Service 访问 Pod。如果您确实需要使用 <code>hostPort</code> 时，Deployment/ReplicationController 中 replicas 副本数不能超过集群中的节点数，因为每台机器的 80 端口只有一个，任何其他端口也只有一个。如果该端口被其他程序占用了，也将导致Pod调度不成功</p></li><li><p><strong>污点和容忍</strong>： 当在Pod的事件中看到 <code>Taints</code> 或 <code>Tolerations</code> 这两个单词时，可以检查Pod是否存在污点或者容忍</p></li></ul><h3 id="Pod一直是Wating"><a href="#Pod一直是Wating" class="headerlink" title="Pod一直是Wating"></a>Pod一直是Wating</h3><p>如果 Pod 停留在 <code>Waiting</code> 状态，此时该 Pod 已经被调度到某个节点上了，但是却不能运行。</p><p>注意 <code>Events</code> 字段的内容。最常见的 Pod 停留在 <code>Waiting</code> 状态的原因是抓取容器镜像失败。请检查：</p><ul><li>容器镜像的名字是对的</li><li>容器镜像已经推送到了镜像仓库中</li><li>在对应的节点上手工执行 <code>docker pull</code> 命令，看是否能够抓取成功。</li></ul><h3 id="Pod已经Crash或者Unhealthy"><a href="#Pod已经Crash或者Unhealthy" class="headerlink" title="Pod已经Crash或者Unhealthy"></a>Pod已经Crash或者Unhealthy</h3><p> 此时通常是容器中应用程序的问题，检查容器的日志，以诊断容器中应用程序出现了何种故障： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs <span class="variable">$&#123;POD_NAME&#125;</span> <span class="variable">$&#123;CONTAINER_NAME&#125;</span></span><br></pre></td></tr></table></figure><p> 如果容器之前 crash，通过上述命令查不到日志，可以尝试使用下面的命令查看上一次 crash 时的日志： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs --previous <span class="variable">$&#123;POD_NAME&#125;</span> <span class="variable">$&#123;CONTAINER_NAME&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Pod处于Running状态，但是不工作"><a href="#Pod处于Running状态，但是不工作" class="headerlink" title="Pod处于Running状态，但是不工作"></a>Pod处于Running状态，但是不工作</h3><p>Pod已经处于Running状态了，但是不像期望的那样工作，此时，很有可能是部署描述yaml文件（例如 Pod、Deployment、StatefulSet等）出现了问题，而创建时，kubectl 忽略了该错误。</p><p>例如环境变量中某一个 Key 写错了，<code>command</code> 拼写成了 <code>commnd</code> 等。如果 <code>command</code> 拼写成了 <code>commnd</code>，仍然能够使用该 yaml 文件创建工作负载，但是容器在运行时，却不会使用原本期望的命令，而是执行了镜像中的 <code>EntryPoint</code>。</p><ul><li>首先，在使用 <code>kubectl apply -f</code> 命令之前，可以尝试为其添加 <code>--validate</code> 选项，例如， <code>kubectl apply --validate -f mypod.yaml</code>。如果将 <code>command</code> 拼写成 <code>commnd</code>，将看到如下错误信息： </li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl apply --validate -f  security-context-1.yaml </span></span><br><span class="line">error: error validating <span class="string">"security-context-1.yaml"</span>: error validating data: ValidationError(Pod.spec.containers[0]): unknown field <span class="string">"commnd"</span> <span class="keyword">in</span> io.k8s.api.core.v1.Container; <span class="keyword">if</span> you choose to ignore these errors, turn validation off with --validate=<span class="literal">false</span></span><br></pre></td></tr></table></figure><ul><li>其次，请检查已经创建的 Pod 和预期的是一致的。执行命令 <code>kubectl get pods/mypod -o yaml &gt; mypod-on-apiserver.yaml</code>。将输出结果与创建 Pod 时所使用的文件做一个对比。通常通过此命令从服务器端获取到的信息比创建 Pod 时所使用的文件要多几行，这是正常的。然而，如果创建的Pod时所示用的文件中，存在从服务器上获取的信息中没有的代码行，这可能就是问题所在了。 </li></ul><h2 id="Debugging-Deployment"><a href="#Debugging-Deployment" class="headerlink" title="Debugging Deployment"></a>Debugging Deployment</h2><p>Deployment（或者 DaemonSet/StatefulSet/Job等），都会比较直接，要么可以创建 Pod，要么不可以。</p><p>可以通过 <code>kubectl describe deployment ${DEPLOYMENT_NAME}</code> （或者statefulset / job 等）命令查看与 Deployment 相关的事件，来发现到底出了什么问题。</p><h2 id="Debugging-Service"><a href="#Debugging-Service" class="headerlink" title="Debugging Service"></a>Debugging Service</h2><p>Service 可以为一组 Pod 提供负载均衡的功能。</p><p>首先，检查Service的Endpoints。 </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get endpoints <span class="variable">$&#123;SERVICE_NAME&#125;</span></span><br></pre></td></tr></table></figure><p>请确保 enpoints 的个数与期望与该 Service 匹配的 Pod 的个数是相同的。例如，如果使用 Deployment 部署了 web-press，副本数为 2，此时，在输出结果的 ENDPOINTS 字段，应该有两个不同的 IP 地址。 </p><h3 id="Service中没有Endpoints"><a href="#Service中没有Endpoints" class="headerlink" title="Service中没有Endpoints"></a>Service中没有Endpoints</h3><p>如果Service中没有Endpoints，请尝试使用 Service 的 label selector 查询一下是否存在 Pod。假设 Service 如下： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myservice</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ns1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">selector:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">frontend</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p> 执行如下命令可以查看 Service 所匹配的 Pod： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods --selector=name=nginx,<span class="built_in">type</span>=frontend -n ns1</span><br></pre></td></tr></table></figure><p>如果 Pod 列表是期望的结果，但是 ENDPOINTS 还是空的，此时很可能是没有为 Service 指定正确的端口。</p><p>如果 Service 中指定的 <code>containerPort</code> 实际上并不存在于 Pod 中，该 Pod 不会被添加到 ENDPOINTS 列表里。请确保 Service 指定的 <code>containerPort</code> 在 Pod 中是可以访问的。 </p><h2 id="网络转发问题"><a href="#网络转发问题" class="headerlink" title="网络转发问题"></a>网络转发问题</h2><p>如果客户端可以连接上 Service，但是连接很快就被断开了，并且 endpoints 中有合适的内容，此时，有可能是 proxy 不能转发到 Pod 上。</p><p>请检查：</p><ul><li>Pod是否正常工作？<code>kubectl get pods</code> 查看 Pod 的 restart count，诊断一下 Pod 是否有问题。</li><li>是否可以直接连接到 Pod ？<code>kubectl get pods -o wide</code> 可以获得 Pod 的IP地址，从任意一个节点上执行 <code>ping</code> 命令，确认网络连接是否正常。</li><li>应用程序是否正常地监听了端口？Kubernetes 不对网络端口做映射，如果您的应用程序监听 8080 端口，则在 Service 中应该指定 <code>containerPort</code> 为 8080。在任意节点上执行命令 <code>curl :</code> 可查看 Pod 中容器的端口是否正常。</li></ul><h1 id="诊断集群问题"><a href="#诊断集群问题" class="headerlink" title="诊断集群问题"></a>诊断集群问题</h1><h2 id="查看集群中的节点："><a href="#查看集群中的节点：" class="headerlink" title="查看集群中的节点："></a>查看集群中的节点：</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes -o wide</span><br><span class="line">kubectl describe node <span class="variable">$&#123;NODE_NAME&#125;</span></span><br></pre></td></tr></table></figure><ul><li><p><code>kube-</code> 开头的 Pod 都是 Kubernetes 集群的系统级组件</p></li><li><p><code>calico-</code> 开头是的 calico 网络插件</p></li><li><p><code>etcd-</code> 开头的是 etcd</p></li><li><p><code>coredns-</code> 开头的是 DNS 插件。 </p><p>假设 apiserver 可能有故障，可以执行以下命令以查看其日志 </p></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f kube-apiserver-demo-master<span class="_">-a</span>-1 -n kube-system</span><br></pre></td></tr></table></figure><h3 id="查看-kubelet-的日志"><a href="#查看-kubelet-的日志" class="headerlink" title="查看 kubelet 的日志"></a>查看 kubelet 的日志</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -u kubelet</span><br></pre></td></tr></table></figure><h2 id="集群故障的常见原因"><a href="#集群故障的常见原因" class="headerlink" title="集群故障的常见原因"></a>集群故障的常见原因</h2><p>一部分 kubernetes 集群常见的故障原因以及应对办法：</p><p>可能的 Root causes：</p><ul><li>虚拟机（或所在物理机）停机</li><li>集群内部发生网络不通的情况，或者集群和用户之间网络不通</li><li>Kubernetes 系统组件崩溃</li><li>数据丢失，或持久化存储不可用</li></ul><p>具体的故障场景有：</p><ul><li>Apiserver 所在虚拟机 shotdown 或者 apiserver 崩溃<ul><li>导致的结果：<ul><li>不能创建、停止、更新 Pod、Service、Deployment等</li><li>已有的 Pod 和 Service 仍然能够正常工作，除非该 Pod 或 Service 需要调用 Kubernetes 的接口，例如 Kubernetes Dashboard 和 Kuboard</li></ul></li></ul></li><li>Apiserver 的后端数据丢失<ul><li>导致的结果：<ul><li>apiserver 将不能再启动</li><li>已有的 Pod 和 Service 仍然能够正常工作，除非该 Pod 或 Service 需要调用 Kubernetes 的接口，例如 Kubernetes Dashboard 和 Kuboard</li><li>需要手工恢复（或重建） apiserver 的数据才能启动 apiserver</li></ul></li></ul></li><li>其他 Master 组件崩溃<ul><li>导致的结果和 apiserver 相同</li></ul></li><li>个别节点（虚拟机或物理机）停机<ul><li>导致的结果<ul><li>该节点上的所有 Pod 不再运行</li></ul></li></ul></li><li>网络分片<ul><li>导致的结果<ul><li>区域A认为区域B中的节点已死机；区域B认为区域A中的 apiserver 已死机（假设apiserver在区域A）</li></ul></li></ul></li><li>kubelet 软件故障<ul><li>导致的结果<ul><li>已崩溃的 Kubelet 不能在该节点上再创建新的 Pod</li><li>kubelet 有可能错误地删除了 Pod</li><li>节点被标记为 <code>unhealthy</code></li><li>Deployment/ReplicationController 在其他节点创建新的 Pod</li></ul></li></ul></li><li>集群管理员的人为错误<ul><li>导致的结果<ul><li>丢失 Pod、Service 等</li><li>丢失 apiserver 的数据</li><li>用户不能访问接口，等等</li></ul></li></ul></li></ul><h2 id="应对办法："><a href="#应对办法：" class="headerlink" title="应对办法："></a>应对办法：</h2><ul><li>Action： 为 apiserver + etcd 使用 IaaS 供应商提供的稳定可靠的持久化存储<ul><li>应对问题： Apiserver 的后端数据丢失</li></ul></li><li>Action： 使用高可用配置<ul><li>应对问题：Apiserver 所在虚拟机 shotdown 或者 apiserver 崩溃</li><li>应对问题：其他 Master 组件崩溃</li><li>应对问题：个别节点（虚拟机或物理机）停机</li></ul></li><li>Action：周期性的为 apiserver 的 etcd 所使用的数据卷创建磁盘快照（Snapshot）<ul><li>应对问题：Apiserver 的后端数据丢失</li><li>应对问题：集群管理员的人为错误</li><li>应对问题：kubelet 软件故障</li></ul></li><li>Action：使用Deployment/StatefulSet/DaemonSet 等控制器，而不是直接创建 Pod<ul><li>应对问题：个别节点（虚拟机或物理机）停机，或者 kubelet 软件故障</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Kubernetes 出现问题排查&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-Security Context</title>
    <link href="http://yoursite.com/2020/04/25/K8s-Security-Context/"/>
    <id>http://yoursite.com/2020/04/25/K8s-Security-Context/</id>
    <published>2020-04-25T06:25:08.000Z</published>
    <updated>2020-04-26T06:37:34.237Z</updated>
    
    <content type="html"><![CDATA[<p> Security Context 概述及配置</p><a id="more"></a> <h1 id="Security-Context-概述"><a href="#Security-Context-概述" class="headerlink" title="Security Context 概述"></a>Security Context 概述</h1><p>Security Context（安全上下文）用来限制容器对宿主节点的可访问范围，以避免容器非法操作宿主节点的系统级别的内容，使得节点的系统或者节点上其他容器组受到影响。</p><p>Security Context可以按照如下几种方式设定：</p><ul><li>访问权限控制：是否可以访问某个对象（例如文件）是基于 userID（UID）和 groupID（GID）的</li><li>Security Enhanced Linux (SELinux)：为对象分配Security标签</li><li>以 privileged（特权）模式运行</li><li>Linux Capabilities：为容器组（或容器）分配一部分特权，而不是 root 用户的所有特权</li><li>AppArmor：自 Kubernetes v1.4 以来，一直处于 beta 状态</li><li>Seccomp：过滤容器中进程的系统调用（system call）</li><li>AllowPrivilegeEscalation（允许特权扩大）：此项配置是一个布尔值，定义了一个进程是否可以比其父进程获得更多的特权，直接效果是，容器的进程上是否被设置 no_new_privs 标记。当出现如下情况时，AllowPrivilegeEscalation 的值始终为 true：<ul><li>容器以 privileged 模式运行</li><li>容器拥有 CAP_SYS_ADMIN 的 Linux Capability</li></ul></li></ul><h1 id="为Pod设置Security-Context"><a href="#为Pod设置Security-Context" class="headerlink" title="为Pod设置Security Context"></a>为Pod设置Security Context</h1><p>在 Pod 的定义中增加 <code>securityContext</code> 字段，即可为 Pod 指定 Security 相关的设定。 <code>securityContext</code> 字段是一个 PodSecurityContext 对象。通过该字段指定的内容将对该 Pod 中所有的容器生效。 </p><h2 id="Pod示例："><a href="#Pod示例：" class="headerlink" title="Pod示例："></a>Pod示例：</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">security-context-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">runAsUser:</span> <span class="number">1000</span></span><br><span class="line">    <span class="attr">runAsGroup:</span> <span class="number">3000</span></span><br><span class="line">    <span class="attr">fsGroup:</span> <span class="number">2000</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sec-ctx-vol</span></span><br><span class="line">    <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sec-ctx-demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">[</span> <span class="string">"sh"</span><span class="string">,</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"sleep 1h"</span> <span class="string">]</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sec-ctx-vol</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/data/demo</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>在上面的例子中：</p><ul><li><p>spec.securityContext.runAsUser 字段指定了该 Pod 中所有容器的进程都以UserID 1000 的身份运行，spec.securityContext.runAsGroup 字段指定了该 Pod 中所有容器的进程都以GroupID 3000 的身份运行</p></li><li><ul><li>如果该字段被省略，容器进程的GroupID为 root(0)</li><li>容器中创建的文件，其所有者为 userID 1000，groupID 3000</li></ul></li><li><p>spec.securityContext.fsGroup 字段指定了该 Pod 的 fsGroup 为 2000</p></li><li><ul><li>数据卷 （本例中，对应挂载点 /data/demo 的数据卷为 sec-ctx-demo） 的所有者以及在该数据卷下创建的任何文件，其 GroupID 为 2000</li></ul></li></ul><h2 id="执行Pod示例"><a href="#执行Pod示例" class="headerlink" title="执行Pod示例"></a>执行Pod示例</h2><ul><li>创建 Pod</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl apply -f security-context-1.yaml </span></span><br><span class="line">pod/security-context-demo created</span><br></pre></td></tr></table></figure><ul><li>验证 Pod 已运行</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl get pod security-context-demo</span></span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">security-context-demo   1/1     Running   2          159m</span><br></pre></td></tr></table></figure><ul><li>进入容器的命令行界面</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl exec -it security-context-demo -- sh</span></span><br></pre></td></tr></table></figure><ul><li>在该命令行界面中，查看正在运行的进程，所有的进程都以 user 1000 的身份运行（由 runAsUser 指定）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/ $ ps</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 1000      0:00 sleep 1h</span><br><span class="line">    6 1000      0:00 sh</span><br><span class="line">   11 1000      0:00 ps</span><br></pre></td></tr></table></figure><ul><li>切换到目录 /data，并查看目录中的文件列表，/data/demo 目录的 groupID 为 2000（由 fsGroup 指定）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/ $ <span class="built_in">cd</span> /data/</span><br><span class="line">/data $ ls -l</span><br><span class="line">total 0</span><br><span class="line">drwxrwsrwx    2 root     2000             6 Apr 25 06:42 demo</span><br></pre></td></tr></table></figure><ul><li>在命令行界面中，切换到目录 /data/demo，并创建一个文件，testfile 的 groupID 为 2000 （由 FSGroup 指定），输出结果如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/ $ <span class="built_in">cd</span> /data/demo/</span><br><span class="line">/data/demo $ <span class="built_in">echo</span> hello &gt; testfile</span><br><span class="line">/data/demo $ ls -l</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--    1 1000     2000             6 Apr 25 09:19 testfile</span><br></pre></td></tr></table></figure><ul><li>在命令行界面中执行 id 命令，输出结果如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/data/demo $ id</span><br><span class="line">uid=1000 gid=3000 groups=2000</span><br></pre></td></tr></table></figure><ul><li>gid 为 3000，与 <code>runAsGroup</code> 字段所指定的一致</li><li>如果 <code>runAsGroup</code> 字段被省略，则 gid 取值为 0（即 root），此时容器中的进程将可以操作 root Group 的文件</li></ul><h1 id="为容器设置Security-Context"><a href="#为容器设置Security-Context" class="headerlink" title="为容器设置Security Context"></a>为容器设置Security Context</h1><p> 容器的定义中包含 <code>securityContext</code> 字段，该字段接受 SecurityContext 对象。通过指定该字段，可以为容器设定安全相关的配置，当该字段的配置与 Pod 级别的 <code>securityContext</code> 配置相冲突时，容器级别的配置将覆盖 Pod 级别的配置。容器级别的 <code>securityContext</code> 不影响 Pod 中的数据卷。 </p><p>下面的示例中的 Pod 包含一个 Container，且 Pod 和 Container 都有定义 <code>securityContext</code> 字段： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">security-context-demo-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">runAsUser:</span> <span class="number">1000</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sec-ctx-demo-2</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">[</span> <span class="string">"sh"</span><span class="string">,</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"sleep 1h"</span> <span class="string">]</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">2000</span></span><br><span class="line">      <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><ul><li>执行命令以创建 Pod </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0426]<span class="comment"># kubectl apply -f security-context-demo-2.yaml </span></span><br><span class="line">pod/security-context-demo-2 created</span><br></pre></td></tr></table></figure><ul><li>执行命令以验证容器已运行</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0426]<span class="comment"># kubectl get pod security-context-demo-2</span></span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">security-context-demo-2   1/1     Running   0          8s</span><br></pre></td></tr></table></figure><ul><li>执行命令进入容器的命令行界面： </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0426]<span class="comment"># kubectl exec -it security-context-demo-2 -- sh</span></span><br><span class="line">/ $ ps</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 2000      0:00 sleep 1h</span><br><span class="line">    6 2000      0:00 sh</span><br><span class="line">   11 2000      0:00 ps</span><br></pre></td></tr></table></figure><p>注意： 容器的进程以 userID 2000 的身份运行。该取值由 <code>spec.containers[*].securityContext.runAsUser</code> 容器组中的字段定义。Pod 中定义的 <code>spec.securityContext.runAsUser</code> 取值 1000 被覆盖。</p><h1 id="为容器设置SELinux标签"><a href="#为容器设置SELinux标签" class="headerlink" title="为容器设置SELinux标签"></a>为容器设置SELinux标签</h1><p>Pod 或容器定义的 <code>securityContext</code> 中 <code>seLinuxOptions</code> 字段是一个 SELinuxOptions 对象，该字段可用于为容器指定 SELinux 标签。如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line">  <span class="attr">seLinuxOptions:</span></span><br><span class="line">    <span class="attr">level:</span> <span class="string">"s0:c123,c456"</span></span><br></pre></td></tr></table></figure><p> 为容器指定 SELinux 标签时，宿主节点的 SELinux 模块必须加载。 </p><h1 id="关于数据卷"><a href="#关于数据卷" class="headerlink" title="关于数据卷"></a>关于数据卷</h1><p>Pod 的 securityContext 作用于 Pod 中所有的容器，同时对 Pod 的数据卷也同样生效。具体来说，<code>fsGroup</code> 和 <code>seLinuxOptions</code> 将被按照如下方式应用到 Pod 中的数据卷：</p><ul><li><code>fsGroup</code>：对于支持 ownership 管理的数据卷，通过 <code>fsGroup</code> 指定的 GID 将被设置为该数据卷的 owner，并且可被 <code>fsGroup</code> 写入。</li><li><code>seLinuxOptions</code>：对于支持 SELinux 标签的数据卷，将按照 <code>seLinuxOptions</code> 的设定重新打标签，以使 Pod 可以访问数据卷内容。通常只需要设置 <code>seLinuxOptions</code> 中 <code>level</code> 这一部分内容。该设定为 Pod 中所有容器及数据卷设置 Multi-Category Security (MCS) 标签。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Security Context 概述及配置&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-Secret概述</title>
    <link href="http://yoursite.com/2020/04/25/K8s-Secret%E6%A6%82%E8%BF%B0/"/>
    <id>http://yoursite.com/2020/04/25/K8s-Secret%E6%A6%82%E8%BF%B0/</id>
    <published>2020-04-25T01:41:52.000Z</published>
    <updated>2020-04-25T06:24:00.437Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes Secret 对象可以用来储存敏感信息，例如：密码、OAuth token、ssh 密钥等</p> <a id="more"></a> <h1 id="一、Secret概述"><a href="#一、Secret概述" class="headerlink" title="一、Secret概述"></a>一、Secret概述</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Kubernetes <code>Secret</code> 对象可以用来储存敏感信息，例如：密码、OAuth token、ssh 密钥等。如果不使用 <code>Secret</code>，此类信息可能被放置在 Pod 定义中或者容器镜像中。将此类敏感信息存储到 <code>Secret</code> 中，可以更好地：</p><ul><li>控制其使用</li><li>降低信息泄露的风险</li></ul><p>用户可以直接创建 Secret，Kubernetes 系统也会创建一些 Secret。</p><p>Secret有如下几种使用方式：</p><ul><li>作为 Pod 的数据卷挂载</li><li>作为 Pod 的环境变量</li><li>kubelet 在抓取容器镜像时，作为 docker 镜像仓库的用户名密码</li></ul><h2 id="内建Secret"><a href="#内建Secret" class="headerlink" title="内建Secret"></a>内建Secret</h2><p>Service Account 将自动创建 Secret</p><p>Kubernetes 自动创建包含访问 Kubernetes APIServer 身份信息的 Secret，并自动修改 Pod 使其引用这类 Secret。</p><p>如果需要，可以禁用或者自定义自动创建并使用 Kubernetes APIServer 身份信息的特性。然而，如果期望安全地访问 Kubernetes APIServer，应该使用默认的 Secret 创建使用过程。</p><h2 id="自建Secret"><a href="#自建Secret" class="headerlink" title="自建Secret"></a>自建Secret</h2><p>可以使用如下方式创建自己的 Secret：</p><ul><li>使用 kubectl 创建 Secret</li><li>手动创建 Secret</li><li>使用 Generator 创建 Secret</li></ul><h2 id="解码和编辑"><a href="#解码和编辑" class="headerlink" title="解码和编辑"></a>解码和编辑</h2><p>Kubenetes 中，Secret 使用 base64 编码存储，您可以将其解码获得对应信息的原文，创建 Secret 之后，您也可以再次编辑Secret</p><h1 id="二、创建Secret（使用kubectl）"><a href="#二、创建Secret（使用kubectl）" class="headerlink" title="二、创建Secret（使用kubectl）"></a>二、创建Secret（使用kubectl）</h1><p> 假设某个 Pod 需要访问数据库。在您执行 kubectl 命令所在机器的当前目录，创建文件 <code>./username.txt</code> 文件和 <code>./password.txt</code> 暂存数据库的用户名和密码，后续我们根据这两个文件配置 kubernetes secrets。 </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -n <span class="string">'admin'</span> &gt; ./username.txt</span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">'1f2d1e2e67df'</span> &gt; ./password.txt</span><br></pre></td></tr></table></figure><p>在 Kubernetes APIServer 中创建 Secret 对象，并将这两个文件中的内容存储到该 Secret 对象中，执行命令，输出结果如下所示： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt </span></span><br><span class="line">secret/db-user-pass created</span><br></pre></td></tr></table></figure><ul><li><p>上述命令的执行效果与此命令执行效果相同： <code>kubectl create secret generic db-user-pass –from-literal=username=admin –from-literal=password=1f2d1e2e67df</code></p></li><li><p>如果密码中包含特殊字符需要转码（例如 <code>$</code>、<code>*</code>、<code>\</code>、<code>!</code>），请使用 <code>\</code> 进行转码。例如：实际密码为 <code>S!B\*d$zDsb</code>，kubectl 命令应该写成 <code>kubectl create secret generic dev-db-secret –from-literal=username=devuser –from-literal=password=S\!B\\*d\$zDsb</code>。如果通过文件创建（–from-file），则无需对文件中的密码进行转码。</p><p>检查 Secret 的创建结果，输出信息如下所示： </p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl get secrets</span></span><br><span class="line">NAME                  TYPE                                  DATA   AGE</span><br><span class="line">db-user-pass          Opaque                                2      57s</span><br><span class="line">default-token-xws5p   kubernetes.io/service-account-token   3      28d</span><br><span class="line">nginxsecret           Opaque                                2      15d</span><br></pre></td></tr></table></figure><p> 查看 Secret 详情，输出信息如下所示： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl describe secrets db-user-pass</span></span><br><span class="line">Name:         db-user-pass</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Type:  Opaque</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line"></span><br><span class="line">password.txt:  12 bytes</span><br><span class="line">username.txt:  5 bytes</span><br></pre></td></tr></table></figure><p>默认情况下，<code>kubectl get</code> 和 <code>kubectl describe</code> 命令都避免展示 Secret 的内容。这种做法可以避免密码被偷窥，或者被存储到终端的日志中 </p><h1 id="三、创建Secret（手动）"><a href="#三、创建Secret（手动）" class="headerlink" title="三、创建Secret（手动）"></a>三、创建Secret（手动）</h1><p>可以在 yaml 文件中定义好 Secret，然后通过 <code>kubectl apply -f</code> 命令创建。通过如下两种方式在 yaml 文件中定义 Secret：</p><ul><li><strong>data</strong>：使用 data 字段时，取值的内容必须是 base64 编码的</li><li><strong>stringData</strong>：使用 stringData 时，更为方便，您可以直接将取值以明文的方式写在 yaml 文件中</li></ul><h2 id="在-yaml-中定义-data"><a href="#在-yaml-中定义-data" class="headerlink" title="在 yaml 中定义 data"></a>在 yaml 中定义 data</h2><ul><li>假设要保存 <code>username=admin</code> 和 <code>password=1f2d1e2e67df</code> 到 Secret 中，要先将数据的值转化为 base64 编码，执行如下命令：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -n <span class="string">'admin'</span> | base64</span><br><span class="line">YWRtaW4=</span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">'1f2d1e2e67df'</span> | base64</span><br><span class="line">MWYyZDFlMmU2N2Rm</span><br></pre></td></tr></table></figure><p>创建 secret.yaml 文件，内容如下所示：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">YWRtaW4=</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">MWYyZDFlMmU2N2Rm</span></span><br></pre></td></tr></table></figure><p>执行命令创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f .&#x2F;secret.yaml</span><br><span class="line">secret &quot;mysecret&quot; created</span><br></pre></td></tr></table></figure><h2 id="在-yaml-中定义-stringData"><a href="#在-yaml-中定义-stringData" class="headerlink" title="在 yaml 中定义 stringData"></a>在 yaml 中定义 stringData</h2><p>假如并不想先将用户名和密码转换为 base64 编码之后再创建 Secret，则，可以通过定义 stringData 来达成，此时 stringData 中的取值部分将被 apiserver 自动进行 base64 编码之后再保存。</p><ul><li>创建文件 secret.yaml，内容如下所示：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">1f2d1e2e67df</span></span><br></pre></td></tr></table></figure><p>执行命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f .&#x2F;secret.yaml</span><br><span class="line">secret &quot;mysecret&quot; created</span><br></pre></td></tr></table></figure><p>执行命令 <code>kubectl get -f ./secret.yaml -o yaml</code> 输出结果如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">MWYyZDFlMmU2N2Rm</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">YWRtaW4=</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubectl.kubernetes.io/last-applied-configuration:</span> <span class="string">|</span></span><br><span class="line">      <span class="string">&#123;"apiVersion":"v1","kind":"Secret","metadata":&#123;"annotations":&#123;&#125;,"name":"mysecret","namespace":"default"&#125;,"stringData":&#123;"password":"1f2d1e2e67df","username":"admin"&#125;,"type":"Opaque"&#125;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">"2019-09-23T14:16:56Z"</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">"10318365"</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/default/secrets/mysecret</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="number">24602031</span><span class="string">-e18d-467a-b7fe-0962af8ec8b8</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br></pre></td></tr></table></figure><h2 id="同时定义了-data-和-stringData"><a href="#同时定义了-data-和-stringData" class="headerlink" title="同时定义了 data 和 stringData"></a>同时定义了 data 和 stringData</h2><p>如果同时定义了 data 和 stringData，对于两个对象中key 重复的字段，最终将采纳 stringData 中的 value </p><p>创建文件 secret.yaml，该文件同时定义了 data 和 stringData，内容如下所示：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">YWRtaW4=</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">administrator</span></span><br></pre></td></tr></table></figure><p>执行命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f .&#x2F;secret.yaml</span><br><span class="line">secret &quot;mysecret&quot; created</span><br></pre></td></tr></table></figure><p> 执行命令 <code>kubectl get -f ./secret.yaml -o yaml</code> 输出结果如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="number">0425</span><span class="string">]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">-f</span>  <span class="string">secret.yaml</span> <span class="string">-o</span> <span class="string">yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">YWRtaW5pc3RyYXRvcg==</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubectl.kubernetes.io/last-applied-configuration:</span> <span class="string">|</span></span><br><span class="line">      <span class="string">&#123;"apiVersion":"v1","data":&#123;"username":"YWRtaW4="&#125;,"kind":"Secret","metadata":&#123;"annotations":&#123;&#125;,"name":"mysecret","namespace":"default"&#125;,"stringData":&#123;"username":"administrator"&#125;,"type":"Opaque"&#125;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">"2020-04-25T05:46:21Z"</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">"1544704"</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/default/secrets/mysecret</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">1b1d10a7-87e2-40a4-8101-693aad84eca2</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br></pre></td></tr></table></figure><p> 此处 <code>YWRtaW5pc3RyYXRvcg==</code> 解码后的值是 <code>administrator</code> </p><h2 id="将配置文件存入-Secret"><a href="#将配置文件存入-Secret" class="headerlink" title="将配置文件存入 Secret"></a>将配置文件存入 Secret</h2><p>假设某个应用程序需要从一个配置文件中读取敏感信息，此时，可以将该文件的内容存入 Secret，再通过数据卷的形式挂载到容器。</p><p>例如，应用程序需要读取如下配置文件内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apiUrl: &quot;https:&#x2F;&#x2F;my.api.com&#x2F;api&#x2F;v1&quot;</span><br><span class="line">username: user</span><br><span class="line">password: password</span><br></pre></td></tr></table></figure><p>可以使用下面的 secret.yaml 创建 Secret</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">config.yaml:</span> <span class="string">|-</span></span><br><span class="line">    <span class="attr">apiUrl:</span> <span class="string">"https://my.api.com/api/v1"</span></span><br><span class="line">    <span class="attr">username:</span> <span class="string">user</span></span><br><span class="line">    <span class="attr">password:</span> <span class="string">password</span></span><br></pre></td></tr></table></figure><p>执行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ./secret.yaml</span><br><span class="line">secret <span class="string">"mysecret"</span> created</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl get -f secret.yaml -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  config.yaml: YXBpVXJsOiAiaHR0cHM6Ly9teS5hcGkuY29tL2FwaS92MSIKdXNlcm5hbWU6IHVzZXIKcGFzc3dvcmQ6IHBhc3N3b3Jk</span><br><span class="line">  username: <span class="string">""</span></span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubectl.kubernetes.io/last-applied-configuration: |</span><br><span class="line">      &#123;<span class="string">"apiVersion"</span>:<span class="string">"v1"</span>,<span class="string">"kind"</span>:<span class="string">"Secret"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"mysecret"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"stringData"</span>:&#123;<span class="string">"config.yaml"</span>:<span class="string">"apiUrl: \"https://my.api.com/api/v1\"\nusername: user\npassword: password"</span>&#125;,<span class="string">"type"</span>:<span class="string">"Opaque"</span>&#125;</span><br><span class="line">  creationTimestamp: <span class="string">"2020-04-25T05:46:21Z"</span></span><br><span class="line">  name: mysecret</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: <span class="string">"1545171"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/default/secrets/mysecret</span><br><span class="line">  uid: 1b1d10a7-87e2-40a4-8101-693aad84eca2</span><br><span class="line"><span class="built_in">type</span>: Opaque</span><br></pre></td></tr></table></figure><h1 id="四、解码和编辑Secret"><a href="#四、解码和编辑Secret" class="headerlink" title="四、解码和编辑Secret"></a>四、解码和编辑Secret</h1><h2 id="解码Secret"><a href="#解码Secret" class="headerlink" title="解码Secret"></a>解码Secret</h2><p>Secret 中的信息可以通过 <code>kubectl get secret</code> 命令获取。</p><p>执行命令 <code>kubectl get secret mysecret -o yaml</code> 可获取所创建的 Secret，输出信息如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl get secret mysecret -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  config.yaml: <span class="string">""</span></span><br><span class="line">  password: MWYyZDFlMmU2N2Rm</span><br><span class="line">  username: YWRtaW4=</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubectl.kubernetes.io/last-applied-configuration: |</span><br><span class="line">      &#123;<span class="string">"apiVersion"</span>:<span class="string">"v1"</span>,<span class="string">"kind"</span>:<span class="string">"Secret"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"mysecret"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"stringData"</span>:&#123;<span class="string">"password"</span>:<span class="string">"1f2d1e2e67df"</span>,<span class="string">"username"</span>:<span class="string">"admin"</span>&#125;,<span class="string">"type"</span>:<span class="string">"Opaque"</span>&#125;</span><br><span class="line">  creationTimestamp: <span class="string">"2020-04-25T05:46:21Z"</span></span><br><span class="line">  name: mysecret</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: <span class="string">"1546981"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/default/secrets/mysecret</span><br><span class="line">  uid: 1b1d10a7-87e2-40a4-8101-693aad84eca2</span><br><span class="line"><span class="built_in">type</span>: Opaque</span><br></pre></td></tr></table></figure><p> 执行命令 <code>echo &#39;MWYyZDFlMmU2N2Rm&#39; | base64 --decode</code> 可解码密码字段，输出结果如下： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># echo 'MWYyZDFlMmU2N2Rm' | base64 --decode</span></span><br><span class="line">1f2d1e2e67df</span><br></pre></td></tr></table></figure><p> 执行命令 <code>echo &#39;YWRtaW4=&#39; | base64 --decode</code> 可解码用户名字段，输出结果如下： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># echo 'YWRtaW4=' | base64 --decode</span></span><br><span class="line">admin</span><br></pre></td></tr></table></figure><h2 id="编辑Secret"><a href="#编辑Secret" class="headerlink" title="编辑Secret"></a>编辑Secret</h2><p>执行命令 <code>kubectl edit secrets mysecret</code> 可以编辑已经创建的 Secret，该命令将打开一个类似于 <code>vi</code> 的文本编辑器，可以直接编辑已经进行 base64 编码的字段，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl edit secrets mysecret</span></span><br><span class="line">Edit cancelled, no changes made.</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes Secret 对象可以用来储存敏感信息，例如：密码、OAuth token、ssh 密钥等&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-污点和容忍</title>
    <link href="http://yoursite.com/2020/04/24/K8s-%E6%B1%A1%E7%82%B9%E5%92%8C%E5%AE%B9%E5%BF%8D/"/>
    <id>http://yoursite.com/2020/04/24/K8s-%E6%B1%A1%E7%82%B9%E5%92%8C%E5%AE%B9%E5%BF%8D/</id>
    <published>2020-04-24T07:00:09.000Z</published>
    <updated>2020-04-25T01:39:57.208Z</updated>
    
    <content type="html"><![CDATA[<p> 污点和容忍</p><a id="more"></a> <h1 id="K8s-污点和容忍"><a href="#K8s-污点和容忍" class="headerlink" title="K8s-污点和容忍"></a>K8s-污点和容忍</h1><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Pod 中存在属性 Node selector / Node affinity，用于将 Pod 指定到合适的节点。</p><p>相对的，节点中存在属性 <code>污点 taints</code>，使得节点可以排斥某些 Pod。</p><p>污点和容忍（taints and tolerations）成对工作，以确保 Pod 不会被调度到不合适的节点上。</p><ul><li>可以为节点增加污点（taints，一个节点可以有 0-N 个污点）</li><li>可以为 Pod 增加容忍（toleration，一个 Pod 可以有 0-N 个容忍）</li><li>如果节点上存在污点，则该节点不会接受任何不能容忍（tolerate）该污点的 Pod</li></ul><h2 id="向节点添加污点"><a href="#向节点添加污点" class="headerlink" title="向节点添加污点"></a>向节点添加污点</h2><ul><li><p>执行 <code>kubectl taint</code> 命令，可向节点添加污点，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes node1 key=value:NoSchedule</span><br></pre></td></tr></table></figure><p>该命令为节点 <code>node1</code> 添加了一个污点。污点是一个键值对，在本例中，污点的键为 <code>key</code>，值为 <code>value</code>，污点效果为 <code>NoSchedule</code>。此污点意味着 Kubernetes 不会向该节点调度任何 Pod，除非该 Pod 有一个匹配的容忍（toleration）</p></li><li><p>执行如下命令可以将本例中的污点移除：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes node1 key:NoSchedule-</span><br></pre></td></tr></table></figure></li></ul><h2 id="向-Pod-添加容忍"><a href="#向-Pod-添加容忍" class="headerlink" title="向 Pod 添加容忍"></a>向 Pod 添加容忍</h2><p>PodSpec 中有一个 <code>tolerations</code> 字段，可用于向 Pod 添加容忍。下面的两个例子中定义的容忍都可以匹配上例中的污点，包含这些容忍的 Pod 也都可以被调度到 <code>node1</code> 节点上：</p><p>容忍1：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">"key"</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">"Equal"</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">"value"</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure><p>容忍2：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">"key"</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">"Exists"</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure><p>下面这个 Pod 的例子中，使用了容忍：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">env:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">"example-key"</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">"Exists"</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure><h2 id="污点与容忍的匹配"><a href="#污点与容忍的匹配" class="headerlink" title="污点与容忍的匹配"></a>污点与容忍的匹配</h2><p>当满足如下条件时，Kubernetes 认为容忍和污点匹配：</p><ul><li><p>键（key）相同</p></li><li><p>效果（effect）相同</p></li><li><p>污点的 operator 为：</p><ul><li><code>Exists</code> （此时污点中不应该指定 <code>value</code>）</li><li>或者 <code>Equal</code> （此时容忍的 <code>value</code> 应与污点的 <code>value</code> 相同）</li></ul></li><li><p>如果不指定 <code>operator</code>，则其默认为 <code>Equal</code></p></li></ul><p>一个节点上可以有多个污点，同时一个 Pod 上可以有多个容忍。Kubernetes 使用一种类似于过滤器的方法来处理多个节点和容忍：</p><ul><li>对于节点的所有污点，检查 Pod 上是否有匹配的容忍，如果存在匹配的容忍，则忽略该污点；</li><li>剩下的不可忽略的污点将对该 Pod 起作用</li></ul><p>例如：</p><ul><li>如果存在至少一个不可忽略的污点带有效果 <code>NoSchedule</code>，则 Kubernetes 不会将 Pod 调度到该节点上</li><li>如果没有不可忽略的污点带有效果 <code>NoSchedule</code>，但是至少存在一个不可忽略的污点带有效果 <code>PreferNoSchedule</code>，则 Kubernetes 将尽量避免将该 Pod 调度到此节点</li><li>如果存在至少一个忽略的污点带有效果 NoExecute，则：<ul><li>假设 Pod 已经在该节点上运行，Kubernetes 将从该节点上驱逐（evict）该 Pod</li><li>假设 Pod 尚未在该节点上运行，Kubernetes 将不会把 Pod 调度到该节点</li></ul></li></ul><h1 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h1><p>污点和容忍使用起来非常灵活，可以用于：</p><ul><li>避免 Pod 被调度到某些特定的节点</li><li>从节点上驱逐本不应该在该节点运行的 Pod</li></ul><p>具体的场景可能有：</p><ul><li><strong>专属的节点：</strong> 如果您想将一组节点专门用于特定的场景，您可以为这些节点添加污点（例如 <code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>）然后向对应的 Pod 添加容忍。带有这些容忍的 Pod 将可以使用这一组专属节点，同时也可以使用集群中的其他节点。如果您想进一步限制这些 Pod 只能使用这一组节点，那么您应该为这一组节点添加一个标签（例如 dedicated=groupName），并为这一组 Pod 添加 node affinity（或 node selector）以限制这些 Pod 只能调度到这一组节点上。</li><li><strong>带有特殊硬件的节点：</strong> 集群中，如果某一组节点具备特殊的硬件（例如 GPU），此时非常有必要将那些不需要这类硬件的 Pod 从这组节点上排除掉，以便需要这类硬件的 Pod 可以得到资源。此时您可以为这类节点添加污点（例如：<code>kubectl taint nodes nodename special=true:NoSchedule</code> 或者 <code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>）并为需要这类硬件的 Pod 添加匹配的容忍。</li><li><strong>基于污点的驱逐</strong> 当节点出现问题时，可以使用污点以 Pod 为单位从节点上驱逐 Pod。</li></ul><h1 id="基于污点的驱逐（TaintBasedEviction）"><a href="#基于污点的驱逐（TaintBasedEviction）" class="headerlink" title="基于污点的驱逐（TaintBasedEviction）"></a>基于污点的驱逐（TaintBasedEviction）</h1><p>如果有 NoExecute 的污点效果，该效果将对已经运行在节点上的 Pod 施加如下影响：</p><ul><li>不容忍该污点的 Pod 将立刻被驱逐</li><li>容忍该污点的 Pod 在未指定 <code>tolerationSeconds</code> 的情况下，将继续在该节点上运行</li><li>容忍该污点的 Pod 在指定了 <code>tolerationSeconds</code> 的情况下，将在指定时间超过时从节点上驱逐</li></ul><blockquote><p><code>tolerationSeconds</code> 字段可以理解为 Pod 容忍该污点的 <code>耐心</code>：</p><ul><li>超过指定的时间，则达到 Pod 忍耐的极限，Pod 离开所在节点</li><li>不指定 <code>tolerationSeconds</code>，则认为 Pod 对该污点的容忍是无期限的</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 污点和容忍&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-配置</title>
    <link href="http://yoursite.com/2020/04/22/K8s-%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2020/04/22/K8s-%E9%85%8D%E7%BD%AE/</id>
    <published>2020-04-22T08:32:39.000Z</published>
    <updated>2020-04-24T06:58:12.808Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 的配置信息管理</p><a id="more"></a> <h1 id="K8s-配置相关"><a href="#K8s-配置相关" class="headerlink" title="K8s-配置相关"></a>K8s-配置相关</h1><h1 id="一、使用ConfigMap配置您的应用程序"><a href="#一、使用ConfigMap配置您的应用程序" class="headerlink" title="一、使用ConfigMap配置您的应用程序"></a>一、使用ConfigMap配置您的应用程序</h1><p> Kubernetes ConfigMap 可以将配置信息和容器镜像解耦，以使得容器化的应用程序可移植。 </p><h1 id="二、管理容器的计算资源"><a href="#二、管理容器的计算资源" class="headerlink" title="二、管理容器的计算资源"></a>二、管理容器的计算资源</h1><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>在 Kubernetes 中创建工作负载时，您可以为 Pod 中的每一个容器指定其所需要的内存（RAM）大小和 CPU 数量。如果这些信息被指定了，Kubernetes 调度器可以更好的决定将 Pod 调度到哪一个节点。对于容器来说，其所需要的资源也将依据其指定的数值得到保证</p><h3 id="资源类型及计量"><a href="#资源类型及计量" class="headerlink" title="资源类型及计量"></a>资源类型及计量</h3><p>当计算资源的时候，主要是指 CPU 和 内存。CPU 的计量单位是内核的单元数，内存的计量单位是 byte 字节数。应用程序可以按量请求、分配、消耗计算资源</p><h4 id="CPU-的计量"><a href="#CPU-的计量" class="headerlink" title="CPU 的计量"></a>CPU 的计量</h4><p>Kubernetes 中，0.5 代表请求半个 CPU 资源。表达式 0.1 等价于 表达式 100m。在 API Server 中，表达式 0.1 将被转换成 100m，精度低于 1m 的请求是不受支持的。 CPU 的计量代表的是绝对值，而非相对值，例如，请求了 0.1 个 CPU，无论节点是 单核、双核、48核，得到的 CPU 资源都是 0.1 核。 </p><h4 id="内存的计量"><a href="#内存的计量" class="headerlink" title="内存的计量"></a>内存的计量</h4><p>内存的计量单位是 byte 字节。可以使用一个整数来表达内存的大小，也可以使用后缀来表示（E、P、T、G、M、K）。也可以使用 2 的幂数来表示内存大小，其后缀为（Ei、Pi、Ti、Gi、Mi、Ki）。例如，下面的几个表达方式所表示的内存大小大致相等：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">128974848, 129e6, 129M, 123Mi</span><br></pre></td></tr></table></figure><h3 id="容器组及容器的计算资源请求及限制"><a href="#容器组及容器的计算资源请求及限制" class="headerlink" title="容器组及容器的计算资源请求及限制"></a>容器组及容器的计算资源请求及限制</h3><p>Kubernetes 中，可以为容器指定计算资源的请求数量 request 和限制数量 limit。尽管资源的请求/限制数量只能在容器上指定，我们仍然经常讨论容器组的资源请求/限制数量。容器组对某一个类型的资源请求/限制数量是该容器组中所有工作容器对该资源请求/限制数量的求和。</p><h3 id="带有资源请求的容器组是如何调度的"><a href="#带有资源请求的容器组是如何调度的" class="headerlink" title="带有资源请求的容器组是如何调度的"></a>带有资源请求的容器组是如何调度的</h3><p>当创建 Pod 时（直接创建，或者通过控制器创建），Kubernetes 调度程序选择一个节点去运行该 Pod。每一个节点都有一个最大可提供的资源数量：CPU 数量和内存大小。调度程序将确保：对于每一种资源类型，已调度的 Pod 对该资源的请求之和小于该节点最大可供使用资源数量。</p><p>尽管某个节点实际使用的CPU、内存数量非常低，如果新加入一个 Pod 使得该节点上对 CPU 或内存请求的数量之和大于了该节点最大可供使用 CPU 或内存数量，则调度程序不会将该 Pod 分配到该节点。Kubernetes 这样做可以避免在日常的流量高峰时段，节点上出现资源短缺的情况。 </p><h3 id="带有资源限制的容器组是如何运行的"><a href="#带有资源限制的容器组是如何运行的" class="headerlink" title="带有资源限制的容器组是如何运行的"></a>带有资源限制的容器组是如何运行的</h3><p>Kubelet 启动容器组的容器时，将 CPU、内存的最大使用限制作为参数传递给容器引擎。</p><p>以 Docker 容器引擎为例：</p><ul><li>容器的 cpu 请求将转换成 docker 要求的格式，并以 <code>--cpu-shares</code> 标志传递到 <code>docker run</code> 命令</li><li>容器的 cpu 限制将也将转换成 millicore 表达式并乘以 100。结果数字是每 100ms 的周期内，该容器可以使用的 CPU 份额</li><li>容器的内存限制将转换成一个整数，并使用 <code>--memory</code> 标志传递到 <code>docker run</code> 命令</li></ul><p>如下情况可能会发生：</p><ul><li>如果某个容器超出了其内存限制，它可能将被终止。如果 restartPolicy 为 Always 或 OnFailure，kubelet 将重启该容器</li><li>如果某个容器超出了其内存申请（仍低于其内存限制），当节点超出内存使用时，该容器仍然存在从节点驱逐的可能性</li><li>短时间内容器有可能能够超出其 CPU 使用限制运行。kubernetes 并不会终止这些超出 CPU 使用限制的容器</li></ul><h1 id="三、将容器组调度到指定的节点"><a href="#三、将容器组调度到指定的节点" class="headerlink" title="三、将容器组调度到指定的节点"></a>三、将容器组调度到指定的节点</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>在 Kubernetes，可以限定 Pod 只能在特定的节点上运行，或者优先选择在特定的节点上运行。通常您并不需要这样做，而应该交由 kubernetes 调度程序根据资源使用情况自动地为 Pod 分配节点。但是少数情况下，这种限定仍然是必要的，例如：</p><ul><li>确保某些 Pod 被分配到具有固态硬盘的节点</li><li>将相互通信频繁的两个 Pod 分配到同一个高可用区的节点</li></ul><p>Kubernetes 一共提供了四种方法，可以将 Pod 调度到指定的节点上，这些方法从简便到复杂的顺序如下：</p><ul><li>指定节点 nodeName</li><li>节点选择器 nodeSelector（Kubernetes 推荐用法）</li><li>Node isolation/restriction</li><li>Affinity and anti-affinity</li></ul><h2 id="指定节点-nodeName"><a href="#指定节点-nodeName" class="headerlink" title="指定节点 nodeName"></a>指定节点 nodeName</h2><p>nodeName 是四种方法中最简单的一个，但是因为它的局限性，也是使用最少的。nodeName 是 PodSpec 当中的一个字段。如果该字段非空，调度程序直接将其指派到 nodeName 对应的节点上运行。</p><p>通过 nodeName 限定 Pod 所运行的节点有如下局限性：</p><ul><li>如果 nodeName 对应的节点不存在，Pod 将不能运行</li><li>如果 nodeName 对应的节点没有足够的资源，Pod 将运行失败，可能的原因有：OutOfmemory /OutOfcpu</li><li>集群中的 nodeName 通常是变化的（新的集群中可能没有该 nodeName 的节点，指定的 nodeName 的节点可能从集群中移除）</li></ul><h2 id="节点选择器-nodeSelector"><a href="#节点选择器-nodeSelector" class="headerlink" title="节点选择器 nodeSelector"></a>节点选择器 nodeSelector</h2><p>nodeSelector 是 PodSpec 中的一个字段。指定了一组名值对。节点的 labels 中必须包含 Pod 的 nodeSelector 中所有的名值对，该节点才可以运行此 Pod。最普遍的用法中， nodeSelector 只包含一个名值对。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 的配置信息管理&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>搭建NFS Server</title>
    <link href="http://yoursite.com/2020/04/22/%E6%90%AD%E5%BB%BANFS-Server/"/>
    <id>http://yoursite.com/2020/04/22/%E6%90%AD%E5%BB%BANFS-Server/</id>
    <published>2020-04-22T07:40:54.000Z</published>
    <updated>2020-04-25T06:18:54.521Z</updated>
    
    <content type="html"><![CDATA[<p>搭建 NFS 服务器</p> <a id="more"></a> <h1 id="搭建NFS-Server"><a href="#搭建NFS-Server" class="headerlink" title="搭建NFS Server"></a>搭建NFS Server</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Kubernetes 对 Pod 进行调度时，以当时集群中各节点的可用资源作为主要依据，自动选择某一个可用的节点，并将 Pod 分配到该节点上。在这种情况下，Pod 中容器数据的持久化如果存储在所在节点的磁盘上，就会产生不可预知的问题，例如，当 Pod 出现故障，Kubernetes 重新调度之后，Pod 所在的新节点上，并不存在上一次 Pod 运行时所在节点上的数据。</p><p>为了使 Pod 在任何节点上都能够使用同一份持久化存储数据，我们需要使用网络存储的解决方案为 Pod 提供数据卷。常用的网络存储方案有：NFS/cephfs/glusterfs。</p><h2 id="配置要求"><a href="#配置要求" class="headerlink" title="配置要求"></a>配置要求</h2><ul><li>两台 linux 服务器，centos 7<ul><li>一台用作 nfs server</li><li>另一台用作 nfs 客户端</li></ul></li></ul><h2 id="配置NFS服务器"><a href="#配置NFS服务器" class="headerlink" title="配置NFS服务器"></a>配置NFS服务器</h2><p>执行以下命令安装 nfs 服务器所需的软件包</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>执行命令 <code>vim /etc/exports</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;root&#x2F;nfs_root&#x2F; *(insecure,rw,sync,no_root_squash)</span><br></pre></td></tr></table></figure><p>执行以下命令，启动 nfs 服务</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建共享目录，如果要使用自己的目录，请替换本文档中所有的 /root/nfs_root/</span></span><br><span class="line">mkdir /root/nfs_root</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> rpcbind</span><br><span class="line">systemctl <span class="built_in">enable</span> nfs-server</span><br><span class="line"></span><br><span class="line">systemctl start rpcbind</span><br><span class="line">systemctl start nfs-server</span><br><span class="line">exportfs -r</span><br></pre></td></tr></table></figure><p>检查配置是否生效</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exportfs</span><br><span class="line"><span class="comment"># 输出结果如下所示</span></span><br><span class="line">/root/nfs_root /root/nfs_root</span><br></pre></td></tr></table></figure><h2 id="在客户端测试NFS"><a href="#在客户端测试NFS" class="headerlink" title="在客户端测试NFS"></a>在客户端测试NFS</h2><ul><li>服务器端防火墙开放111、662、875、892、2049的 tcp / udp 允许，否则远端客户无法连接。</li></ul><p>执行以下命令安装 nfs 客户端所需的软件包</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>执行以下命令检查 nfs 服务器端是否有设置共享目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># showmount -e $(nfs服务器的IP)</span></span><br><span class="line">showmount -e 172.17.216.82</span><br><span class="line"><span class="comment"># 输出结果如下所示</span></span><br><span class="line">Export list <span class="keyword">for</span> 172.17.216.82:</span><br><span class="line">/root/nfs_root *</span><br></pre></td></tr></table></figure><p>执行以下命令挂载 nfs 服务器上的共享目录到本机路径 <code>/root/nfsmount</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/nfsmount</span><br><span class="line"><span class="comment"># mount -t nfs $(nfs服务器的IP):/root/nfs_root /root/nfsmount</span></span><br><span class="line">mount -t nfs 172.17.216.82:/root/nfs_root /root/nfsmount</span><br><span class="line"><span class="comment"># 写入一个测试文件</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello nfs server"</span> &gt; /root/nfsmount/test.txt</span><br></pre></td></tr></table></figure><p>在 nfs 服务器上执行以下命令，验证文件写入成功</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /root/nfs_root/test.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;搭建 NFS 服务器&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-存储卷PersistentVolume</title>
    <link href="http://yoursite.com/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/"/>
    <id>http://yoursite.com/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/</id>
    <published>2020-04-17T02:58:16.000Z</published>
    <updated>2020-04-22T07:53:40.804Z</updated>
    
    <content type="html"><![CDATA[<p> PV &amp; PVC</p><a id="more"></a> <h1 id="存储卷PersistentVolume"><a href="#存储卷PersistentVolume" class="headerlink" title="存储卷PersistentVolume"></a>存储卷PersistentVolume</h1><p>PersistentVolume（PV 存储卷）是集群中的一块存储空间，由集群管理员管理、或者由 Storage Class（存储类）自动管理。PV（存储卷）和 node（节点）一样，是集群中的资源（kubernetes 集群由存储资源和计算资源组成）。</p><p>PersistentVolumeClaim（存储卷声明）是一种类型的 Volume（数据卷），PersistentVolumeClaim（存储卷声明）引用的 PersistentVolume（存储卷）有自己的生命周期，该生命周期独立于任何使用它的容器组。PersistentVolume（存储卷）描述了如何提供存储的细节信息（NFS、cephfs等存储的具体参数）。</p><p>PersistentVolumeClaim（PVC 存储卷声明）代表用户使用存储的请求。Pod 容器组消耗 node 计算资源，PVC 存储卷声明消耗 PersistentVolume 存储资源。Pod 容器组可以请求特定数量的计算资源（CPU / 内存）；PersistentVolumeClaim 可以请求特定大小/特定访问模式（只能被单节点读写/可被多节点只读/可被多节点读写）的存储资源。</p><p>根据应用程序的特点不同，其所需要的存储资源也存在不同的要求，例如读写性能等。集群管理员必须能够提供关于 PersistentVolume（存储卷）的更多选择，无需用户关心存储卷背后的实现细节。为了解决这个问题，Kubernetes 引入了 StorageClass（存储类）的概念</p><h2 id="存储卷和存储卷声明的关系"><a href="#存储卷和存储卷声明的关系" class="headerlink" title="存储卷和存储卷声明的关系"></a>存储卷和存储卷声明的关系</h2><p>存储卷和存储卷声明的关系如下图所示：</p><ul><li>PersistentVolume 是集群中的存储资源，通常由集群管理员创建和管理</li><li>StorageClass 用于对 PersistentVolume 进行分类，如果正确配置，StorageClass 也可以根据 PersistentVolumeClaim 的请求动态创建 Persistent Volume</li><li>PersistentVolumeClaim 是使用该资源的请求，通常由应用程序提出请求，并指定对应的 StorageClass 和需求的空间大小</li><li>PersistentVolumeClaim 可以做为数据卷的一种，被挂载到容器组/容器中使用</li></ul><p><img src="/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/1.jpg" alt></p><h2 id="存储卷声明的管理过程"><a href="#存储卷声明的管理过程" class="headerlink" title="存储卷声明的管理过程"></a>存储卷声明的管理过程</h2><p>PersistantVolume 和 PersistantVolumeClaim 的管理过程描述如下：</p><blockquote><p>下图主要描述的是 PV 和 PVC 的管理过程，因为绘制空间的问题，将挂载点与Pod关联了，实际结构应该如上图所示：</p><ul><li>Pod 中添加数据卷，数据卷关联PVC</li><li>Pod 中包含容器，容器挂载数据卷</li></ul></blockquote><p><img src="/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/2.png" alt></p><h3 id="1-提供-Provisioning"><a href="#1-提供-Provisioning" class="headerlink" title="1.提供 Provisioning"></a>1.提供 Provisioning</h3><p>有两种方式为 PersistentVolumeClaim 提供 PersistentVolume : 静态、动态</p><ul><li><p><strong>静态提供 Static</strong></p><p>集群管理员实现创建好一系列 PersistentVolume，它们包含了可供集群中应用程序使用的关于实际存储的具体信息。</p></li><li><p><strong>动态提供 Dynamic</strong></p><p>在配置有合适的 StorageClass（存储类）且 PersistentVolumeClaim 关联了该 StorageClass 的情况下，kubernetes 集群可以为应用程序动态创建 PersistentVolume。</p></li></ul><h3 id="2-绑定-Binding"><a href="#2-绑定-Binding" class="headerlink" title="2.绑定 Binding"></a>2.绑定 Binding</h3><p>假设用户创建了一个 PersistentVolumeClaim 存储卷声明，并指定了需求的存储空间大小以及访问模式。Kubernets master 将立刻为其匹配一个 PersistentVolume 存储卷，并将存储卷声明和存储卷绑定到一起。如果一个 PersistentVolume 是动态提供给一个新的 PersistentVolumeClaim，Kubernetes master 会始终将其绑定到该 PersistentVolumeClaim。除此之外，应用程序将被绑定一个不小于（可能大于）其 PersistentVolumeClaim 中请求的存储空间大小的 PersistentVolume。一旦绑定，PersistentVolumeClaim 将拒绝其他 PersistentVolume 的绑定关系。PVC 与 PV 之间的绑定关系是一对一的映射。</p><p>PersistentVolumeClaim 将始终停留在 <strong><em>未绑定 unbound</em></strong> 状态，直到有合适的 PersistentVolume 可用。举个例子：集群中已经存在一个 50Gi 的 PersistentVolume，同时有一个 100Gi 的 PersistentVolumeClaim，在这种情况下，该 PVC 将一直处于 <strong><em>未绑定 unbound</em></strong> 状态，直到管理员向集群中添加了一个 100Gi 的 PersistentVolume。</p><h3 id="3-使用-Using"><a href="#3-使用-Using" class="headerlink" title="3.使用 Using"></a>3.使用 Using</h3><p>对于 Pod 容器组来说，PersistentVolumeClaim 存储卷声明是一种类型的 Volume 数据卷。Kubernetes 集群将 PersistentVolumeClaim 所绑定的 PersistentVolume 挂载到容器组供其使用。</p><h3 id="4-使用中保护-Storage-Object-in-Use-Protection"><a href="#4-使用中保护-Storage-Object-in-Use-Protection" class="headerlink" title="4.使用中保护 Storage Object in Use Protection"></a>4.使用中保护 Storage Object in Use Protection</h3><ul><li>使用中保护的目的是确保正在被容器组使用的 PersistentVolumeClaim 以及其绑定的 PersistentVolume 不能被系统删除，以避免可能的数据丢失。</li><li>如果用户删除一个正在使用中的 PersistentVolumeClaim，则该 PVC 不会立即被移除掉，而是推迟到该 PVC 不在被任何容器组使用时才移除；同样的如果管理员删除了一个已经绑定到 PVC 的 PersistentVolume，则该 PV 也不会立刻被移除掉，而是推迟到其绑定的 PVC 被删除后才移除掉</li></ul><h3 id="5-回收-Reclaiming"><a href="#5-回收-Reclaiming" class="headerlink" title="5.回收 Reclaiming"></a>5.回收 Reclaiming</h3><p>当用户不在需要其数据卷时，可以删除掉其 PersistentVolumeClaim，此时其对应的 PersistentVolume 将被集群回收并再利用。Kubernetes 集群根据 PersistentVolume 中的 reclaim policy（回收策略）决定在其被回收时做对应的处理。当前支持的回收策略有：Retained（保留）、Recycled（重复利用）、Deleted（删除）</p><ul><li><p><strong>保留 Retain</strong></p><p>保留策略需要集群管理员手工回收该资源。当绑定的 PersistentVolumeClaim 被删除后，PersistentVolume 仍然存在，并被认为是”已释放“。但是此时该存储卷仍然不能被其他 PersistentVolumeClaim 绑定，因为前一个绑定的 PersistentVolumeClaim 对应容器组的数据还在其中。集群管理员可以通过如下步骤回收该 PersistentVolume：</p><ul><li>删除该 PersistentVolume。PV 删除后，其数据仍然存在于对应的外部存储介质中（nfs、cefpfs、glusterfs 等）</li><li>手工删除对应存储介质上的数据</li><li>手工删除对应的存储介质，您也可以创建一个新的 PersistentVolume 并再次使用该存储介质</li></ul></li><li><p><strong>删除 Delete</strong></p><p>删除策略将从 kubernete 集群移除 PersistentVolume 以及其关联的外部存储介质（云环境中的 AWA EBS、GCE PD、Azure Disk 或 Cinder volume）。</p></li><li><p><strong>再利用 Recycle</strong></p><ul><li>再利用策略将在 PersistentVolume 回收时，执行一个基本的清除操作（rm -rf /thevolume/*），并使其可以再次被新的 PersistentVolumeClaim 绑定。</li><li>集群管理员也可以自定义一个 recycler pod template，用于执行清除操作。</li></ul></li></ul><h2 id="存储卷类型"><a href="#存储卷类型" class="headerlink" title="存储卷类型"></a>存储卷类型</h2><p>Kubernetes 支持 20 种存储卷类型，如下所示：</p><ul><li>非持久性存储<ul><li>emptyDir</li><li>HostPath (只在单节点集群上用做测试目的)</li></ul></li><li>网络连接性存储<ul><li>SAN：iSCSI、ScaleIO Volumes、FC (Fibre Channel)</li><li>NFS：nfs，cfs</li></ul></li><li>分布式存储<ul><li>Glusterfs</li><li>RBD (Ceph Block Device)</li><li>CephFS</li><li>Portworx Volumes</li><li>Quobyte Volumes</li></ul></li><li>云端存储<ul><li>GCEPersistentDisk</li><li>AWSElasticBlockStore</li><li>AzureFile</li><li>AzureDisk</li><li>Cinder (OpenStack block storage)</li><li>VsphereVolume</li><li>StorageOS</li></ul></li><li>自定义存储<ul><li>FlexVolume</li></ul></li><li>不推荐<ul><li>Flocker</li></ul></li></ul><h1 id="节点相关的数据卷限制"><a href="#节点相关的数据卷限制" class="headerlink" title="节点相关的数据卷限制"></a>节点相关的数据卷限制</h1><p>类似于 Google、Amazon、Microsoft 这样的云供应商，通常都会限定单个节点可挂载的数据卷的最大数量。Kubernetes 必须遵守这些限定，否则，当 Pod 调度上某节点上时，可能会因为不能实现数据卷挂载而启动不了。 </p><h2 id="自定义限制"><a href="#自定义限制" class="headerlink" title="自定义限制"></a>自定义限制</h2><p>修改此限制值的步骤如下：</p><ul><li>设置环境变量 <code>KUBE_MAX_PD_VOLS</code> 的取值</li><li>重启调度器 kube-scheduler</li></ul><p>建议不要将此数值设置得比默认值更大。在修改之前，请认真查询云供应商的相关文档，确保节点机器可以支持设置的限制取值。</p><p>该限定对整个集群生效，因此，将影响到集群中的所有节点。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; PV &amp;amp; PVC&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-数据卷Volume</title>
    <link href="http://yoursite.com/2020/04/16/K8s-%E6%95%B0%E6%8D%AE%E5%8D%B7Volume/"/>
    <id>http://yoursite.com/2020/04/16/K8s-%E6%95%B0%E6%8D%AE%E5%8D%B7Volume/</id>
    <published>2020-04-16T08:09:30.000Z</published>
    <updated>2020-04-17T02:55:30.993Z</updated>
    
    <content type="html"><![CDATA[<p>数据卷Volume及挂载</p> <a id="more"></a> <h1 id="一、数据卷Volume"><a href="#一、数据卷Volume" class="headerlink" title="一、数据卷Volume"></a>一、数据卷Volume</h1><h2 id="数据卷概述"><a href="#数据卷概述" class="headerlink" title="数据卷概述"></a>数据卷概述</h2><p>Kubernetes Volume（数据卷）主要解决了如下两方面问题：</p><ul><li>数据持久性：通常情况下，容器运行起来之后，写入到其文件系统的文件暂时性的。当容器崩溃后，kubelet 将会重启该容器，此时原容器运行后写入的文件将丢失，因为容器将重新从镜像创建。</li><li>数据共享：同一个 Pod（容器组）中运行的容器之间，经常会存在共享文件/文件夹的需求</li></ul><p>Docker 里同样也存在一个 volume（数据卷）的概念，但是 docker 对数据卷的管理相对 kubernetes 而言要更少一些。在 Docker 里，一个 Volume（数据卷）仅仅是宿主机（或另一个容器）文件系统上的一个文件夹。Docker 并不管理 Volume（数据卷）的生命周期。</p><p>在 Kubernetes 里，Volume（数据卷）存在明确的生命周期（与包含该数据卷的容器组相同）。因此，Volume（数据卷）的生命周期比同一容器组中任意容器的生命周期要更长，不管容器重启了多少次，数据都能被保留下来。当然，如果容器组退出了，数据卷也就自然退出了。此时，根据容器组所使用的 Volume（数据卷）类型不同，数据可能随数据卷的退出而删除，也可能被真正持久化，并在下次容器组重启时仍然可以使用。</p><p>从根本上来说，一个 Volume（数据卷）仅仅是一个可被容器组中的容器访问的文件目录（也许其中包含一些数据文件）。这个目录是怎么来的，取决于该数据卷的类型（不同类型的数据卷使用不同的存储介质）。</p><p>使用 Volume（数据卷）时，我们需要先在容器组中定义一个数据卷，并将其挂载到容器的挂载点上。容器中的一个进程所看到（可访问）的文件系统是由容器的 docker 镜像和容器所挂载的数据卷共同组成的。Docker 镜像将被首先加载到该容器的文件系统，任何数据卷都被在此之后挂载到指定的路径上。Volume（数据卷）不能被挂载到其他数据卷上，或者通过引用其他数据卷。同一个容器组中的不同容器各自独立地挂载数据卷，即同一个容器组中的两个容器可以将同一个数据卷挂载到各自不同的路径上。</p><p>我们现在通过下图来理解 容器组、容器、挂载点、数据卷、存储介质（nfs、PVC、ConfigMap）等几个概念之间的关系：</p><ul><li>一个容器组可以包含多个数据卷、多个容器</li><li>一个容器通过挂载点决定某一个数据卷被挂载到容器中的什么路径</li><li>不同类型的数据卷对应不同的存储介质</li></ul><h2 id="数据卷的常用类型"><a href="#数据卷的常用类型" class="headerlink" title="数据卷的常用类型"></a>数据卷的常用类型</h2><h3 id="emptyDir"><a href="#emptyDir" class="headerlink" title="emptyDir"></a>emptyDir</h3><ul><li><p><strong>描述</strong></p><p>emptyDir类型的数据卷在容器组被创建时分配给该容器组，并且直到容器组被移除，该数据卷才被释放。该数据卷初始分配时，始终是一个空目录。同一容器组中的不同容器都可以对该目录执行读写操作，并且共享其中的数据，（尽管不同的容器可能将该数据卷挂载到容器中的不同路径）。当容器组被移除时，emptyDir数据卷中的数据将被永久删除</p><p>容器崩溃时，kubelet 并不会删除容器组，而仅仅是将容器重启，因此 emptyDir 中的数据在容器崩溃并重启后，仍然是存在的。</p></li><li><p><strong>适用场景</strong></p><ul><li>空白的初始空间，例如合并/排序算法中，临时将数据存在磁盘上</li><li>长时间计算中存储检查点（中间结果），以便容器崩溃时，可以从上一次存储的检查点（中间结果）继续进行，而不是从头开始</li><li>作为两个容器的共享存储，使得第一个内容管理的容器可以将生成的页面存入其中，同时由一个 webserver 容器对外提供这些页面</li><li>默认情况下，emptyDir 数据卷被存储在 node（节点）的存储介质（机械硬盘、SSD、或者网络存储）上。此外，您可以设置 emptyDir.medium 字段为 “Memory”，此时 Kubernetes 将挂载一个 tmpfs（基于 RAM 的文件系统）。tmpfs 的读写速度非常快，但是与磁盘不一样，tmpfs 在节点重启后将被清空，且您向该 emptyDir 写入文件时，将消耗对应容器的内存限制。</li></ul></li></ul><h3 id="nfs"><a href="#nfs" class="headerlink" title="nfs"></a>nfs</h3><ul><li><p><strong>描述</strong></p><p>nfs 类型的数据卷可以加载 NFS（Network File System）到容器组/容器。容器组被移除时，将仅仅 umount（卸载）NFS 数据卷，NFS 中的数据仍将被保留。</p><ul><li>可以在加载 NFS 数据卷前就在其中准备好数据；</li><li>可以在不同容器组之间共享数据；</li><li>可以被多个容器组加载并同时读写；</li></ul></li><li><p><strong>适用场景</strong></p><ul><li>存储日志文件</li><li>MySQL的data目录（建议只在测试环境中）</li><li>用户上传的临时文件</li></ul></li></ul><h3 id="cephfs"><a href="#cephfs" class="headerlink" title="cephfs"></a>cephfs</h3><ul><li><p><strong>描述</strong></p><p>cephfs 数据卷可以挂载一个外部 CephFS 卷到容器组中。对于 kubernetes 而言，cephfs 与 nfs 的管理方式和行为完全相似，适用场景也相同。不同的仅仅是背后的存储介质。</p></li><li><p><strong>适用场景</strong></p><p>同 nfs 数据卷</p></li></ul><h3 id="configMap"><a href="#configMap" class="headerlink" title="configMap"></a>configMap</h3><ul><li><p><strong>描述</strong></p><p>ConfigMap 提供了一种向容器组注入配置信息的途径。ConfigMap 中的数据可以被 Pod（容器组）中的容器作为一个数据卷挂载。</p><p>在数据卷中引用 ConfigMap 时：</p><ul><li>可以直接引用整个 ConfigMap 到数据卷，此时 ConfigMap 中的每一个 key 对应一个文件名，value 对应该文件的内容</li><li>也可以只引用 ConfigMap 中的某一个名值对，此时可以将 key 映射成一个新的文件名</li></ul><p>将 ConfigMap 数据卷挂载到容器时，如果该挂载点指定了 <strong><em>数据卷内子路径</em></strong> （subPath），则该 ConfigMap 被改变后，该容器挂载的内容仍然不变。</p></li><li><p><strong>适用场景</strong></p><ul><li>使用 ConfigMap 中的某一 key 作为文件名，对应 value 作为文件内容，替换 nginx 容器中的 /etc/nginx/conf.d/default.conf 配置文件。</li></ul></li></ul><h3 id="secret"><a href="#secret" class="headerlink" title="secret"></a>secret</h3><ul><li><p><strong>描述</strong></p><p>secret 数据卷可以用来注入敏感信息（例如密码）到容器组。可以将敏感信息存入 kubernetes secret 对象，并通过 Volume（数据卷）以文件的形式挂载到容器组（或容器）。secret 数据卷使用 tmpfs（基于 RAM 的文件系统）挂载。</p><p>将 Secret 数据卷挂载到容器时，如果该挂载点指定了 <strong><em>数据卷内子路径</em></strong> （subPath），则该 Secret 被改变后，该容器挂载的内容仍然不变。</p></li><li><p><strong>适用场景</strong></p><ul><li>将 HTTPS 证书存入 kubernets secret，并挂载到 /etc/nginx/conf.d/myhost.crt、/etc/nginx/conf.d/myhost.pem 路径，用来配置 nginx 的 HTTPS 证书</li></ul></li></ul><h3 id="PersistentVolumeClaim"><a href="#PersistentVolumeClaim" class="headerlink" title="PersistentVolumeClaim"></a>PersistentVolumeClaim</h3><ul><li><p><strong>描述</strong></p><p>persistentVolumeClaim 数据卷用来挂载 PersistentVolume 存储卷。</p><p> PersistentVolume 存储卷为用户提供了一种在无需关心具体所在云环境的情况下”声明“ 所需持久化存储的方式。</p></li></ul><h1 id="二、数据卷-挂载"><a href="#二、数据卷-挂载" class="headerlink" title="二、数据卷-挂载"></a>二、数据卷-挂载</h1><p>挂载是指将定义在 Pod 中的数据卷关联到容器，同一个 Pod 中的同一个数据卷可以被挂载到该 Pod 中的多个容器上。</p><h2 id="数据卷内子路径"><a href="#数据卷内子路径" class="headerlink" title="数据卷内子路径"></a>数据卷内子路径</h2><p>同一个 Pod 的不同容器间共享数据卷。使用 <code>volumeMounts.subPath</code> 属性，可以使容器在挂载数据卷时指向数据卷内部的一个子路径，而不是直接指向数据卷的根路径。</p><p>下面的例子中，一个 LAMP（Linux Apache Mysql PHP）应用的 Pod 使用了一个共享数据卷，HTML 内容映射到数据卷的 <code>html</code> 目录，数据库的内容映射到了 <code>mysql</code> 目录： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-lamp-site</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">    <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">mysql</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MYSQL_ROOT_PASSWORD</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">"rootpasswd"</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/lib/mysql</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">        <span class="attr">subPath:</span> <span class="string">mysql</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">php</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">php:7.0-apache</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/www/html</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">        <span class="attr">subPath:</span> <span class="string">html</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">my-lamp-site-data</span></span><br></pre></td></tr></table></figure><h3 id="通过环境变量指定数据卷内子路径"><a href="#通过环境变量指定数据卷内子路径" class="headerlink" title="通过环境变量指定数据卷内子路径"></a>通过环境变量指定数据卷内子路径</h3><p>使用 <code>volumeMounts.subPathExpr</code> 字段，可以通过容器的环境变量指定容器内路径。使用此特性时，必须启用 <code>VolumeSubpathEnvExpansion</code>（自 Kubernetes v1.15 开始，是默认启用的。） </p><p>如下面的例子，该 Pod 使用 <code>subPathExpr</code> 在 hostPath 数据卷 <code>/var/log/pods</code> 中创建了一个目录 <code>pod1</code>（该参数来自于Pod的名字）。此时，宿主机目录 <code>/var/log/pods/pod1</code> 挂载到了容器的 <code>/logs</code> 路径： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container1</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAME</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">[</span> <span class="string">"sh"</span><span class="string">,</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt"</span> <span class="string">]</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir1</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/logs</span></span><br><span class="line">      <span class="attr">subPathExpr:</span> <span class="string">$(POD_NAME)</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir1</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/log/pods</span></span><br></pre></td></tr></table></figure><h2 id="容器内路径"><a href="#容器内路径" class="headerlink" title="容器内路径"></a>容器内路径</h2><p><code>mountPath</code> 数据卷被挂载到容器的路径，不能包含 <code>:</code></p><h2 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h2><p>容器对挂载的数据卷是否具备读写权限，如果 <code>readOnly</code> 为 <code>true</code>，则只读，否则可以读写（为 <code>false</code> 或者不指定）。默认为 <code>false</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据卷Volume及挂载&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>Kubernetes网络模型</title>
    <link href="http://yoursite.com/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-04-14T06:55:20.000Z</published>
    <updated>2020-04-16T07:54:50.876Z</updated>
    
    <content type="html"><![CDATA[<p>Container-to-Container 的网络</p><p>Pod-to-Pod 的网络</p><p>Pod-to-Service 的网络</p><p>Internet-to-Service 的网络</p><a id="more"></a> <h1 id="Kubernetes网络模型"><a href="#Kubernetes网络模型" class="headerlink" title="Kubernetes网络模型"></a>Kubernetes网络模型</h1><h2 id="一、Kubernetes基本概念"><a href="#一、Kubernetes基本概念" class="headerlink" title="一、Kubernetes基本概念"></a>一、Kubernetes基本概念</h2><p>Kubernetes 基于少数几个核心概念，不断完善，提供了非常丰富和实用的功能。本章节罗列了这些核心概念，并简要的做了概述，以便更好地支持后面的讨论。熟悉 Kubernetes 的读者可跳过这个章节。</p><h3 id="Kubernetes-API-Server"><a href="#Kubernetes-API-Server" class="headerlink" title="Kubernetes API Server"></a>Kubernetes API Server</h3><p>操作 Kubernetes 的方式，是调用 Kubernetes API Server（kube-apiserver）的 API 接口。kubectl、kubernetes dashboard、kuboard 都是通过调用 kube-apiserver 的接口实现对 kubernetes 的管理。API server 最终将集群状态的数据存储在 <a href="https://github.com/coreos/etcd" target="_blank" rel="noopener">etcd</a> 中。</p><h3 id="控制器Controller"><a href="#控制器Controller" class="headerlink" title="控制器Controller"></a>控制器Controller</h3><p>控制器（Controller）是 Kubernetes 中最核心的抽象概念。在用户通过 kube-apiserver 声明了期望的状态以后，控制器通过不断监控 apiserver 中的当前状态，并对当前状态与期望状态之间的差异做出反应，以确保集群的当前状态不断地接近用户声明的期望状态。这个过程实现在一个循环中，参考如下伪代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">while <span class="literal">true</span>:</span><br><span class="line">  X = currentState()</span><br><span class="line">  Y = desiredState()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> X == Y:</span><br><span class="line">    <span class="keyword">return</span>  # Do nothing</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    do(tasks to get to Y)</span><br></pre></td></tr></table></figure><p>例如，当你通过 API Server 创建一个新的 Pod 对象时，Kubernetes调度器（是一个控制器）注意到此变化，并做出将该 Pod 运行在集群中哪个节点的决定。然后，通过 API Server 修改 Pod 对象的状态。此时，对应节点上的kubelet（是一个控制器）注意到此变化，并将在其所在节点运行该 Pod，设置需要的网络，使 Pod 在集群内可以访问。此处，两个控制器针对不同的状态变化做出反应，以使集群的当前状态与用户指定的期望状态匹配。</p><h3 id="容器组Pod"><a href="#容器组Pod" class="headerlink" title="容器组Pod"></a>容器组Pod</h3><p>Pod 是 Kubernetes 中的最小可部署单元。一个 Pod 代表了集群中运行的一个工作负载，可以包括一个或多个 docker 容器、挂载需要的存储，并拥有唯一的 IP 地址。Pod 中的多个容器将始终在同一个节点上运行。</p><h3 id="节点Node"><a href="#节点Node" class="headerlink" title="节点Node"></a>节点Node</h3><p>节点是Kubernetes集群中的一台机器，可以是物理机，也可以是虚拟机。</p><h2 id="二、Kubernetes网络模型"><a href="#二、Kubernetes网络模型" class="headerlink" title="二、Kubernetes网络模型"></a>二、Kubernetes网络模型</h2><p>关于 Pod 如何接入网络这件事情，Kubernetes 做出了明确的选择。具体来说，Kubernetes 要求所有的网络插件实现必须满足如下要求：</p><ul><li>所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射（network address translation）</li><li>所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射</li><li>Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个</li></ul><p>在这些限制条件下，需要解决如下四种完全不同的网络使用场景的问题：</p><ol><li>Container-to-Container 的网络</li><li>Pod-to-Pod 的网络</li><li>Pod-to-Service 的网络</li><li>Internet-to-Service 的网络</li></ol><h2 id="三、Container-to-Container的网络"><a href="#三、Container-to-Container的网络" class="headerlink" title="三、Container-to-Container的网络"></a>三、Container-to-Container的网络</h2><p>Linux系统中，每一个进程都在一个 network namespace 中进行通信，network namespace 提供了一个逻辑上的网络堆栈（包含自己的路由、防火墙规则、网络设备）。换句话说，network namespace 为其中的所有进程提供了一个全新的网络堆栈。 </p><p>Linux 用户可以使用 <code>ip</code> 命令创建 network namespace。</p><p>例如，下面的命令创建了一个新的 network namespace 名称为 <code>ns1</code>：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns add ns1</span><br></pre></td></tr></table></figure><p>当创建 network namespace 时，同时将在 <code>/var/run/netns</code> 下创建一个挂载点（mount point）用于存储该 namespace 的信息。</p><p>执行 <code>ls /var/run/netns</code> 命令，或执行 <code>ip</code> 命令，可以查看所有的 network namespace： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls /var/run/netns</span><br><span class="line">ns1</span><br><span class="line">$ ip netns</span><br><span class="line">ns1</span><br></pre></td></tr></table></figure><p>默认情况下，Linux 将所有的进程都分配到 root network namespace，以使得进程可以访问外部网络，如下图所示： </p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/1.png" alt></p><p>在 Kubernetes 中，Pod 是一组 docker 容器的集合，这一组 docker 容器将共享一个 network namespace。Pod 中所有的容器都：</p><ul><li>使用该 network namespace 提供的同一个 IP 地址以及同一个端口空间</li><li>可以通过 localhost 直接与同一个 Pod 中的另一个容器通信</li></ul><p>Kubernetes 为每一个 Pod 都创建了一个 network namespace。具体做法是，把一个 Docker 容器当做 “Pod Container” 用来获取 network namespace，在创建 Pod 中新的容器时，都使用 docker run 的 <code>--network:container</code> 功能来加入该 network namespace。</p><p>如下图所示，每一个 Pod 都包含了多个 docker 容器（<code>ctr*</code>），这些容器都在同一个共享的 network namespace 中：</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/2.png" alt></p><p>此外，Pod 中可以定义数据卷，Pod 中的容器都可以共享这些数据卷，并通过挂载点挂载到容器内部不同的路径。</p><h2 id="四、Pod-to-Pod的网络"><a href="#四、Pod-to-Pod的网络" class="headerlink" title="四、Pod-to-Pod的网络"></a>四、Pod-to-Pod的网络</h2><p>在 Kubernetes 中，每一个 Pod 都有一个真实的 IP 地址，并且每一个 Pod 都可以使用此 IP 地址与 其他 Pod 通信。Pod-to-Pod 通信中使用真实 IP ，不管两个 Pod 是在同一个节点上，还是集群中的不同节点上。</p><p>从 Pod 的视角来看，Pod 是在其自身所在的 network namespace 与同节点上另外一个 network namespace 进程通信。在Linux上，不同的 network namespace 可以通过 <strong>Virtual Ethernet Device</strong> 或 <strong><em>veth pair</em></strong> (两块跨多个名称空间的虚拟网卡)进行通信。为连接 pod 的 network namespace，可以将 <strong><em>veth pair</em></strong> 的一段指定到 root network namespace，另一端指定到 Pod 的 network namespace。每一组 <strong><em>veth pair</em></strong> 类似于一条网线，连接两端，并可以使流量通过。节点上有多少个 Pod，就会设置多少组 <strong><em>veth pair</em></strong>。下图展示了 veth pair 连接 Pod 到 root namespace 的情况：</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/3.png" alt></p><p>此时，每个Pod 都有了自己的 network namespace，从 Pod 的角度来看，他们都有自己的以太网卡以及 IP 地址，并且都连接到了节点的 root network namespace。为了让 Pod 可以互相通过 root network namespace 通信，通过使用 network bridge（网桥）。</p><p>Linux Ethernet bridge 是一个虚拟的 Layer 2 网络设备，可用来连接两个或多个网段（network segment）。网桥的工作原理是，在源于目标之间维护一个转发表（forwarding table），通过检查通过网桥的数据包的目标地址（destination）和该转发表来决定是否将数据包转发到与网桥相连的另一个网段。桥接代码通过网络中具备唯一性的网卡MAC地址来判断是否桥接或丢弃数据。</p><p>网桥实现了 <code>ARP</code> 协议，以发现链路层与 IP 地址绑定的 MAC 地址。当网桥收到数据帧时，网桥将该数据帧广播到所有连接的设备上（除了发送者以外），对该数据帧做出相应的设备被记录到一个查找表中（lookup table）。后续网桥再收到发向同一个 IP 地址的流量时，将使用查找表（lookup table）来找到对应的 MAC 地址，并转发数据包。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/4.png" alt></p><h3 id="数据包的传递：Pod-to-Pod，同节点"><a href="#数据包的传递：Pod-to-Pod，同节点" class="headerlink" title="数据包的传递：Pod-to-Pod，同节点"></a>数据包的传递：Pod-to-Pod，同节点</h3><p>在 network namespace 将每一个 Pod 隔离到各自的网络堆栈的情况下，虚拟以太网设备（virtual Ethernet device）将每一个 namespace 连接到 root namespace，网桥将 namespace 又连接到一起，此时，Pod 可以向同一节点上的另一个 Pod 发送网络报文了。下图演示了同节点上，网络报文从一个Pod传递到另一个Pod的情况。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/5.gif" alt></p><p>Pod1 发送一个数据包到其自己的默认以太网设备 <code>eth0</code>。</p><ol><li>对 Pod1 来说，<code>eth0</code> 通过虚拟以太网设备（veth0）连接到 root namespace</li><li>网桥 <code>cbr0</code> 中为 <code>veth0</code> 配置了一个网段。一旦数据包到达网桥，网桥使用 <code>ARP</code> 协议解析出其正确的目标网段 <code>veth1</code></li><li>网桥 <code>cbr0</code> 将数据包发送到 <code>veth1</code></li><li>数据包到达 <code>veth1</code> 时，被直接转发到 Pod2 的 network namespace 中的 <code>eth0</code> 网络设备。</li></ol><p>在整个数据包传递过程中，每一个 Pod 都只和 <code>localhost</code> 上的 <code>eth0</code> 通信，且数包被路由到正确的 Pod 上。</p><p>Kubernetes 的网络模型规定，在跨节点的情况下 Pod 也必须可以通过 IP 地址访问。也就是说，Pod 的 IP 地址必须始终对集群中其他 Pod 可见；且从 Pod 内部和从 Pod 外部来看，Pod 的IP地址都是相同的。</p><h3 id="数据包的传递：Pod-to-Pod，跨节点"><a href="#数据包的传递：Pod-to-Pod，跨节点" class="headerlink" title="数据包的传递：Pod-to-Pod，跨节点"></a>数据包的传递：Pod-to-Pod，跨节点</h3><p>Kubernetes 网络模型要求 Pod 的 IP 在整个网络中都可访问，但是并不指定如何实现这一点。实际上，这是所使用网络插件相关的，但是，仍然有一些模式已经被确立了。</p><p>通常，集群中每个节点都被分配了一个 CIDR 网段，指定了该节点上的 Pod 可用的 IP 地址段。一旦发送到该 CIDR 网段的流量到达节点，就由节点负责将流量继续转发给对应的 Pod。下图展示了两个节点之间的数据报文传递过程。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/6.gif" alt></p><p>图中，目标 Pod（以绿色高亮）与源 Pod（以蓝色高亮）在不同的节点上，数据包传递过程如下：</p><ol><li>数据包从 Pod1 的网络设备 <code>eth0</code>，该设备通过 <code>veth0</code> 连接到 root namespace</li><li>数据包到达 root namespace 中的网桥 <code>cbr0</code></li><li>网桥上执行 ARP 将会失败，因为与网桥连接的所有设备中，没有与该数据包匹配的 MAC 地址。一旦 ARP 失败，网桥会将数据包发送到默认路由（root namespace 中的 <code>eth0</code> 设备）。此时，数据包离开节点进入网络</li><li>假设网络可以根据各节点的CIDR网段，将数据包路由到正确的节点</li><li>数据包进入目标节点的 root namespace（VM2 上的 <code>eth0</code>）后，通过网桥路由到正确的虚拟网络设备（<code>veth1</code>）</li><li>最终，数据包通过 <code>veth1</code> 发送到对应 Pod 的 <code>eth0</code>，完成了数据包传递的过程</li></ol><p>通常来说，每个节点知道如何将数据包分发到运行在该节点上的 Pod。一旦一个数据包到达目标节点，数据包的传递方式与同节点上不同Pod之间数据包传递的方式就是一样的了。</p><p>此处，我们直接跳过了如何配置网络，以使得数据包可以从一个节点路由到匹配的节点。这些是与具体的网络插件实现相关的。</p><p><code>Container Network Interface(CNI) plugin</code> 提供了一组通用 API 用来连接容器与外部网络。具体到容器化应用开发者来说，只需要了解在整个集群中，可以通过 Pod 的 IP 地址直接访问 Pod；网络插件是如何做到跨节点的数据包传递这件事情对容器化应用来说是透明的。</p><h2 id="五、Pod-to-Service的网络"><a href="#五、Pod-to-Service的网络" class="headerlink" title="五、Pod-to-Service的网络"></a>五、Pod-to-Service的网络</h2><p>Pod 可以通过 IP 地址之间传递数据包，但是，Pod 的 IP 地址并非是固定不变的，随着 Pod 的重新调度（例如水平伸缩、应用程序崩溃、节点重启等），Pod 的 IP 地址将会出现又消失。此时，Pod 的客户端无法得知该访问哪一个 IP 地址。Kubernetes 中，Service 的概念用于解决此问题。 </p><p>一个 Kubernetes Service 管理了一组 Pod 的状态，可以追踪一组 Pod 的 IP 地址的动态变化过程。一个 Service 拥有一个 IP 地址，并且充当了一组 Pod 的 IP 地址的“虚拟 IP 地址”。任何发送到 Service 的 IP 地址的数据包将被负载均衡到该 Service 对应的 Pod 上。在此情况下，Service 关联的 Pod 可以随时间动态变化，客户端只需要知道 Service 的 IP 地址即可（该地址不会发生变化）。</p><p>从效果上来说，Kubernetes 自动为 Service 创建和维护了集群内部的分布式负载均衡，可以将发送到 Service IP 地址的数据包分发到 Service 对应的健康的 Pod 上。</p><h3 id="netfilter-and-iptables"><a href="#netfilter-and-iptables" class="headerlink" title="netfilter and iptables"></a>netfilter and iptables</h3><p>Kubernetes 利用 Linux 内建的网络框架 - <code>netfilter</code> 来实现负载均衡。Netfilter 是由 Linux 提供的一个框架，可以通过自定义 handler 的方式来实现多种网络相关的操作。Netfilter 提供了许多用于数据包过滤、网络地址转换、端口转换的功能，通过这些功能，自定义的 handler 可以在网络上转发数据包、禁止数据包发送到敏感的地址等。</p><p><code>iptables</code> 是一个 user-space 应用程序，可以提供基于决策表的规则系统，以使用 netfilter 操作或转换数据包。在 Kubernetes 中，kube-proxy 控制器监听 apiserver 中的变化，并配置 iptables 规则。当 Service 或 Pod 发生变化时（例如 Service 被分配了 IP 地址，或者新的 Pod 被关联到 Service），kube-proxy 控制器将更新 iptables 规则，以便将发送到 Service 的数据包正确地路由到其后端 Pod 上。iptables 规则将监听所有发向 Service 的虚拟 IP 的数据包，并将这些数据包转发到该Service 对应的一个随机的可用 Pod 的 IP 地址，同时 iptables 规则将修改数据包的目标 IP 地址（从 Service 的 IP 地址修改为选中的 Pod 的 IP 地址）。当 Pod 被创建或者被终止时，iptables 的规则也被对应的修改。换句话说，iptables 承担了从 Service IP 地址到实际 Pod IP 地址的负载均衡的工作。</p><p>在返回数据包的路径上，数据包从目标 Pod 发出，此时，iptables 规则又将数据包的 IP 头从 Pod 的 IP 地址替换为 Service 的 IP 地址。从请求的发起方来看，就好像始终只是在和 Service 的 IP 地址通信一样。</p><h3 id="IPVS"><a href="#IPVS" class="headerlink" title="IPVS"></a>IPVS</h3><p>Kubernetes v1.11 开始，提供了另一个选择用来实现集群内部的负载均衡：IPVS。</p><p>IPVS（IP Virtual Server）也是基于 netfilter 构建的，在 Linux 内核中实现了传输层的负载均衡。</p><p>IPVS 被合并到 LVS（Linux Virtual Server）当中，充当一组服务器的负载均衡器。</p><p>IPVS 可以转发 TCP / UDP 请求到实际的服务器上，使得一组实际的服务器看起来像是只通过一个单一 IP 地址访问的服务一样。IPVS 的这个特点天然适合与用在 Kubernetes Service 的这个场景下。</p><p>当声明一个 Kubernetes Service 时，可以指定是使用 iptables 还是 IPVS 来提供集群内的负载均衡工作。</p><p>IPVS 是转为负载均衡设计的，并且使用更加有效率的数据结构（hash tables），相较于 iptables，可以支持更大数量的网络规模。当创建使用 IPVS 形式的 Service 时，Kubernetes 执行了如下三个操作：</p><ul><li>在节点上创建一个 dummy IPVS interface</li><li>将 Service 的 IP 地址绑定到该 dummy IPVS interface</li><li>为每一个 Service IP 地址创建 IPVS 服务器</li></ul><p>将来，IPVS 有可能成为 kubernetes 中默认的集群内负载均衡方式。这个改变将只影响到集群内的负载均衡，以 iptables 为例子，所有讨论对 IPVS 是同样适用。</p><h3 id="数据包的传递：Pod-to-Service"><a href="#数据包的传递：Pod-to-Service" class="headerlink" title="数据包的传递：Pod-to-Service"></a>数据包的传递：Pod-to-Service</h3><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/7.gif" alt></p><p>在 Pod 和 Service 之间路由数据包时，数据包的发起和以前一样：</p><ol><li>数据包首先通过 Pod 的 <code>eth0</code> 网卡发出</li><li>数据包经过虚拟网卡 <code>veth0</code> 到达网桥 <code>cbr0</code></li><li>网桥上的 APR 协议查找不到该 Service，所以数据包被发送到 root namespace 中的默认路由 - <code>eth0</code></li><li>此时，在数据包被 <code>eth0</code> 接受之前，数据包将通过 iptables 过滤。iptables 使用其规则（由 kube-proxy 根据 Service、Pod 的变化在节点上创建的 iptables 规则）重写数据包的目标地址（从 Service 的 IP 地址修改为某一个具体 Pod 的 IP 地址）</li><li>数据包现在的目标地址是 Pod 4，而不是 Service 的虚拟 IP 地址。iptables 使用 Linux 内核的 <code>conntrack</code> 工具包来记录具体选择了哪一个 Pod，以便可以将未来的数据包路由到同一个 Pod。简而言之，iptables 直接在节点上完成了集群内负载均衡的功能。数据包后续如何发送到 Pod 上，其路由方式与Pod-to-Pod的网络中相同。</li></ol><h3 id="数据包的传递：Service-to-Pod"><a href="#数据包的传递：Service-to-Pod" class="headerlink" title="数据包的传递：Service-to-Pod"></a>数据包的传递：Service-to-Pod</h3><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/8.gif" alt></p><ol><li>接收到此请求的 Pod 将会发送返回数据包，其中标记源 IP 为接收请求 Pod 自己的 IP，目标 IP 为最初发送对应请求的 Pod 的 IP</li><li>当数据包进入节点后，数据包将经过 iptables 的过滤，此时记录在 <code>conntrack</code> 中的信息将被用来修改数据包的源地址（从接收请求的 Pod 的 IP 地址修改为 Service 的 IP 地址）</li><li>然后，数据包将通过网桥、以及虚拟网卡 <code>veth0</code></li><li>最终到达 Pod 的网卡 <code>eth0</code></li></ol><h3 id="使用DNS"><a href="#使用DNS" class="headerlink" title="使用DNS"></a>使用DNS</h3><p>Kubernetes 也可以使用 DNS，以避免将 Service 的 cluster IP 地址硬编码到应用程序当中。Kubernetes DNS 是 Kubernetes 上运行的一个普通的 Service。每一个节点上的 <code>kubelet</code> 都使用该 DNS Service 来执行 DNS 名称的解析。集群中每一个 Service（包括 DNS Service 自己）都被分配了一个 DNS 名称。DNS 记录将 DNS 名称解析到 Service 的 ClusterIP 或者 Pod 的 IP 地址。SRV 记录用来指定 Service 的已命名端口。</p><p>DNS Pod 由三个不同的容器组成：</p><ul><li><code>kubedns</code>：观察 Kubernetes master 上 Service 和 Endpoints 的变化，并维护内存中的 DNS 查找表</li><li><code>dnsmasq</code>：添加 DNS 缓存，以提高性能</li><li><code>sidecar</code>：提供一个健康检查端点，可以检查 <code>dnsmasq</code> 和 <code>kubedns</code> 的健康状态</li></ul><p>DNS Pod 被暴露为 Kubernetes 中的一个 Service，该 Service 及其 ClusterIP 在每一个容器启动时都被传递到容器中（环境变量及 /etc/resolves），因此，每一个容器都可以正确的解析 DNS。DNS 条目最终由 <code>kubedns</code> 解析，<code>kubedns</code> 将 DNS 的所有信息都维护在内存中。<code>etcd</code> 中存储了集群的所有状态，<code>kubedns</code> 在必要的时候将 <code>etcd</code> 中的 key-value 信息转化为 DNS 条目信息，以重建内存中的 DNS 查找表。</p><p>CoreDNS 的工作方式与 <code>kubedns</code> 类似，但是通过插件化的架构构建，因而灵活性更强。自 Kubernetes v1.11 开始，CoreDNS 是 Kubernetes 中默认的 DNS 实现。</p><h2 id="六、Internet-to-Service的网络"><a href="#六、Internet-to-Service的网络" class="headerlink" title="六、Internet-to-Service的网络"></a>六、Internet-to-Service的网络</h2><ul><li>从集群内部访问互联网</li><li>从互联网访问集群内部</li></ul><h3 id="出方向-从集群内部访问互联网"><a href="#出方向-从集群内部访问互联网" class="headerlink" title="出方向 - 从集群内部访问互联网"></a>出方向 - 从集群内部访问互联网</h3><p>将网络流量从集群内的一个节点路由到公共网络是与具体网络以及实际网络配置紧密相关的。为了更加具体地讨论此问题，本文将使用 AWS VPC 来讨论其中的具体问题。</p><p>在 AWS，Kubernetes 集群在 VPC 内运行，在此处，每一个节点都被分配了一个内网地址（private IP address）可以从 Kubernetes 集群内部访问。为了使访问外部网络，通常会在 VPC 中添加互联网网关（Internet Gateway），以实现如下两个目的：</p><ul><li>作为 VPC 路由表中访问外网的目标地址</li><li>提供网络地址转换（NAT Network Address Translation），将节点的内网地址映射到一个外网地址，以使外网可以访问内网上的节点</li></ul><p>在有互联网网关（Internet Gateway）的情况下，虚拟机可以任意访问互联网。但是，存在一个小问题：Pod 有自己的 IP 地址，且该 IP 地址与其所在节点的 IP 地址不一样，并且，互联网网关上的 NAT 地址映射只能够转换节点（虚拟机）的 IP 地址，因为网关不知道每个节点（虚拟机）上运行了哪些 Pod （互联网网关不知道 Pod 的存在）。那么 Kubernetes 是如何使用 iptables 解决此问题的。</p><h4 id="数据包的传递：Node-to-Internet"><a href="#数据包的传递：Node-to-Internet" class="headerlink" title="数据包的传递：Node-to-Internet"></a>数据包的传递：Node-to-Internet</h4><p>下图中：</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/9.gif" alt></p><ol><li>数据包从 Pod 的 network namespace 发出</li><li>通过 <code>veth0</code> 到达虚拟机的 root network namespace</li><li>由于网桥上找不到数据包目标地址对应的网段，数据包将被网桥转发到 root network namespace 的网卡 <code>eth0</code>。在数据包到达 <code>eth0</code> 之前，iptables 将过滤该数据包。</li><li>在此处，数据包的源地址是一个 Pod，如果仍然使用此源地址，互联网网关将拒绝此数据包，因为其 NAT 只能识别与节点（虚拟机）相连的 IP 地址。因此，需要 iptables 执行源地址转换（source NAT），这样子，对互联网网关来说，该数据包就是从节点（虚拟机）发出的，而不是从 Pod 发出的</li><li>数据包从节点（虚拟机）发送到互联网网关</li><li>互联网网关再次执行源地址转换（source NAT），将数据包的源地址从节点（虚拟机）的内网地址修改为网关的外网地址，最终数据包被发送到互联网</li></ol><p>在回路径上，数据包沿着相同的路径反向传递，源地址转换（source NAT）在对应的层级上被逆向执行。</p><h3 id="入方向-从互联网访问Kubernetes"><a href="#入方向-从互联网访问Kubernetes" class="headerlink" title="入方向 - 从互联网访问Kubernetes"></a>入方向 - 从互联网访问Kubernetes</h3><ol><li>Service LoadBalancer</li><li>Ingress Controller</li></ol><h4 id="4-层：LoadBalancer"><a href="#4-层：LoadBalancer" class="headerlink" title="4 层：LoadBalancer"></a>4 层：LoadBalancer</h4><p>当创建 Kubernetes Service 时，可以指定其类型为 LoadBalancer。 LoadBalancer 的实现由 cloud controller 提供，cloud controller 可以调用云供应商 IaaS 层的接口，为 Kubernetes Service 创建负载均衡器（如果是自建 Kubernetes 集群，可以使用 NodePort 类型的 Service，并手动创建负载均衡器）。用户可以将请求发送到负载均衡器来访问 Kubernetes 中的 Service。</p><p>在 AWS，负载均衡器可以将网络流量分发到其目标服务器组（即 Kubernetes 集群中的所有节点）。一旦数据包到达节点，Service 的 iptables 规则将确保其被转发到 Service 的一个后端 Pod。</p><h4 id="数据包的传递：LoadBalancer-to-Service"><a href="#数据包的传递：LoadBalancer-to-Service" class="headerlink" title="数据包的传递：LoadBalancer-to-Service"></a>数据包的传递：LoadBalancer-to-Service</h4><p>接下来了解一下 Layer 4 的入方向访问具体是如何做到的：</p><ol><li>Loadbalancer 类型的 Service 创建后，cloud controller 将为其创建一个负载均衡器</li><li>负载均衡器只能直接和节点（虚拟机沟通），不知道 Pod 的存在，当数据包从请求方（互联网）到达 LoadBalancer 之后，将被分发到集群的节点上</li><li>节点上的 iptables 规则将数据包转发到合适的 Pod 上</li></ol><p>从 Pod 到请求方的相应数据包将包含 Pod 的 IP 地址，但是请求方需要的是负载均衡器的 IP 地址。iptables 和 <code>conntrack</code> 被用来重写返回路径上的正确的 IP 地址。</p><p>下图描述了一个负载均衡器和三个集群节点：</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/10.gif" alt></p><ol><li>请求数据包从互联网发送到负载均衡器</li><li>负载均衡器将数据包随机分发到其中的一个节点（虚拟机），此处，我们假设数据包被分发到了一个没有对应 Pod 的节点（VM2）上</li><li>在 VM2 节点上，kube-proxy 在节点上安装的 iptables 规则会将该数据包的目标地址判定到对应的 Pod 上（集群内负载均衡将生效）</li><li>iptables 完成 NAT 映射，并将数据包转发到目标 Pod</li></ol><h4 id="7-层：Ingress控制器"><a href="#7-层：Ingress控制器" class="headerlink" title="7 层：Ingress控制器"></a>7 层：Ingress控制器</h4><p>Layer 7 网络入方向访问在网络堆栈的 HTTP/HTTPS 协议层面工作，并且依赖于 KUbernetes Service。要实现 Layer 7 网络入方向访问，首先需要将 Service 指定为 <code>NodtePort</code> 类型，此时 Kubernetes master 将会为该 Service 分配一个<strong>节点端口</strong>，每一个节点上的 iptables 都会将此端口上的请求转发到 Service 的后端 Pod 上。此时，Service-to-Pod 的路由与数据包的传递：Service-to-Pod的描述相同。</p><p>接下来，创建一个 Kubernetes Ingress 对象可以将该 Service 发布到互联网。Ingress 是一个高度抽象的 HTTP 负载均衡器，可以将 HTTP 请求映射到 Kubernetes Service。在不同的 Kubernetes 集群中，Ingress 的具体实现可能是不一样的。与 Layer 4 的网络负载均衡器相似，HTTP 负载均衡器只理解节点的 IP 地址（而不是 Pod 的 IP 地址），因此，也同样利用了集群内部通过 iptables 实现的负载均衡特性。</p><p>在 AWS 中，ALB Ingress 控制器使用 Amazon 的 Layer 7 Application Load Balancer实现了 Kubernetes Ingress 的功能。下图展示了 AWS 上 Ingress 控制器的细节，也展示了网络请求是如何从 ALB 路由到 Kubernetes 集群的。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/11.png" alt></p><ol><li>ALB Ingress Controller 创建后，将监听 Kubernetes API 上关于 Ingress 的事件。当发现匹配的 Ingress 对象时，Ingress Controller 开始创建 AWS 资源</li><li>AWS 使用 Application Load Balancer（ALB）来满足 Ingress 对象的要求，并使用 Target Group 将请求路由到目标节点</li><li>ALB Ingress Controller 为 Kubernetes Ingress 对象中用到的每一个 Kubernetes Service 创建一个 AWS Target Group</li><li>Listener 是一个 ALB 进程，由 ALB Ingress Controller 根据 Ingress 的注解（annotations）创建，监听 ALB 上指定的协议和端口，并接收外部的请求</li><li>ALB Ingress Controller 还根据 Kubernetes Ingress 中的路径定义，创建了 Target Group Rule，确保指定路径上的请求被路由到合适的 Kubernetes Service</li></ol><h4 id="数据包的传递：Ingress-to-Service"><a href="#数据包的传递：Ingress-to-Service" class="headerlink" title="数据包的传递：Ingress-to-Service"></a>数据包的传递：Ingress-to-Service</h4><p>Ingress-to-Service 的数据包传递与 LoadBalancer-to-Service 的数据包传递非常相似。核心差别是：</p><ul><li>Ingress 能够解析 URL 路径（可基于路径进行路由）</li><li>Ingress 连接到 Service 的 NodePort</li></ul><p>下图展示了 Ingress-to-Service 的数据包传递过程。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/12.gif" alt></p><ol><li>创建 Ingress 之后，cloud controller 将会为其创建一个新的 Ingress Load Balancer</li><li>由于 Load Balancer 并不知道 Pod 的 IP 地址，当路由到达 Ingress Load Balancer 之后，会被转发到集群中的节点上（Service的节点端口）</li><li>节点上的 iptables 规则将数据包转发到合适的 Pod</li><li>Pod 接收到数据包</li></ol><p>从 Pod 返回的响应数据包将包含 Pod 的 IP 地址，但是请求客户端需要的是 Ingress Load Balancer 的 IP 地址。iptables 和 <code>conntrack</code> 被用来重写返回路径上的 IP 地址。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Container-to-Container 的网络&lt;/p&gt;
&lt;p&gt;Pod-to-Pod 的网络&lt;/p&gt;
&lt;p&gt;Pod-to-Service 的网络&lt;/p&gt;
&lt;p&gt;Internet-to-Service 的网络&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-网络策略</title>
    <link href="http://yoursite.com/2020/04/10/K8s-%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/"/>
    <id>http://yoursite.com/2020/04/10/K8s-%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/</id>
    <published>2020-04-10T08:59:31.000Z</published>
    <updated>2020-04-14T06:50:29.028Z</updated>
    
    <content type="html"><![CDATA[<p> Network Policies 网络策略</p><a id="more"></a> <h1 id="Network-Policies"><a href="#Network-Policies" class="headerlink" title="Network Policies"></a>Network Policies</h1><p>Kubernetes 中，Network Policy（网络策略）定义了一组 Pod 是否允许相互通信，或者与网络中的其他端点 endpoint 通信。</p><p><code>NetworkPolicy</code> 对象使用标签选择Pod，并定义规则指定选中的Pod可以执行什么样的网络通信</p><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>Network Policy 由网络插件实现，因此，使用的网络插件必须能够支持 <code>NetworkPolicy</code> 才可以使用此特性。如果仅仅是创建了一个 Network Policy 对象，但使用的网络插件并不支持此特性，所创建的 Network Policy 对象是不生效的。</p><h2 id="solated-Non-isolated-Pods"><a href="#solated-Non-isolated-Pods" class="headerlink" title="solated/Non-isolated Pods"></a>solated/Non-isolated Pods</h2><p>默认情况下，Pod 都是非隔离的（non-isolated），可以接受来自任何请求方的网络请求。</p><p>如果一个 NetworkPolicy 的标签选择器选中了某个 Pod，则该 Pod 将变成隔离的（isolated），并将拒绝任何不被 NetworkPolicy 许可的网络连接。（名称空间中其他未被 NetworkPolicy 选中的 Pod 将认可接受来自任何请求方的网络请求。）</p><p>Network Police 不会相互冲突，而是相互叠加的。如果多个 NetworkPolicy 选中了同一个 Pod，则该 Pod 可以接受这些 NetworkPolicy 当中任何一个 NetworkPolicy 定义的（入口/出口）规则，是所有NetworkPolicy规则的并集，因此，NetworkPolicy 的顺序并不重要，因为不会影响到最终的结果。</p><h2 id="NetworkPolicy对象"><a href="#NetworkPolicy对象" class="headerlink" title="NetworkPolicy对象"></a>NetworkPolicy对象</h2><p> 一个 NetworkPolicy 的 Example 如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-network-policy</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">role:</span> <span class="string">db</span></span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">172.17</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">        <span class="attr">except:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">172.17</span><span class="number">.1</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">project:</span> <span class="string">myproject</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">podSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">role:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">10.0</span><span class="number">.0</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">5978</span></span><br></pre></td></tr></table></figure><ul><li><p>基本信息： 同其他的 Kubernetes 对象一样，<code>NetworkPolicy</code> 需要 <code>apiVersion</code>、<code>kind</code>、<code>metadata</code> 字段</p></li><li><p>spec： <code>NetworkPolicy</code> 的 <code>spec</code> 字段包含了定义网络策略的主要信息：</p></li><li><ul><li>podSelector： 同名称空间中，符合此标签选择器 <code>.spec.podSelector</code> 的 Pod 都将应用这个 NetworkPolicy。上面的 Example中的 podSelector 选择了 role=db 的 Pod。如果该字段为空，则将对名称空间中所有的 Pod 应用这个 NetworkPolicy</li><li>policyTypes： .spec.policyTypes 是一个数组类型的字段，该数组中可以包含 Ingress、Egress 中的一个，也可能两个都包含。该字段标识了此 NetworkPolicy 是否应用到 入方向的网络流量、出方向的网络流量、或者两者都有。如果不指定 policyTypes 字段，该字段默认将始终包含 Ingress，当 NetworkPolicy 中包含出方向的规则时，Egress 也将被添加到默认值。</li><li>ingress： ingress 是一个数组，代表入方向的白名单规则。每一条规则都将允许与 from 和 ports 匹配的入方向的网络流量发生。例子中的 ingress 包含了一条规则，允许的入方向网络流量必须符合如下条件：</li></ul></li><li><ul><li><ul><li>Pod 的监听端口为 6379</li><li>请求方可以是如下三种来源当中的任意一种：</li></ul></li></ul></li><li><ul><li><ul><li><ul><li>ipBlock 为 172.17.0.0/16 网段，但是不包括 172.17.1.0/24 网段</li><li>namespaceSelector 标签选择器，匹配标签为 project=myproject</li><li>podSelector 标签选择器，匹配标签为 role=frontend</li></ul></li></ul></li></ul></li><li><ul><li>egress： egress 是一个数组，代表出方向的白名单规则。每一条规则都将允许与 to 和 ports 匹配的出方向的网络流量发生。例子中的 egress 允许的出方向网络流量必须符合如下条件：</li></ul></li><li><ul><li><ul><li>目标端口为 5978</li><li>目标 ipBlock  10.0.0.0/24网段</li></ul></li></ul></li></ul><p>因此，例子中的 NetworkPolicy 对网络流量做了如下限制：</p><ol><li>隔离了 default 名称空间中带有 role=db 标签的所有 Pod 的入方向网络流量和出方向网络流量</li><li>Ingress规则（入方向白名单规则）：</li></ol><ul><li><ul><li>当请求方是如下三种来源当中的任意一种时，允许访问 default 名称空间中所有带 role=db 标签的 Pod 的 6379 端口：</li></ul></li><li><ul><li><ul><li>ipBlock 为 172.17.0.0/16 网段，但是不包括 172.17.1.0/24 网段</li><li>namespaceSelector 标签选择器，匹配标签为 project=myproject</li><li>podSelector 标签选择器，匹配标签为 role=frontend</li></ul></li></ul></li></ul><ol><li>Egress rules（出方向白名单规则）：</li></ol><ul><li><ul><li>当如下条件满足时，允许出方向的网络流量：</li></ul></li><li><ul><li><ul><li>目标端口为 5978</li><li>目标 ipBlock 为 10.0.0.0/24 网段</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Network Policies 网络策略&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>通过Ingress访问应用程序</title>
    <link href="http://yoursite.com/2020/04/10/%E9%80%9A%E8%BF%87Ingress%E8%AE%BF%E9%97%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"/>
    <id>http://yoursite.com/2020/04/10/%E9%80%9A%E8%BF%87Ingress%E8%AE%BF%E9%97%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/</id>
    <published>2020-04-10T07:05:26.000Z</published>
    <updated>2020-04-10T08:32:27.837Z</updated>
    
    <content type="html"><![CDATA[<p> 用 Ingress 访问应用程序</p><a id="more"></a> <h1 id="通过-Ingress-访问应用程序"><a href="#通过-Ingress-访问应用程序" class="headerlink" title="通过 Ingress 访问应用程序"></a>通过 Ingress 访问应用程序</h1><h2 id="什么是-Ingress？"><a href="#什么是-Ingress？" class="headerlink" title="什么是 Ingress？"></a>什么是 Ingress？</h2><p>通常情况下，Service 和 Pod 的 IP 仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到 Service 在 Node 上暴露的 NodePort 上，然后再由 kube-proxy 通过边缘路由器 (edge router) 将其转发给相关的 Pod 或者丢弃。而 Ingress 就是为进入集群的请求提供路由规则的集合。</p><p>Ingress 可以给 Service 提供集群外部访问的 URL、负载均衡、SSL 终止、HTTP 路由等。为了配置这些 Ingress 规则，集群管理员需要部署一个 Ingress Controller，它监听 Ingress 和 Service 的变化，并根据规则配置负载均衡并提供访问入口。</p><h3 id="Pod-与-Ingress-的关系"><a href="#Pod-与-Ingress-的关系" class="headerlink" title="Pod 与 Ingress 的关系"></a>Pod 与 Ingress 的关系</h3><ul><li>通过label-selector相关联</li><li>通过Ingress Controller实现Pod的负载均衡</li></ul><p>-支持TCP/UDP 4层和HTTP 7层</p><p><img src="/2020/04/10/%E9%80%9A%E8%BF%87Ingress%E8%AE%BF%E9%97%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/1156961-20181225141932348-858339416.png" alt> </p><h3 id="Ingress-组成："><a href="#Ingress-组成：" class="headerlink" title="Ingress 组成："></a>Ingress 组成：</h3><ul><li>ingress controller：将新加入的Ingress转化成Nginx的配置文件并使之生效；</li><li>ingress服务：将Nginx的配置抽象成一个Ingress对象，每添加一个新的服务只需写一个新的Ingress的yaml文件即可；</li></ul><h3 id="Ingress-工作原理："><a href="#Ingress-工作原理：" class="headerlink" title="Ingress 工作原理："></a>Ingress 工作原理：</h3><p>ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，再写到nginx-ingress-control的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。</p><h2 id="实战：通过-Ingress-使的应用程序在互联网可用"><a href="#实战：通过-Ingress-使的应用程序在互联网可用" class="headerlink" title="实战：通过 Ingress 使的应用程序在互联网可用"></a>实战：通过 Ingress 使的应用程序在互联网可用</h2><p> <strong>创建文件 nginx-deployment.yaml</strong> </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br></pre></td></tr></table></figure><p> <strong>创建文件 nginx-service.yaml</strong> </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-port</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">32600</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br></pre></td></tr></table></figure><p> <strong>创建文件 nginx-ingress.yaml</strong> </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-ingress-for-nginx</span>  <span class="comment"># Ingress 的名字，仅用于标识</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span>                      <span class="comment"># Ingress 中定义 L7 路由规则</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">www.test.com</span>   <span class="comment"># 根据 virtual hostname 进行路由（请使用您自己的域名）</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span>                  <span class="comment"># 按路径进行路由</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">nginx-service</span>  <span class="comment"># 指定后端的 Service 为之前创建的 nginx-service</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p> <strong>执行命令</strong> </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx-deployment.yaml</span><br><span class="line">kubectl apply -f nginx-service.yaml</span><br><span class="line">kubectl apply -f nginx-ingress.yaml</span><br></pre></td></tr></table></figure><p> <strong>检查执行结果</strong> </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ingress]<span class="comment"># kubectl get ingress -o wide</span></span><br><span class="line">NAME                   HOSTS               ADDRESS   PORTS   AGE</span><br><span class="line">my-ingress-for-nginx   www.test.com             80      39m</span><br><span class="line"></span><br><span class="line"><span class="comment">#从互联网访问</span></span><br><span class="line">[root@k8s-master ingress]<span class="comment"># curl www.test.com</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 用 Ingress 访问应用程序&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-Service连接应用程序</title>
    <link href="http://yoursite.com/2020/04/08/K8s-Service%E8%BF%9E%E6%8E%A5%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"/>
    <id>http://yoursite.com/2020/04/08/K8s-Service%E8%BF%9E%E6%8E%A5%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/</id>
    <published>2020-04-08T08:00:56.000Z</published>
    <updated>2020-04-10T03:36:38.870Z</updated>
    
    <content type="html"><![CDATA[<p> 创建/访问/Service，保护 Service 安全，暴露 Service；</p><a id="more"></a> <h1 id="Service连接应用程序"><a href="#Service连接应用程序" class="headerlink" title="Service连接应用程序"></a>Service连接应用程序</h1><h2 id="Kubernetes-的网络模型"><a href="#Kubernetes-的网络模型" class="headerlink" title="Kubernetes 的网络模型"></a>Kubernetes 的网络模型</h2><p>通常，Docker 使用一种 <code>host-private</code> 的联网方式，在此情况下，只有两个容器都在同一个节点（主机）上时，一个容器才可以通过网络连接另一个容器。为了使 Docker 容器可以跨节点通信，必须在宿主节点（主机）的 IP 地址上分配端口，并将该端口接收到的网络请求转发（或代理）到容器中。这意味着，用户必须非常小心地为容器分配宿主节点（主机）的端口号，或者端口号可以自动分配。</p><p>在一个集群中，多个开发者之间协调分配端口号是非常困难的。Kubernetes 认为集群中的两个 Pod 应该能够互相通信，无论他们各自在哪个节点上。每一个 Pod 都被分配自己的 <strong>“cluster-private-IP”</strong>，因此，无需在 Pod 间建立连接，或者将容器的端口映射到宿主机的端口。因此：</p><ul><li>Pod 中的任意容器可以使用 localhost 直连同 Pod 中另一个容器的端口</li><li>集群中的任意 Pod 可以使用另一的 Pod 的 <strong>cluster-private-IP</strong> 直连对方的端口，（无需 NAT 映射）</li></ul><h2 id="在集群中部署-Pod"><a href="#在集群中部署-Pod" class="headerlink" title="在集群中部署 Pod"></a>在集群中部署 Pod</h2><p> 创建文件 <code>run-my-nginx.yaml</code>，文件内容如下 </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p> 执行以下命令，部署 Pod 并检查运行情况： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl apply -f  run-my-nginx.yaml </span><br><span class="line">deployment.apps&#x2F;my-nginx created</span><br><span class="line"></span><br><span class="line">[root@k8s-master 0408]# kubectl get pods -l run&#x3D;my-nginx -o wide</span><br><span class="line">NAME                        READY   STATUS         RESTARTS   AGE   IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">my-nginx-75897978cd-6vfrl   0&#x2F;1     ErrImagePull   0          39s   10.244.235.206   k8s-master    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">my-nginx-75897978cd-p2tcd   1&#x2F;1     Running        0          39s   10.244.44.235    k8s-node-02   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>执行命令 <code>kubectl get pods -l run=my-nginx -o yaml | grep podIP</code>， 检查 Pod 的 IP 地址，输出结果如下： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl get pods -l run&#x3D;my-nginx -o yaml | grep podIP</span><br><span class="line">      cni.projectcalico.org&#x2F;podIP: 10.244.235.206&#x2F;32</span><br><span class="line">    podIP: 10.244.235.206</span><br><span class="line">    podIPs:</span><br><span class="line">      cni.projectcalico.org&#x2F;podIP: 10.244.44.235&#x2F;32</span><br><span class="line">    podIP: 10.244.44.235</span><br><span class="line">    podIPs:</span><br></pre></td></tr></table></figure><p>在集群中的任意节点上，您可以执行 <code>curl 10.244.235.206 或</code>curl 10.244.44.235` 获得 nginx 的响应。此时：</p><ul><li>容器并没有使用节点上的 80 端口</li><li>没有使用 NAT 规则对容器端口进行映射</li></ul><p>这意味着，您可以</p><ul><li>在同一节点上使用 80 端口运行多个 nginx Pod</li><li>在集群的任意节点/Pod 上使用 nginx Pod 的 clusterIP 访问 nginx 的 80 端口</li></ul><p>同 Docker 一样，Kubernets 中，仍然可以将 Pod 的端口映射到宿主节点的网络地址上（使用 nodePort），但是使用 Kubernetes 的网络模型时，这类需求已经大大减少了。</p><h1 id="创建-Service"><a href="#创建-Service" class="headerlink" title="创建 Service"></a>创建 Service</h1><p>Pod 因为故障或其他原因终止后，Deployment Controller 将创建一个新的 Pod 以替代该 Pod，但是 IP 地址将发生变化。Kubernetes Service 解决了这样的问题。</p><p>Kubernetes Service：</p><ul><li>定义了集群中一组 Pod 的逻辑集合，该集合中的 Pod 提供了相同的功能</li><li>被创建后，获得一个唯一的 IP 地址（ClusterIP）。直到该 Service 被删除，此地址不会发生改变</li><li>Pod 可以直接连接 Service IP 地址上的端口，且发送到该 IP 地址的网络请求被自动负载均衡分发到 Service 所选取的 Pod 集合中</li></ul><p>执行命令 <code>kubectl expose deployment/my-nginx</code> 可以为上面的两个 nginx Pod 创建 Service，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl expose deployment&#x2F;my-nginx</span><br><span class="line">service&#x2F;my-nginx exposed</span><br></pre></td></tr></table></figure><p> 该命令等价于 <code>kubectl apply -f nginx-svc.yaml</code>，其中 nginx-svc.yaml 文件的内容如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br></pre></td></tr></table></figure><p>该 yaml 文件将创建一个 Service：</p><ul><li>该 Service 通过 label selector 选取包含 <code>run: my-nginx</code> 标签的 Pod 作为后端 Pod</li><li>该 Service 暴露一个端口 80（<code>spec.ports[*].port</code>）</li><li>该 Service 将 80 端口上接收到的网络请求转发到后端 Pod 的 80 （<code>spec.ports[*].targetPort</code>）端口上，支持负载均衡</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl get svc my-nginx</span><br><span class="line">NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">my-nginx   ClusterIP   10.97.72.97   &lt;none&gt;        80&#x2F;TCP    28m</span><br></pre></td></tr></table></figure><p>Service 的后端 Pod 实际上通过 <code>Endpoints</code> 来暴露。Kubernetes 会持续检查 Service 的 label selector <code>spec.selector</code>，并将符合条件的 Pod 更新到与 Service 同名（my-nginx）的 Endpoints 对象。如果 Pod 终止了，该 Pod 将被自动从 Endpoints 中移除，新建的 Pod 将自动被添加到该 Endpoint。 </p><p>执行命令 <code>kubectl describe svc my-nginx</code>，输出结果如下，请注意 Endpoints 中的 IP 地址与上面获得的 Pod 地址相同： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl describe svc my-nginx</span><br><span class="line">Name:              my-nginx</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          run&#x3D;my-nginx</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.97.72.97</span><br><span class="line">Port:              &lt;unset&gt;  80&#x2F;TCP</span><br><span class="line">TargetPort:        80&#x2F;TCP</span><br><span class="line">Endpoints:         10.244.154.209:80,10.244.44.236:80</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none</span><br></pre></td></tr></table></figure><p> 执行命令 <code>kubectl get ep my-nginx</code>，输出结果如下： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl get ep my-nginx</span><br><span class="line">NAME       ENDPOINTS                            AGE</span><br><span class="line">my-nginx   10.244.154.209:80,10.244.44.236:80   38m</span><br></pre></td></tr></table></figure><p> 此时，可以在集群的任意节点上执行 <code>curl 10.0.162.149:80</code>，通过 Service 的 ClusterIP:Port 访问 nginx。 </p><p> Service 的 IP 地址是虚拟地址。 </p><h1 id="访问-Service"><a href="#访问-Service" class="headerlink" title="访问 Service"></a>访问 Service</h1><p>Kubernetes 支持两种方式发现服务：</p><ul><li>环境变量</li><li>DNS</li></ul><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>针对每一个有效的 Service，kubelet 在创建 Pod 时，向 Pod 添加一组环境变量。这种做法引发了一个 Pod 和 Service 的顺序问题。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]<span class="comment"># kubectl exec my-nginx-75897978cd-4t7zb  -- printenv | grep SERVICE</span></span><br><span class="line">MY_SERVICE_PORT_80_TCP_PORT=80</span><br><span class="line">MY_SERVICE_PORT=tcp://10.100.240.247:80</span><br><span class="line">MY_SERVICE_SERVICE_HOST=10.100.240.247</span><br><span class="line">MY_SERVICE_PORT_80_TCP=tcp://10.100.240.247:80</span><br><span class="line">KUBERNETES_SERVICE_PORT_HTTPS=443</span><br><span class="line">KUBERNETES_SERVICE_PORT=443</span><br><span class="line">NGINX_DEP_SERVICE_PORT_80_80=80</span><br><span class="line">MY_SERVICE_PORT_80_TCP_ADDR=10.100.240.247</span><br><span class="line">MYAPP_SERVICE_PORT_80_80=80</span><br><span class="line">MY_SERVICE_SERVICE_PORT=80</span><br><span class="line">MYAPP_SERVICE_HOST=10.102.48.254</span><br><span class="line">MYAPP_SERVICE_PORT=80</span><br><span class="line">NGINX_DEP_SERVICE_HOST=10.98.115.52</span><br><span class="line">NGINX_DEP_SERVICE_PORT=80</span><br><span class="line">MY_SERVICE_PORT_80_TCP_PROTO=tcp</span><br><span class="line">KUBERNETES_SERVICE_HOST=10.96.0.1</span><br></pre></td></tr></table></figure><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><p>Kubernetes 提供了一个 DNS cluster addon，可自动为 Service 分配 DNS name。该 addon 已经默认安装。</p><p>执行命令 <code>kubectl get services kube-dns --namespace=kube-system</code> 查看该 addon 在集群上是否可用，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl get services kube-dns --namespace&#x3D;kube-system</span><br><span class="line">NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53&#x2F;UDP,53&#x2F;TCP,9153&#x2F;TCP   13d</span><br></pre></td></tr></table></figure><p>此时，可以从集群中任何 Pod 中按 Service 的名称访问该 Service。</p><ul><li>执行命令 <code>kubectl run curl --image=radial/busyboxplus:curl -i --tty</code> 获得 busyboxplus 容器的命令行终端，该命令输出结果如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]<span class="comment"># kubectl run curl --image=radial/busyboxplus:curl -i --tty</span></span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed <span class="keyword">in</span> a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">If you don<span class="string">'t see a command prompt, try pressing enter.</span></span><br></pre></td></tr></table></figure><ul><li>执行命令 <code>nslookup my-nginx</code>，输出结果如下所示： </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[ root@curl-69c656fd45-848f6:/ ]$ nslookup my-nginx</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-nginx</span><br><span class="line">Address 1: 10.97.72.97 my-nginx.default.svc.cluster.local</span><br></pre></td></tr></table></figure><ul><li><p>执行命令 <code>curl my-nginx:80</code>，可获得 Nginx 的响应。 </p></li><li><p>执行命令 <code>kubectl delete deployment curl</code> 可删除刚才创建的 <code>curl</code> 测试容器 </p></li></ul><h1 id="保护-Service-的安全"><a href="#保护-Service-的安全" class="headerlink" title="保护 Service 的安全"></a>保护 Service 的安全</h1><p>在将该 Service 公布到互联网时，可能需要确保该通信渠道是安全的。为此：</p><ul><li>准备 https 证书（购买，或者自签名）</li><li>将该 nginx 服务配置好，并使用该 https 证书</li><li>配置 Secret，以使得其他 Pod 可以使用该证书</li></ul><p>配置 nginx 使用自签名证书：</p><ul><li>创建密钥对 </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]<span class="comment"># openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/nginx.key -out /tmp/nginx.crt -subj "/CN=my-nginx/O=my-nginx"</span></span><br><span class="line">Generating a 2048 bit RSA private key</span><br><span class="line">.............................................+++</span><br><span class="line">.............................................+++</span><br><span class="line">writing new private key to <span class="string">'/tmp/nginx.key'</span></span><br></pre></td></tr></table></figure><ul><li>将密钥对转换为 base64 编码</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /tmp/nginx.crt | base64</span><br><span class="line">cat /tmp/nginx.key | base64</span><br></pre></td></tr></table></figure><ul><li>创建一个如下格式的 nginxsecrets.yaml 文件，使用前面命令输出的 base64 编码替换其中的内容（base64编码内容不能换行） </li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"v1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"Secret"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">"nginxsecret"</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">nginx.crt:</span> <span class="string">""</span></span><br><span class="line">  <span class="attr">nginx.key:</span> <span class="string">""</span></span><br></pre></td></tr></table></figure><ul><li>使用该文件创建 Secrets </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建 Secrets</span></span><br><span class="line">[root@k8s-master 0408]<span class="comment"># kubectl apply -f 1.yaml </span></span><br><span class="line">secret/nginxsecret created</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看 Secrets</span></span><br><span class="line">[root@k8s-master 0408]<span class="comment"># kubectl get secrets</span></span><br><span class="line">NAME                  TYPE                                  DATA   AGE</span><br><span class="line">default-token-xws5p   kubernetes.io/service-account-token   3      13d</span><br><span class="line">nginxsecret           Opaque                                2      38s</span><br></pre></td></tr></table></figure><ul><li>修改 nginx 部署，使 nginx 使用 Secrets 中的 https 证书，修改 Service，使其暴露 80 端口和 443端口。nginx-secure-app.yaml 文件如下所示： </li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">        <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">nginxsecret</span>       <span class="comment">#####</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginxhttps</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">bprashanth/nginxhttps:1.0</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">443</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/ssl</span>      <span class="comment">#####</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">secret-volume</span></span><br></pre></td></tr></table></figure><p>关于 nginx-secure-app.yaml</p><ul><li>该文件同时包含了 Deployment 和 Service 的定义</li><li>nginx server 监听 HTTP 80 端口和 HTTPS 443 端口的请求， nginx Service 同时暴露了这两个端口</li><li>nginx 容器可以通过 <code>/etc/nginx/ssl</code> 访问到 https 证书，https 证书存放在 Secrets 中，且必须在 Pod 创建之前配置好。</li></ul><p>执行命令使该文件生效：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete deployments,svc my-nginx</span><br><span class="line">kubectl create -f ./nginx-secure-app.yaml</span><br></pre></td></tr></table></figure><p>此时，可以从任何节点访问该 nginx server</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o yaml | grep -i podip</span><br><span class="line">    podIP: 10.244.3.5</span><br><span class="line">node $ curl -k https://10.244.3.5</span><br><span class="line">...</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br></pre></td></tr></table></figure><p> 创建 curlpod.yaml 文件，内容如下： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">curl-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">curlpod</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">curlpod</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">        <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">nginxsecret</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">curlpod</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">while</span> <span class="literal">true</span><span class="string">;</span> <span class="string">do</span> <span class="string">sleep</span> <span class="number">1</span><span class="string">;</span> <span class="string">done</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">radial/busyboxplus:curl</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/ssl</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">secret-volume</span></span><br></pre></td></tr></table></figure><ul><li>执行命令，完成 curlpod 的部署 </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl apply -f curlpod.yaml </span><br><span class="line">deployment.apps&#x2F;curl-deployment created</span><br><span class="line"></span><br><span class="line">[root@k8s-master 0408]# kubectl get pods -l app&#x3D;curlpod</span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">curl-deployment-f8c5c685b-cwqrp   1&#x2F;1     Running   0          17s</span><br><span class="line"></span><br><span class="line">#执行 curl，访问 nginx 的 https 端口</span><br><span class="line">[root@k8s-master 0408]# kubectl exec curl-deployment-f8c5c685b-cwqrp -- curl https:&#x2F;&#x2F;my-nginx --cacert &#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;nginx.crt</span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--</span><br></pre></td></tr></table></figure><h1 id="暴露-Service"><a href="#暴露-Service" class="headerlink" title="暴露 Service"></a>暴露 Service</h1><p>在应用程序中，可能有一部分功能需要通过 Service 发布到一个外部的 IP 地址上。Kubernetes 支持如下两种方式：</p><ul><li>NodePort</li><li>LoadBalancer<ul><li>需要云环境支持</li></ul></li></ul><p><strong>执行命令查看service暴露的外部端口</strong></p><ul><li><pre><code>[root@k8s-master 0408]# kubectl get svc my-nginx -o yaml | grep nodePort -C 5spec:  clusterIP: 10.107.66.92  externalTrafficPolicy: Cluster  ports:  - name: http    nodePort: 30407      ###        port: 80    protocol: TCP    targetPort: 80  - name: https    nodePort: 30534      ###    port: 443    protocol: TCP    targetPort: 443      selector:    run: my-nginx<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">假设某一节点的公网 IP 地址为 23.251.152.56，可以在任意一台可上网的机器执行命令 &#96;curl https:&#x2F;&#x2F;23.251.152.56:30407 -k&#96;。输出结果为：</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;text</span><br><span class="line">...</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br></pre></td></tr></table></figure></code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 创建/访问/Service，保护 Service 安全，暴露 Service；&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-Service</title>
    <link href="http://yoursite.com/2020/04/08/K8s-Service/"/>
    <id>http://yoursite.com/2020/04/08/K8s-Service/</id>
    <published>2020-04-08T05:30:32.000Z</published>
    <updated>2020-04-08T07:59:36.978Z</updated>
    
    <content type="html"><![CDATA[<p> Service的功能及作用</p><a id="more"></a> <h1 id="一、Service概述"><a href="#一、Service概述" class="headerlink" title="一、Service概述"></a>一、Service概述</h1><h2 id="为何需要-Service"><a href="#为何需要-Service" class="headerlink" title="为何需要 Service"></a>为何需要 Service</h2><p>Kubernetes 中 Pod 是随时可以消亡的（节点故障、容器内应用程序错误等原因）。如果使用 Deployment运行应用程序，Deployment 将会在 Pod 消亡后再创建一个新的 Pod 以维持所需要的副本数。每一个 Pod 有自己的 IP 地址，然而，对于 Deployment 而言，对应 Pod 集合是动态变化的。</p><p>这个现象导致了如下问题：</p><ul><li>如果某些 Pod（假设是 ‘backends’）为另外一些 Pod（假设是 ‘frontends’）提供接口，在 ‘backends’ 中的 Pod 集合不断变化（IP 地址也跟着变化）的情况下，’frontends’ 中的 Pod 如何才能知道应该将请求发送到哪个 IP 地址？</li></ul><p>Service 存在的意义，就是为了解决这个问题</p><h2 id="Kubernetes-Service"><a href="#Kubernetes-Service" class="headerlink" title="Kubernetes Service"></a>Kubernetes Service</h2><p>Kubernetes 中 Service 是一个 API 对象，通过 kubectl + YAML，定义一个 Service，可以将符合 Service 指定条件的 Pod 作为可通过网络访问的服务提供给服务调用者。</p><p>Service 是 Kubernetes 中的一种服务发现机制：</p><ul><li>Pod 有自己的 IP 地址</li><li>Service 被赋予一个唯一的 dns name</li><li>Service 通过 label selector 选定一组 Pod</li><li>Service 实现负载均衡，可将请求均衡分发到选定这一组 Pod 中</li></ul><p>例如，假设有一个无状态的图像处理后端程序运行了 3 个 Pod 副本。这些副本是相互可替代的（前端程序调用其中任何一个都可以）。在后端程序的副本集中的 Pod 经常变化（销毁、重建、扩容、缩容等）的情况下，前端程序不应该关注这些变化。</p><p>Kubernetes 通过引入 Service 的概念，将前端与后端解耦</p><h1 id="二、Service详细描述"><a href="#二、Service详细描述" class="headerlink" title="二、Service详细描述"></a>二、Service详细描述</h1><h2 id="创建-Service"><a href="#创建-Service" class="headerlink" title="创建 Service"></a>创建 Service</h2><p>Kubernetes Servies 是一个 RESTFul 接口对象，可通过 yaml 文件创建。</p><p>例如，假设您有一组 Pod：</p><ul><li>每个 Pod 都监听 9376 TCP 端口</li><li>每个 Pod 都有标签 app=MyApp</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">MyApp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>上述 YAML 文件可用来创建一个 Service：</p><ul><li>名字为 <code>my-service</code></li><li>目标端口为 TCP 9376</li><li>选取所有包含标签 app=MyApp 的 Pod</li></ul><h3 id="关于-Service："><a href="#关于-Service：" class="headerlink" title="关于 Service："></a>关于 Service：</h3><ul><li><p>Kubernetes 将为该 Service 分配一个 IP 地址（ClusterIP 或 集群内 IP），供 Service Proxy 使用。</p></li><li><p>Kubernetes 将不断扫描符合该 selector 的 Pod，并将最新的结果更新到与 Service 同名 <code>my-service</code> 的 Endpoint 对象中。</p></li><li><p>Service 从自己的 IP 地址和 <code>port</code> 端口接收请求，并将请求映射到符合条件的 Pod 的 <code>targetPort</code>。为了方便，默认 <code>targetPort</code> 的取值 与 <code>port</code> 字段相同 </p></li><li><p>Pod 的定义中，Port 可能被赋予了一个名字，可以在 Service 的 <code>targetPort</code> 字段引用这些名字，而不是直接写端口号。这种做法可以使得将来修改后端程序监听的端口号，而无需影响到前端程序。</p></li><li><p>Service 的默认传输协议是 TCP，也可以使用其他支持的传输协议。</p></li><li><p>Kubernetes Service 中，可以定义多个端口，不同的端口可以使用相同或不同的传输协议。</p></li></ul><h2 id="创建-Service（无-label-selector）"><a href="#创建-Service（无-label-selector）" class="headerlink" title="创建 Service（无 label selector）"></a>创建 Service（无 label selector）</h2><p>Service 通常用于提供对 Kubernetes Pod 的访问，但是也可以将其用于任何其他形式的后端。例如：</p><ul><li>在生产环境中使用一个 Kubernetes 外部的数据库集群，在测试环境中使用 Kubernetes 内部的 数据库</li><li>将 Service 指向另一个名称空间中的 Service，或者另一个 Kubernetes 集群中的 Service</li><li>将程序迁移到 Kubernetes，但是根据迁移路径，只将一部分后端程序运行在 Kubernetes 中</li></ul><p>在上述这些情况下，可以定义一个没有 Pod Selector 的 Service。例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>因为该 Service 没有 selector，相应的 Endpoint 对象就无法自动创建。可以手动创建一个 Endpoint 对象，以便将该 Service 映射到后端服务真实的 IP 地址和端口： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">192.0</span><span class="number">.2</span><span class="number">.42</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>注意：</p><ul><li>对于 Service 的访问者来说，Service 是否有 label selector 都是一样的。在上述例子中，Service 将请求路由到 Endpoint 192.0.2.42:9376 (TCP)。 </li><li>Endpoint 中的 IP 地址不可以是集群中其他 Service 的 ClusterIP。</li></ul><h2 id="Kubernetes-支持三种-proxy-mode（代理模式）"><a href="#Kubernetes-支持三种-proxy-mode（代理模式）" class="headerlink" title="Kubernetes 支持三种 proxy mode（代理模式）"></a>Kubernetes 支持三种 proxy mode（代理模式）</h2><h3 id="1-User-space-代理模式"><a href="#1-User-space-代理模式" class="headerlink" title="1.User space 代理模式"></a>1.User space 代理模式</h3><p><strong>在 user space proxy mode 下：</strong></p><ul><li>kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件</li><li>kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 打开一个随机端口</li><li>kube-proxy 安装 iptables 规则，将发送到该 Service 的 ClusterIP（虚拟 IP）/ Port 的请求重定向到该随机端口</li><li>任何发送到该随机端口的请求将被代理转发到该 Service 的后端 Pod 上（kube-proxy 从 Endpoint 信息中获得可用 Pod）</li><li>kube-proxy 在决定将请求转发到后端哪一个 Pod 时，默认使用 round-robin（轮询）算法，并会考虑到 Service 中的 <code>SessionAffinity</code> 的设定</li></ul><p>如下图所示：</p><p> <img src="https://kuboard.cn/assets/img/services-userspace-overview.7dfebdc9.svg" alt="Kubernetes教程：Service user space"> </p><h3 id="2-Iptables-代理模式-默认模式"><a href="#2-Iptables-代理模式-默认模式" class="headerlink" title="2.Iptables 代理模式 默认模式"></a>2.Iptables 代理模式 <strong>默认模式</strong></h3><p><strong>在 iptables proxy mode 下：</strong></p><ul><li><p>kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件</p></li><li><p>kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 安装 iptable 规则</p></li><li><p>iptables 将发送到 Service 的 ClusterIP / Port 的请求重定向到 Service 的后端 Pod 上</p><ul><li>对于 Service 中的每一个 Endpoint，kube-proxy 安装一个 iptable 规则</li><li>默认情况下，kube-proxy 随机选择一个 Service 的后端 Pod</li></ul><p><img src="https://kuboard.cn/assets/img/services-iptables-overview.fc39e9e4.svg" alt="Kubernetes教程：Service iptables proxy"> </p></li></ul><p><strong>iptables proxy mode 的优点：</strong></p><ul><li>更低的系统开销：在 linux netfilter 处理请求，无需在 userspace 和 kernel space 之间切换</li><li>更稳定</li></ul><p><strong>与 user space mode 的差异：</strong></p><ul><li>使用 iptables mode 时，如果第一个 Pod 没有响应，则创建连接失败</li><li>使用 user space mode 时，如果第一个 Pod 没有响应，kube-proxy 会自动尝试连接另外一个后端 Pod</li></ul><p>可以配置 Pod 就绪检查（readiness probe）确保后端 Pod 正常工作，此时，在 iptables 模式下 kube-proxy 将只使用健康的后端 Pod，从而避免了 kube-proxy 将请求转发到已经存在问题的 Pod 上。</p><h3 id="3-IPVS-代理模式"><a href="#3-IPVS-代理模式" class="headerlink" title="3.IPVS 代理模式"></a>3.IPVS 代理模式</h3><p><strong>在 IPVS proxy mode 下：</strong></p><ul><li><p>kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件</p></li><li><p>kube-proxy 根据监听到的事件，调用 netlink 接口，创建 IPVS 规则；并且将 Service/Endpoint 的变化同步到 IPVS 规则中</p></li><li><p>当访问一个 Service 时，IPVS 将请求重定向到后端 Pod</p><p><img src="https://kuboard.cn/assets/img/services-ipvs-overview.88a8453f.svg" alt="Kubernetes教程：Service IPVS proxy"> </p></li></ul><p><strong>IPVS 模式的优点</strong></p><p>IPVS proxy mode 基于 netfilter 的 hook 功能，与 iptables 代理模式相似，但是 IPVS 代理模式使用 hash table 作为底层的数据结构，并在 kernel space 运作。这就意味着</p><ul><li>IPVS 代理模式可以比 iptables 代理模式有更低的网络延迟，在同步代理规则时，也有更高的效率</li><li>与 user space 代理模式 / iptables 代理模式相比，IPVS 模式可以支持更大的网络流量</li></ul><p><strong>注意：</strong></p><ul><li>如果要使用 IPVS 模式，必须在启动 kube-proxy 前为节点的 linux 启用 IPVS</li><li>kube-proxy 以 IPVS 模式启动时，如果发现节点的 linux 未启用 IPVS，则退回到 iptables 模式</li></ul><h3 id="代理模式总结"><a href="#代理模式总结" class="headerlink" title="代理模式总结"></a>代理模式总结</h3><p>在所有的代理模式中，发送到 Service 的 IP:Port 的请求将被转发到一个合适的后端 Pod，而无需调用者知道任何关于 Kubernetes/Service/Pods 的细节。</p><p>Service 中额外字段的作用：</p><ul><li><pre><code>service.spec.sessionAffinity<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  - 默认值为 &quot;None&quot;</span><br><span class="line">  - 如果设定为 &quot;ClientIP&quot;，则同一个客户端的连接将始终被转发到同一个 Pod</span><br><span class="line"></span><br><span class="line">-</span><br></pre></td></tr></table></figure>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  - 默认值为 10800 （3 小时）</span><br><span class="line">  - 设定会话保持的持续时间</span><br><span class="line"></span><br><span class="line">## 多端口的Service</span><br><span class="line"></span><br><span class="line">Kubernetes 中，可以在一个 Service 对象中定义多个端口，但必须为每个端口定义一个名字。如下所示：</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: my-service</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: MyApp</span><br><span class="line">  ports:</span><br><span class="line">    - name: http</span><br><span class="line">      protocol: TCP</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 9376</span><br><span class="line">    - name: https</span><br><span class="line">      protocol: TCP</span><br><span class="line">      port: 443</span><br><span class="line">      targetPort: 9377</span><br></pre></td></tr></table></figure></code></pre></li></ul><h2 id="使用自定义的-IP-地址"><a href="#使用自定义的-IP-地址" class="headerlink" title="使用自定义的 IP 地址"></a>使用自定义的 IP 地址</h2><p>创建 Service 时，如果指定 <code>.spec.clusterIP</code> 字段，可以使用自定义的 Cluster IP 地址。该 IP 地址必须是 APIServer 中配置字段 <code>service-cluster-ip-range</code> CIDR 范围内的合法 IPv4 或 IPv6 地址，否则不能创建成功。</p><h2 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h2><p>Kubernetes 支持两种主要的服务发现模式：</p><ul><li>环境变量</li><li>DNS</li></ul><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>kubelet 查找有效的 Service，并针对每一个 Service，向其所在节点上的 Pod 注入一组环境变量。支持的环境变量有：</p><ul><li>{SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT<ul><li>Service name 被转换为大写</li><li>小数点 <code>.</code> 被转换为下划线 <code>_</code> </li></ul></li></ul><p>如果要在 Pod 中使用基于环境变量的服务发现方式，必须先创建 Service，再创建调用 Service 的 Pod。否则，Pod 中不会有该 Service 对应的环境变量。 </p><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><p>CoreDNS 监听 Kubernetes API 上创建和删除 Service 的事件，并为每一个 Service 创建一条 DNS 记录。集群中所有的 Pod 都可以使用 DNS Name 解析到 Service 的 IP 地址。 </p><h2 id="虚拟-IP-的实现"><a href="#虚拟-IP-的实现" class="headerlink" title="虚拟 IP 的实现"></a>虚拟 IP 的实现</h2><h3 id="避免冲突"><a href="#避免冲突" class="headerlink" title="避免冲突"></a>避免冲突</h3><p>Kubernetes 的一个设计哲学是：尽量避免非人为错误产生的可能性。就设计 Service 而言，Kubernetes 应该将选择的端口号与其他人选择的端口号隔离开。为此，Kubernetes 为每一个 Service 分配一个该 Service 专属的 IP 地址。</p><p>为了确保每个 Service 都有一个唯一的 IP 地址，kubernetes 在创建 Service 之前，先更新 etcd 中的一个全局分配表，如果更新失败（例如 IP 地址已被其他 Service 占用），则 Service 不能成功创建。</p><p>Kubernetes 使用一个后台控制器检查该全局分配表中的 IP 地址的分配是否仍然有效，并且自动清理不再被 Service 使用的 IP 地址。</p><h3 id="Service-的-IP-地址"><a href="#Service-的-IP-地址" class="headerlink" title="Service 的 IP 地址"></a>Service 的 IP 地址</h3><p>Pod 的 IP 地址路由到一个确定的目标，然而 Service 的 IP 地址则不同，通常背后并不对应一个唯一的目标。 kube-proxy 使用 iptables （Linux 中的报文处理逻辑）来定义虚拟 IP 地址。当客户端连接到该虚拟 IP 地址时，它们的网络请求将自动发送到一个合适的 Endpoint。Service 对应的环境变量和 DNS 实际上反应的是 Service 的虚拟 IP 地址（和端口）。</p><h4 id="Userspace"><a href="#Userspace" class="headerlink" title="Userspace"></a>Userspace</h4><p>以上面提到的图像处理程序为例。当后端 Service 被创建时，Kubernetes master 为其分配一个虚拟 IP 地址（假设是 10.0.0.1），并假设 Service 的端口是 1234。集群中所有的 kube-proxy 都实时监听者 Service 的创建和删除。Service 创建后，kube-proxy 将打开一个新的随机端口，并设定 iptables 的转发规则（以便将该 Service 虚拟 IP 的网络请求全都转发到这个新的随机端口上），并且 kube-proxy 将开始接受该端口上的连接。</p><p>当一个客户端连接到该 Service 的虚拟 IP 地址时，iptables 的规则被触发，并且将网络报文重定向到 kube-proxy 自己的随机端口上。kube-proxy 接收到请求后，选择一个后端 Pod，再将请求以代理的形式转发到该后端 Pod。</p><p>这意味着 Service 可以选择任意端口号，而无需担心端口冲突。客户端可以直接连接到一个 IP:port，无需关心最终在使用哪个 Pod 提供服务。</p><h4 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h4><p>仍然以上面提到的图像处理程序为例。当后端 Service 被创建时，Kubernetes master 为其分配一个虚拟 IP 地址（假设是 10.0.0.1），并假设 Service 的端口是 1234。集群中所有的 kube-proxy 都实时监听者 Service 的创建和删除。Service 创建后，kube-proxy 设定了一系列的 iptables 规则（这些规则可将虚拟 IP 地址映射到 per-Service 的规则）。per-Service 规则进一步链接到 per-Endpoint 规则，并最终将网络请求重定向（使用 destination-NAT）到后端 Pod。</p><p>当一个客户端连接到该 Service 的虚拟 IP 地址时，iptables 的规则被触发。一个后端 Pod 将被选中（基于 session affinity 或者随机选择），且网络报文被重定向到该后端 Pod。与 userspace proxy 不同，网络报文不再被复制到 userspace，kube-proxy 也无需处理这些报文，且报文被直接转发到后端 Pod。</p><p>在使用 node-port 或 load-balancer 类型的 Service 时，以上的代理处理过程是相同的。</p><h4 id="IPVS"><a href="#IPVS" class="headerlink" title="IPVS"></a>IPVS</h4><p>在一个大型集群中（例如，存在 10000 个 Service）iptables 的操作将显著变慢。IPVS 的设计是基于 in-kernel hash table 执行负载均衡。因此，使用 IPVS 的 kube-proxy 在 Service 数量较多的情况下仍然能够保持好的性能。同时，基于 IPVS 的 kube-proxy 可以使用更复杂的负载均衡算法（最少连接数、基于地址的、基于权重的等）</p><h2 id="支持的传输协议"><a href="#支持的传输协议" class="headerlink" title="支持的传输协议"></a>支持的传输协议</h2><ul><li>TCP</li><li>UDP</li><li>HTTP</li><li>Proxy Protocol</li><li>SCTP</li></ul><h1 id="三、发布Service"><a href="#三、发布Service" class="headerlink" title="三、发布Service"></a>三、发布Service</h1><h2 id="Service-类型"><a href="#Service-类型" class="headerlink" title="Service 类型"></a>Service 类型</h2><p>Kubernetes 中可以通过不同方式发布 Service，通过 <code>ServiceType</code> 字段指定，该字段的默认值是 <code>ClusterIP</code>，可选值有：</p><ul><li><strong>ClusterIP</strong>:  默认值。通过集群内部的一个 IP 地址暴露 Service，只在集群内部可以访问</li><li><strong>NodePort</strong>:  通过每一个节点上的的静态端口（NodePort）暴露 Service，同时自动创建 ClusterIP 类型的访问方式<ul><li>在集群内部通过 $(ClusterIP): $(Port) 访问</li><li>在集群外部通过 $(NodeIP): $(NodePort) 访问</li></ul></li><li><strong>LoadBalancer</strong>:  通过云服务供应商（AWS、Azure、GCE 等）的负载均衡器在集群外部暴露 Service，同时自动创建 NodePort 和 ClusterIP 类型的访问方式<ul><li>在集群内部通过 $(ClusterIP): $(Port) 访问</li><li>在集群外部通过 $(NodeIP): $(NodePort) 访问</li><li>在集群外部通过 $(LoadBalancerIP): $(Port) 访问</li></ul></li><li><strong>ExternalName</strong>: 将 Service 映射到 <code>externalName</code> 指定的地址（例如：foo.bar.example.com），返回值是一个 CNAME 记录。不使用任何代理机制。</li></ul><h2 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h2><p>ClusterIP 是 ServiceType 的默认值</p><h2 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h2><p>对于 <code>NodePort</code> 类型的 Service，Kubernetes 为其分配一个节点端口（对于同一 Service，在每个节点上的节点端口都相同），该端口的范围在初始化 apiserver 时可通过参数 <code>--service-node-port-range</code> 指定（默认是：30000-32767）。节点将该端口上的网络请求转发到对应的 Service 上。可通过 Service 的 <code>.spec.ports[*].nodePort</code> 字段查看该 Service 分配到的节点端口号。</p><p>在启动 kube-proxy 时使用参数 <code>--nodeport-address</code> 可指定阶段端口可以绑定的 IP 地址段。该参数接收以逗号分隔的 CIDR 作为参数值（例如：10.0.0.0/8,192.0.2.0/25），kube-proxy 将查找本机符合该 CIDR 的 IP 地址，并将节点端口绑定到符合的 IP 地址上。</p><p>NodePort 类型的 Service 可通过如下方式访问：</p><ul><li>在集群内部通过 $(ClusterIP): $(Port) 访问</li><li>在集群外部通过 $(NodeIP): $(NodePort) 访问</li></ul><h2 id="LoadBalancer"><a href="#LoadBalancer" class="headerlink" title="LoadBalancer"></a>LoadBalancer</h2><p>在支持外部负载均衡器的云环境中（例如 GCE、AWS、Azure 等），将 <code>.spec.type</code> 字段设置为 <code>LoadBalancer</code>，Kubernetes 将为该Service 自动创建一个负载均衡器，负载均衡器的创建操作异步完成。</p><h2 id="ExternalName"><a href="#ExternalName" class="headerlink" title="ExternalName"></a>ExternalName</h2><p>ExternalName 类型的 Service 映射到一个外部的 DNS name，而不是一个 pod label selector。可通过 <code>spec.externalName</code> 字段指定外部 DNS name。</p><h2 id="External-IP"><a href="#External-IP" class="headerlink" title="External IP"></a>External IP</h2><p>如果有外部 IP 路由到 Kubernetes 集群的一个或多个节点，Kubernetes Service 可以通过这些 <code>externalIPs</code> 进行访问。<code>externalIP</code> 需要由集群管理员在 Kubernetes 之外配置。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Service的功能及作用&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>控制器-CronJob</title>
    <link href="http://yoursite.com/2020/04/08/%E6%8E%A7%E5%88%B6%E5%99%A8-CronJob/"/>
    <id>http://yoursite.com/2020/04/08/%E6%8E%A7%E5%88%B6%E5%99%A8-CronJob/</id>
    <published>2020-04-08T02:20:54.000Z</published>
    <updated>2020-04-08T05:31:25.505Z</updated>
    
    <content type="html"><![CDATA[<p> <strong>控制器CronJob：</strong> 周期性任务控制，不需要持续后台运行 </p> <a id="more"></a> <h1 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h1><p> CronJob 按照预定的时间计划（schedule）创建 Job。一个 CronJob 对象类似于 crontab (cron table) 文件中的一行记录。该对象根据 Cron 格式定义的时间计划，周期性地创建 Job 对象。 </p><p> CronJob 只负责按照时间计划的规定创建 Job 对象，由 Job 来负责管理具体 Pod 的创建和执行。 </p><h1 id="使用CronJob执行自动任务"><a href="#使用CronJob执行自动任务" class="headerlink" title="使用CronJob执行自动任务"></a>使用CronJob执行自动任务</h1><p>CronJob可以用来执行基于时间计划的定时任务，类似于Linux/Unix系统中的crontable。</p><p>CronJob 执行周期性的重复任务时非常有用，例如备份数据、发送邮件等。CronJob 也可以用来指定将来某个时间点执行单个任务，例如将某项任务定时到系统负载比较低的时候执行。</p><p>CronJob 也存在某些限制，例如，在某些情况下，一个 CronJob 可能会创建多个 Job。因此，Job 必须是幂等的。</p><h2 id="创建CronJob"><a href="#创建CronJob" class="headerlink" title="创建CronJob"></a>创建CronJob</h2><p>下面例子中的 CronJob 每分钟，打印一次当前时间并输出 hello world 信息。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronJob</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">schedule:</span> <span class="string">"*/1 * * * *"</span></span><br><span class="line">  <span class="attr">jobTemplate:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line">            <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">            <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">date;</span> <span class="string">echo</span> <span class="string">Hello</span> <span class="string">from</span> <span class="string">the</span> <span class="string">Kubernetes</span> <span class="string">cluster</span></span><br><span class="line">          <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br></pre></td></tr></table></figure><p> 执行命令以创建 CronJob： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]# kubectl create -f cronjob.yaml </span><br><span class="line">cronjob.batch&#x2F;hello created</span><br></pre></td></tr></table></figure><p>或者直接用kubectl run 命令来创建CronJob</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run hello --schedule=<span class="string">"*/1 * * * *"</span> --restart=OnFailure --image=busybox -- /bin/sh -c <span class="string">"date; echo Hello from the Kubernetes cluster"</span></span><br></pre></td></tr></table></figure><p>查看已创建的CronJob</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]# kubectl get cronjob hello</span><br><span class="line">NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE</span><br><span class="line">hello   *&#x2F;1 * * * *   False     1        31s             16m</span><br></pre></td></tr></table></figure><p>输出结果显示，该 CronJob 在 <code>LAST SCHEDULE</code> 这个时间点成功创建了一个 Job。当前 <code>ACTIVE</code> Job 数为 0，意味着，该 Job 已经成功结束，或者已经失败。 </p><h2 id="删除CronJob"><a href="#删除CronJob" class="headerlink" title="删除CronJob"></a>删除CronJob</h2><p>当您不再需要某个 CronJob 时，可以使用命令将其删除 <code>kubectl delete cronjob</code>，在本例中，可以执行命令：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete cronjob hello</span><br><span class="line">cronjob.batch <span class="string">"hello"</span> deleted</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line">kubectl delete -f  cronjob.yaml</span><br></pre></td></tr></table></figure><p>删除 CronJob 时，将移除该 CronJob 创建的所有 Job 和 Pod，并且 CronJob 控制器将不会为其在创建任何新的 Job。 </p><h2 id="写CronJob-YAML"><a href="#写CronJob-YAML" class="headerlink" title="写CronJob YAML"></a>写CronJob YAML</h2><p>与其他所有 Kubernetes 对象一样，CronJob 对象需要 <code>apiVersion</code>、<code>kind</code>、<code>metadata</code> 这几个字段。CronJob 还需要 <code>.spec</code> 字段。</p><p>所有对 CronJob 对象作出的修改，尤其是 <code>.spec</code> 的修改，都只对修改之后新建的 Job 有效，已经创建的 Job 不会受到影响</p><h3 id="Schedule"><a href="#Schedule" class="headerlink" title="Schedule"></a>Schedule</h3><p><code>.spec.schedule</code> 是一个必填字段。类型为 Cron 格式的字符串，例如 <code>0 * * * *</code> 或者 <code>@hourly</code>，该字段定义了 CronJob 应该何时创建和执行 Job。</p><p>该字段同样支持 <code>vixie cron</code> step 值，指定 CronJob 每隔两个小时执行一次，可以有如下三种写法：</p><ul><li><code>0 0,2,4,5,6,8,12,14,16,17,20,22 * * *</code>）</li><li>使用 范围值 + Step 值的写法：<code>0 0-23/2 * * *</code></li><li>Step 也可以跟在一个星号后面，如 <code>0 */2 * * *</code></li></ul><p>问号 <code>?</code> 与 星号 <code>*</code> 的含义相同，代表着该字段不做限定</p><h3 id="Job-Template"><a href="#Job-Template" class="headerlink" title="Job Template"></a>Job Template</h3><p><code>.spec.jobTemplate</code> 字段是必填字段。该字段的结构与Job相同，只是不需要 <code>apiVersion</code> 和 <code>kind</code>。</p><h3 id="tarting-Deadline"><a href="#tarting-Deadline" class="headerlink" title="tarting Deadline"></a>tarting Deadline</h3><p><code>.spec.startingDeadlineSeconds</code> 为可选字段，代表着从计划的时间点开始，最迟多少秒之内必须启动 Job。如果超过了这个时间点，CronJob 就不会为其创建 Job，并将其记录为一次错过的执行次数。如果该字段未指定，则 Job 必须在指定的时间点执行。</p><p>CronJob 控制器将为每一个 CronJob 记录错过了多少次执行次数，如果错过的执行次数超过 100，则控制器将不会再为该 CronJob 创建新的 Job。如果 <code>.spec.startingDeadlineSeconds</code> 未指定，CronJob 控制器计算从 <code>.status.lastScheduleTime</code> 开始到现在为止总共错过的执行次数。</p><p>例如，某一个 CronJob 应该每分钟执行一次，<code>.status.lastScheduleTime</code> 的值是 上午5:00，假设现在已经是上午7:00。这意味着已经有 120 次执行时间点被错过，因此该 CronJob 将不再执行了。</p><p>如果 <code>.spec.startingDeadlineSeconds</code> 字段被设置为一个非空的值，则 CronJob 控制器计算将从 <code>.spec.startingDeadlineSeconds</code> 秒以前到现在这个时间段内错过的执行次数。</p><p>例如，假设该字段被设置为 <code>200</code>，控制器将只计算过去 200 秒内错过的执行次数。如果在过去 200 秒之内，有超过 100 次错过的执行次数，则 CronJob 将不再执行。</p><h3 id="Concurrency-Policy"><a href="#Concurrency-Policy" class="headerlink" title="Concurrency Policy"></a>Concurrency Policy</h3><p><code>.spec.concurrencyPolicy</code> 是选填字段，指定了如何控制该 CronJob 创建的 Job 的并发性，可选的值有：</p><ul><li><code>Allow</code>： 默认值，允许并发运行 Job</li><li><code>Forbid</code>： 不允许并发运行 Job；如果新的执行时间点到了，而上一个 Job 还未执行完，则 CronJob 将跳过新的执行时间点，保留仍在运行的 Job，且不会在此刻创建新的 Job</li><li><code>Replace</code>： 如果新的执行时间点到了，而上一个 Job 还未执行完，则 CronJob 将创建一个新的 Job 以替代正在执行的 Job</li></ul><p>TIP</p><p>Concurrency policy 只对由同一个 CronJob 创建的 Job 生效。如果有多个 CronJob，则他们各自创建的 Job 之间不会相互影响。</p><h3 id="Suspend"><a href="#Suspend" class="headerlink" title="Suspend"></a>Suspend</h3><p><code>.spec.suspend</code> 是选填字段。如果该字段设置为 <code>true</code>，所有的后续执行都将挂起，该字段不会影响到已经创建的 Job。默认值为 <code>false</code>。</p><p><strong>警告</strong></p><p>挂起（suspend）的时间段内，如果恰好存在有计划的执行时间点，则这些执行时间计划都被记录下来。如果不指定 <code>.spec.startingDeadlineSeconds</code>，并将 <code>.spec.suspend</code> 字段从 <code>true</code> 修改为 <code>false</code>，则挂起这段时间内的执行计划都将被立刻执行。</p><h3 id="Job-History-Limits"><a href="#Job-History-Limits" class="headerlink" title="Job History Limits"></a>Job History Limits</h3><p><code>.spec.successfulJobsHistoryLimit</code> 和 <code>.spec.failedJobsHistoryLimit</code> 字段是可选的。这些字段指定了 CronJob 应该保留多少个 completed 和 failed 的 Job 记录。</p><ul><li><code>.spec.successfulJobsHistoryLimit</code> 的默认值为 3</li><li><code>.spec.failedJobsHistoryLimit</code> 的默认值为 1</li></ul><p>如果将其设置为 <code>0</code>，则 CronJob 不会保留已经结束的 Job 的记录。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;strong&gt;控制器CronJob：&lt;/strong&gt; 周期性任务控制，不需要持续后台运行 &lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>控制器-Job</title>
    <link href="http://yoursite.com/2020/04/07/%E6%8E%A7%E5%88%B6%E5%99%A8-Job/"/>
    <id>http://yoursite.com/2020/04/07/%E6%8E%A7%E5%88%B6%E5%99%A8-Job/</id>
    <published>2020-04-07T09:01:49.000Z</published>
    <updated>2020-04-08T02:21:57.991Z</updated>
    
    <content type="html"><![CDATA[<p> <strong>控制器-Job：</strong> 只要完成就立即退出，不需要重启或重建。 </p> <a id="more"></a> <h1 id="控制器-Job"><a href="#控制器-Job" class="headerlink" title="控制器 - Job"></a>控制器 - Job</h1><p>Kubernetes中的 Job 对象将创建一个或多个 Pod，并确保指定数量的 Pod 可以成功执行到进程正常结束：</p><ul><li>当 Job 创建的 Pod 执行成功并正常结束时，Job 将记录成功结束的 Pod 数量</li><li>当成功结束的 Pod 达到指定的数量时，Job 将完成执行</li><li>删除 Job 对象时，将清理掉由 Job 创建的 Pod</li></ul><p>一个简单的例子是：创建一个 Job 对象用来确保一个 Pod 的成功执行并结束。在第一个 Pod 执行失败或者被删除（例如，节点硬件故障或机器重启）的情况下，该 Job 对象将创建一个新的 Pod 以重新执行。</p><p>当然，也可以使用 Job 对象并行执行多个 Pod。</p><h2 id="运行一个Job的例子"><a href="#运行一个Job的例子" class="headerlink" title="运行一个Job的例子"></a>运行一个Job的例子</h2><p>在下面这个 Job 的例子中，Pod 执行了一个跟 π 相关的计算，并打印出最终结果，该计算大约需要 10 秒钟执行结束。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">perl</span></span><br><span class="line">        <span class="attr">command:</span> <span class="string">["perl",</span>  <span class="string">"-Mbignum=bpi"</span><span class="string">,</span> <span class="string">"-wle"</span><span class="string">,</span> <span class="string">"print bpi(2000)"</span><span class="string">]</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">4</span></span><br></pre></td></tr></table></figure><p>创建Job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]# kubectl apply -f  job1.yaml </span><br><span class="line">job.batch&#x2F;pi created</span><br></pre></td></tr></table></figure><p>查看已创建的Job</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="number">0407</span><span class="string">]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">job</span></span><br><span class="line"><span class="string">NAME</span>   <span class="string">COMPLETIONS</span>   <span class="string">DURATION</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">pi</span>     <span class="number">0</span><span class="string">/1</span>           <span class="string">38s</span>        <span class="string">38s</span></span><br><span class="line"></span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="number">0407</span><span class="string">]#</span> <span class="string">kubectl</span> <span class="string">describe</span> <span class="string">job</span></span><br><span class="line"><span class="attr">Name:</span>           <span class="string">pi</span></span><br><span class="line"><span class="attr">Namespace:</span>      <span class="string">default</span></span><br><span class="line"><span class="attr">Selector:</span>       <span class="string">controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60</span></span><br><span class="line"><span class="attr">Labels:</span>         <span class="string">controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60</span></span><br><span class="line">                <span class="string">job-name=pi</span></span><br><span class="line"><span class="attr">Annotations:    kubectl.kubernetes.io/last-applied-configuration:</span></span><br><span class="line">                  <span class="string">&#123;"apiVersion":"batch/v1","kind":"Job","metadata":&#123;"annotations":&#123;&#125;,"name":"pi","namespace":"default"&#125;,"spec":&#123;"backoffLimit":4,"template":...</span></span><br><span class="line"><span class="attr">Parallelism:</span>    <span class="number">1</span></span><br><span class="line"><span class="attr">Completions:</span>    <span class="number">1</span></span><br><span class="line"><span class="attr">Start Time:</span>     <span class="string">Tue,</span> <span class="number">07</span> <span class="string">Apr</span> <span class="number">2020</span> <span class="number">16</span><span class="string">:57:38</span> <span class="string">+0800</span></span><br><span class="line"><span class="attr">Pods Statuses:</span>  <span class="number">1</span> <span class="string">Running</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Succeeded</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Failed</span></span><br><span class="line"><span class="attr">Pod Template:</span></span><br><span class="line">  <span class="attr">Labels:</span>  <span class="string">controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60</span></span><br><span class="line">           <span class="string">job-name=pi</span></span><br><span class="line">  <span class="attr">Containers:</span></span><br><span class="line">   <span class="attr">pi:</span></span><br><span class="line">    <span class="attr">Image:</span>      <span class="string">perl</span></span><br><span class="line">    <span class="attr">Port:</span>       <span class="string">&lt;none&gt;</span></span><br><span class="line">    <span class="attr">Host Port:</span>  <span class="string">&lt;none&gt;</span></span><br><span class="line">    <span class="attr">Command:</span></span><br><span class="line">      <span class="string">perl</span></span><br><span class="line">      <span class="string">-Mbignum=bpi</span></span><br><span class="line">      <span class="string">-wle</span></span><br><span class="line">      <span class="string">print</span> <span class="string">bpi(2000)</span></span><br><span class="line">    <span class="attr">Environment:</span>  <span class="string">&lt;none&gt;</span></span><br><span class="line">    <span class="attr">Mounts:</span>       <span class="string">&lt;none&gt;</span></span><br><span class="line">  <span class="attr">Volumes:</span>        <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="attr">Events:</span></span><br><span class="line">  <span class="string">Type</span>    <span class="string">Reason</span>            <span class="string">Age</span>   <span class="string">From</span>            <span class="string">Message</span></span><br><span class="line"></span><br><span class="line"><span class="string">----</span>    <span class="string">------</span>            <span class="string">----</span>  <span class="string">----</span>            <span class="string">-------</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">Normal  SuccessfulCreate  48s   job-controller  Created pod:</span> <span class="string">pi-nn8pf</span></span><br><span class="line">  </span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="number">0407</span><span class="string">]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pods</span></span><br><span class="line"><span class="string">NAME</span>                                <span class="string">READY</span>   <span class="string">STATUS</span>             <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">pi-nn8pf</span>                            <span class="number">0</span><span class="string">/1</span>     <span class="string">Completed</span>          <span class="number">0</span>          <span class="string">79s</span></span><br></pre></td></tr></table></figure><p> 执行以下命令可获得该 Job 所有 Pod 的名字： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]<span class="comment"># pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='&#123;.items[*].metadata.name&#125;')</span></span><br><span class="line">[root@k8s-master 0407]<span class="comment"># echo $pods</span></span><br><span class="line">pi-nn8pf</span><br><span class="line"></span><br><span class="line">在这个命令中：</span><br><span class="line">selector 与 Job 定义中的 selector 相同</span><br><span class="line">--output=jsonpath 选项指定了一个表达式，该表达式从返回结果列表中的每一个 Pod 的信息中定位出 name 字段的取值</span><br></pre></td></tr></table></figure><p> 执行以下命令可查看 Pod 的日志： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]<span class="comment"># kubectl logs $pods</span></span><br><span class="line">3.14159265358979............................4780275901</span><br></pre></td></tr></table></figure><h1 id="编写Job的定义"><a href="#编写Job的定义" class="headerlink" title="编写Job的定义"></a>编写Job的定义</h1><p>与所有的 Kubernetes 对象一样，Job 对象的 YAML 文件中，都需要包括如下三个字段：</p><ul><li><code>.apiVersion</code></li><li><code>.kind</code></li><li><code>.metadata</code></li></ul><p>Job 对象的 YAML 文件，还需要一个 <code>.spec</code> 字段。</p><h2 id="Pod-Template"><a href="#Pod-Template" class="headerlink" title="Pod Template"></a>Pod Template</h2><p>除了 Pod 所需的必填字段之外，Job 中的 pod template 必须指定</p><ul><li>合适的标签 <code>.spec.template.spec.labels</code></li><li>指定合适的[重启策略 restartPolicy <code>.spec.template.spec.restartPolicy</code>，此处只允许使用 <code>Never</code> 和 <code>OnFailure</code> 两个取值</li></ul><h1 id="处理Pod和容器的失败"><a href="#处理Pod和容器的失败" class="headerlink" title="处理Pod和容器的失败"></a>处理Pod和容器的失败</h1><p>Pod 中的容器可能会因为多种原因执行失败，例如：</p><ul><li>容器中的进程退出了，且退出码（exit code）不为 0</li><li>容器因为超出内存限制而被 Kill</li><li>其他原因</li></ul><p>如果 Pod 中的容器执行失败，且 <code>.spec.template.spec.restartPolicy = &quot;OnFailure&quot;</code>，则 Pod 将停留在该节点上，但是容器将被重新执行。此时，应用程序需要处理在原节点（失败之前的节点）上重启的情况。或者，设置为 <code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code>。</p><p>整个 Pod 也可能因为多种原因执行失败，例如：</p><ul><li>Pod 从节点上被驱逐（节点升级、重启、被删除等）</li><li>Pod 的容器执行失败，且 <code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code></li></ul><p>当 Pod 执行失败时，Job 控制器将创建一个新的 Pod </p><h2 id="Pod失败重试"><a href="#Pod失败重试" class="headerlink" title="Pod失败重试"></a>Pod失败重试</h2><p>Pod backoff failure policy</p><p>某些情况下（例如，配置错误），可以在 Job 多次重试仍然失败的情况下停止该 Job。此时，可通过 <code>.spec.backoffLimit</code> 来设定 Job 最大的重试次数。该字段的默认值为 6.</p><p>Job 中的 Pod 执行失败之后，Job 控制器将按照一个指数增大的时间延迟（10s,20s,40s … 最大为 6 分钟）来多次重新创建 Pod。如果没有新的 Pod 执行失败，则重试次数的计数将被重置。</p><h1 id="Job的终止和清理"><a href="#Job的终止和清理" class="headerlink" title="Job的终止和清理"></a>Job的终止和清理</h1><p>当 Job 完成后：</p><ul><li>将不会创建新的 Pod</li><li>已经创建的 Pod 也不会被清理掉。此时，您仍然可以继续查看已结束 Pod 的日志，以检查 errors/warnings 或者其他诊断用的日志输出</li><li>Job 对象也仍然保留着，以便您可以查看该 Job 的状态</li><li>由用户决定是否删除已完成的 Job 及其 Pod<ul><li>可通过 <code>kubectl</code> 命令删除 Job，例如： <code>kubectl delete jobs/pi</code> 或者 <code>kubectl delete -f job.yaml</code></li><li>删除 Job 对象时，由该 Job 创建的 Pod 也将一并被删除</li></ul></li></ul><p>Job 通常会顺利的执行下去，但是在如下情况可能会非正常终止：</p><ul><li>某一个 Pod 执行失败（且 <code>restartPolicy=Never</code>）</li><li>或者某个容器执行出错（且 <code>restartPolicy=OnFailure</code>） <ul><li>此时，Job 按照处理Pod和容器的失败中 <code>.spec.bakcoffLimit</code> 描述的方式进行处理</li><li>一旦重试次数达到了 <code>.spec.backoffLimit</code> 中的值，Job 将被标记为失败，且尤其创建的所有 Pod 将被终止</li></ul></li><li>Job 中设置了 <code>.spec.activeDeadlineSeconds</code>。该字段限定了 Job 对象在集群中的存活时长，一旦达到 <code>.spec.activeDeadlineSeconds</code> 指定的时长，该 Job 创建的所有的 Pod 都将被终止，Job 的 Status 将变为 <code>type:Failed</code> 、 <code>reason: DeadlineExceeded</code></li></ul><h1 id="Job的自动清理"><a href="#Job的自动清理" class="headerlink" title="Job的自动清理"></a>Job的自动清理</h1><p>系统中已经完成的 Job 通常是不在需要里的，长期在系统中保留这些对象，将给 apiserver 带来很大的压力。如果通过更高级别的控制器（例如 CronJobs）来管理 Job，则 CronJob 可以根据其中定义的基于容量的清理策略自动清理Job。 </p><h2 id="TTL-机制"><a href="#TTL-机制" class="headerlink" title="TTL 机制"></a>TTL 机制</h2><p>除了 CronJob 之外，TTL 机制是另外一种自动清理已结束Job（<code>Completed</code> 或 <code>Finished</code>）的方式：</p><ul><li>TTL 机制由 TTL 控制器 提供</li><li>在 Job 对象中指定 <code>.spec.ttlSecondsAfterFinished</code> 字段可激活该特性</li></ul><p>当 TTL 控制器清理 Job 时，TTL 控制器将删除 Job 对象，以及由该 Job 创建的所有 Pod 对象。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: pi-with-ttl</span><br><span class="line">spec:</span><br><span class="line">  ttlSecondsAfterFinished: 100          <span class="comment">####</span></span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: pi</span><br><span class="line">        image: perl</span><br><span class="line">        <span class="built_in">command</span>: [<span class="string">"perl"</span>,  <span class="string">"-Mbignum=bpi"</span>, <span class="string">"-wle"</span>, <span class="string">"print bpi(2000)"</span>]</span><br><span class="line">      restartPolicy: Never</span><br></pre></td></tr></table></figure><p><strong>字段解释 <code>ttlSecondsAfterFinished</code>：</strong></p><ul><li>Job <code>pi-with-ttl</code> 的 <code>ttlSecondsAfterFinished</code> 值为 100，则，在其结束 <code>100</code> 秒之后，将可以被自动删除</li><li>如果 <code>ttlSecondsAfterFinished</code> 被设置为 <code>0</code>，则 TTL 控制器在 Job 执行结束后，立刻就可以清理该 Job 及其 Pod</li><li>如果 <code>ttlSecondsAfterFinished</code> 值未设置，则 TTL 控制器不会清理该 Job</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;strong&gt;控制器-Job：&lt;/strong&gt; 只要完成就立即退出，不需要重启或重建。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
</feed>
