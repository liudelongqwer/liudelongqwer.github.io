<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>拒绝再玩</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-26T09:12:09.745Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>duoyu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Internet如何访问K8s集群</title>
    <link href="http://yoursite.com/2020/04/26/Internet%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AEK8s%E9%9B%86%E7%BE%A4/"/>
    <id>http://yoursite.com/2020/04/26/Internet%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AEK8s%E9%9B%86%E7%BE%A4/</id>
    <published>2020-04-26T08:47:15.000Z</published>
    <updated>2020-04-26T09:12:09.745Z</updated>
    
    <content type="html"><![CDATA[<p>在Kubernetes中部署应用程序时，通常会定义三个组件：</p><ul><li>一个<strong>Deployment</strong> - 这是一份用于创建你的应用程序的Pod副本的”食谱”；</li><li>一个<strong>Service</strong> - 一个内部负载均衡器，用于将流量路由到内部的Pod上；</li><li>一个<strong>Ingress</strong> - 描述如何流量应该如何从集群外部流入到集群内部的你的服务上。</li></ul><p><img src="/2020/04/26/Internet%E5%A6%82%E4%BD%95%E8%AE%BF%E9%97%AEK8s%E9%9B%86%E7%BE%A4/1.jpg" alt></p><ol><li>在Kubernetes中，应用程序通过两层负载均衡器暴露服务：内部的和外部的。</li><li>内部的负载均衡器称为Service，而外部的负载均衡器称为Ingress。</li><li>Pod不会直接部署。Deployment会负责创建Pod并管理它们。</li></ol><h2 id="一-连接Deployment和Service"><a href="#一-连接Deployment和Service" class="headerlink" title="一. 连接Deployment和Service"></a>一. 连接Deployment和Service</h2><p>令人惊讶的消息是，Service和Deployment之间根本没有连接。</p><p>事实是：Service直接指向Pod，并完全跳过了Deployment。</p><p>因此，应该注意的是Pod和Service之间的相互关系。</p><ul><li>Service selector应至少与Pod的一个标签匹配；</li><li>Service的<strong>targetPort</strong>应与Pod中容器的<strong>containerPort</strong>匹配；</li><li>Service的<strong>port</strong>可以是任何数字。多个Service可以使用同一端口号，因为它们被分配了不同的IP地址</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在Kubernetes中部署应用程序时，通常会定义三个组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个&lt;strong&gt;Deployment&lt;/strong&gt; - 这是一份用于创建你的应用程序的Pod副本的”食谱”；&lt;/li&gt;
&lt;li&gt;一个&lt;strong&gt;Service&lt;/strong
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>K8s-应用诊断</title>
    <link href="http://yoursite.com/2020/04/26/K8s-%E5%BA%94%E7%94%A8%E8%AF%8A%E6%96%AD/"/>
    <id>http://yoursite.com/2020/04/26/K8s-%E5%BA%94%E7%94%A8%E8%AF%8A%E6%96%AD/</id>
    <published>2020-04-26T06:52:37.000Z</published>
    <updated>2020-04-26T08:13:18.984Z</updated>
    
    <content type="html"><![CDATA[<p> Kubernetes 出现问题排查</p><a id="more"></a> <h1 id="诊断应用程序"><a href="#诊断应用程序" class="headerlink" title="诊断应用程序"></a>诊断应用程序</h1><h2 id="Debugging-Pods"><a href="#Debugging-Pods" class="headerlink" title="Debugging Pods"></a>Debugging Pods</h2><p>查看 Pod 的完整描述 </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pods <span class="variable">$&#123;POD_NAME&#125;</span></span><br></pre></td></tr></table></figure><p>注意观察 <code>State</code> <code>Restart</code> <code>Count</code> <code>Conditions</code> <code>Events</code> 字段</p><h3 id="Pod一直是Pending"><a href="#Pod一直是Pending" class="headerlink" title="Pod一直是Pending"></a>Pod一直是Pending</h3><p>如果 Pod 一直停留在 <code>Pending</code>，意味着该 Pod 不能被调度到某一个节点上。通常，这是因为集群中缺乏足够的资源或者 <strong>合适</strong> 的资源。在上述 <code>kubectl describe...</code> 命令的输出中的 <code>Events</code> 字段，会有对应的事件描述为什么 Pod 不能调度到节点上。可能的原因有：</p><ul><li><p>资源不就绪：创建 Pod 时，有时候需要依赖于集群中的其他对象， ConfigMap（配置字典）、PVC（存储卷声明）等，例如</p><ul><li><p>可能该 Pod 需要的存储卷声明尚未与存储卷绑定，Events 信息如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Type     Reason            Age        From               Message</span><br><span class="line">----     ------            ----       ----               -------</span><br><span class="line">Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  pod has unbound immediate PersistentVolumeClaims (repated 2 <span class="built_in">times</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>缺乏足够的资源：可能集群中的CPU或内存都已经耗尽，此时，可以尝试：</p><ul><li>删除某些 Pod</li><li>调整Pod的资源请求</li><li>向集群中添加新的节点</li></ul></li><li><p><strong>该Pod使用<code>hostPort</code></strong>： 当Pod使用 <code>hostPort</code> 时，该Pod可以调度的地方就比较有限了。大多数情况下，是不需要使用 <code>hostPort</code> 的，可以尝试使用 Service 访问 Pod。如果您确实需要使用 <code>hostPort</code> 时，Deployment/ReplicationController 中 replicas 副本数不能超过集群中的节点数，因为每台机器的 80 端口只有一个，任何其他端口也只有一个。如果该端口被其他程序占用了，也将导致Pod调度不成功</p></li><li><p><strong>污点和容忍</strong>： 当在Pod的事件中看到 <code>Taints</code> 或 <code>Tolerations</code> 这两个单词时，可以检查Pod是否存在污点或者容忍</p></li></ul><h3 id="Pod一直是Wating"><a href="#Pod一直是Wating" class="headerlink" title="Pod一直是Wating"></a>Pod一直是Wating</h3><p>如果 Pod 停留在 <code>Waiting</code> 状态，此时该 Pod 已经被调度到某个节点上了，但是却不能运行。</p><p>注意 <code>Events</code> 字段的内容。最常见的 Pod 停留在 <code>Waiting</code> 状态的原因是抓取容器镜像失败。请检查：</p><ul><li>容器镜像的名字是对的</li><li>容器镜像已经推送到了镜像仓库中</li><li>在对应的节点上手工执行 <code>docker pull</code> 命令，看是否能够抓取成功。</li></ul><h3 id="Pod已经Crash或者Unhealthy"><a href="#Pod已经Crash或者Unhealthy" class="headerlink" title="Pod已经Crash或者Unhealthy"></a>Pod已经Crash或者Unhealthy</h3><p> 此时通常是容器中应用程序的问题，检查容器的日志，以诊断容器中应用程序出现了何种故障： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs <span class="variable">$&#123;POD_NAME&#125;</span> <span class="variable">$&#123;CONTAINER_NAME&#125;</span></span><br></pre></td></tr></table></figure><p> 如果容器之前 crash，通过上述命令查不到日志，可以尝试使用下面的命令查看上一次 crash 时的日志： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs --previous <span class="variable">$&#123;POD_NAME&#125;</span> <span class="variable">$&#123;CONTAINER_NAME&#125;</span></span><br></pre></td></tr></table></figure><h3 id="Pod处于Running状态，但是不工作"><a href="#Pod处于Running状态，但是不工作" class="headerlink" title="Pod处于Running状态，但是不工作"></a>Pod处于Running状态，但是不工作</h3><p>Pod已经处于Running状态了，但是不像期望的那样工作，此时，很有可能是部署描述yaml文件（例如 Pod、Deployment、StatefulSet等）出现了问题，而创建时，kubectl 忽略了该错误。</p><p>例如环境变量中某一个 Key 写错了，<code>command</code> 拼写成了 <code>commnd</code> 等。如果 <code>command</code> 拼写成了 <code>commnd</code>，仍然能够使用该 yaml 文件创建工作负载，但是容器在运行时，却不会使用原本期望的命令，而是执行了镜像中的 <code>EntryPoint</code>。</p><ul><li>首先，在使用 <code>kubectl apply -f</code> 命令之前，可以尝试为其添加 <code>--validate</code> 选项，例如， <code>kubectl apply --validate -f mypod.yaml</code>。如果将 <code>command</code> 拼写成 <code>commnd</code>，将看到如下错误信息： </li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl apply --validate -f  security-context-1.yaml </span></span><br><span class="line">error: error validating <span class="string">"security-context-1.yaml"</span>: error validating data: ValidationError(Pod.spec.containers[0]): unknown field <span class="string">"commnd"</span> <span class="keyword">in</span> io.k8s.api.core.v1.Container; <span class="keyword">if</span> you choose to ignore these errors, turn validation off with --validate=<span class="literal">false</span></span><br></pre></td></tr></table></figure><ul><li>其次，请检查已经创建的 Pod 和预期的是一致的。执行命令 <code>kubectl get pods/mypod -o yaml &gt; mypod-on-apiserver.yaml</code>。将输出结果与创建 Pod 时所使用的文件做一个对比。通常通过此命令从服务器端获取到的信息比创建 Pod 时所使用的文件要多几行，这是正常的。然而，如果创建的Pod时所示用的文件中，存在从服务器上获取的信息中没有的代码行，这可能就是问题所在了。 </li></ul><h2 id="Debugging-Deployment"><a href="#Debugging-Deployment" class="headerlink" title="Debugging Deployment"></a>Debugging Deployment</h2><p>Deployment（或者 DaemonSet/StatefulSet/Job等），都会比较直接，要么可以创建 Pod，要么不可以。</p><p>可以通过 <code>kubectl describe deployment ${DEPLOYMENT_NAME}</code> （或者statefulset / job 等）命令查看与 Deployment 相关的事件，来发现到底出了什么问题。</p><h2 id="Debugging-Service"><a href="#Debugging-Service" class="headerlink" title="Debugging Service"></a>Debugging Service</h2><p>Service 可以为一组 Pod 提供负载均衡的功能。</p><p>首先，检查Service的Endpoints。 </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get endpoints <span class="variable">$&#123;SERVICE_NAME&#125;</span></span><br></pre></td></tr></table></figure><p>请确保 enpoints 的个数与期望与该 Service 匹配的 Pod 的个数是相同的。例如，如果使用 Deployment 部署了 web-press，副本数为 2，此时，在输出结果的 ENDPOINTS 字段，应该有两个不同的 IP 地址。 </p><h3 id="Service中没有Endpoints"><a href="#Service中没有Endpoints" class="headerlink" title="Service中没有Endpoints"></a>Service中没有Endpoints</h3><p>如果Service中没有Endpoints，请尝试使用 Service 的 label selector 查询一下是否存在 Pod。假设 Service 如下： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myservice</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ns1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">selector:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">frontend</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p> 执行如下命令可以查看 Service 所匹配的 Pod： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods --selector=name=nginx,<span class="built_in">type</span>=frontend -n ns1</span><br></pre></td></tr></table></figure><p>如果 Pod 列表是期望的结果，但是 ENDPOINTS 还是空的，此时很可能是没有为 Service 指定正确的端口。</p><p>如果 Service 中指定的 <code>containerPort</code> 实际上并不存在于 Pod 中，该 Pod 不会被添加到 ENDPOINTS 列表里。请确保 Service 指定的 <code>containerPort</code> 在 Pod 中是可以访问的。 </p><h2 id="网络转发问题"><a href="#网络转发问题" class="headerlink" title="网络转发问题"></a>网络转发问题</h2><p>如果客户端可以连接上 Service，但是连接很快就被断开了，并且 endpoints 中有合适的内容，此时，有可能是 proxy 不能转发到 Pod 上。</p><p>请检查：</p><ul><li>Pod是否正常工作？<code>kubectl get pods</code> 查看 Pod 的 restart count，诊断一下 Pod 是否有问题。</li><li>是否可以直接连接到 Pod ？<code>kubectl get pods -o wide</code> 可以获得 Pod 的IP地址，从任意一个节点上执行 <code>ping</code> 命令，确认网络连接是否正常。</li><li>应用程序是否正常地监听了端口？Kubernetes 不对网络端口做映射，如果您的应用程序监听 8080 端口，则在 Service 中应该指定 <code>containerPort</code> 为 8080。在任意节点上执行命令 <code>curl :</code> 可查看 Pod 中容器的端口是否正常。</li></ul><h1 id="诊断集群问题"><a href="#诊断集群问题" class="headerlink" title="诊断集群问题"></a>诊断集群问题</h1><h2 id="查看集群中的节点："><a href="#查看集群中的节点：" class="headerlink" title="查看集群中的节点："></a>查看集群中的节点：</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes -o wide</span><br><span class="line">kubectl describe node <span class="variable">$&#123;NODE_NAME&#125;</span></span><br></pre></td></tr></table></figure><ul><li><p><code>kube-</code> 开头的 Pod 都是 Kubernetes 集群的系统级组件</p></li><li><p><code>calico-</code> 开头是的 calico 网络插件</p></li><li><p><code>etcd-</code> 开头的是 etcd</p></li><li><p><code>coredns-</code> 开头的是 DNS 插件。 </p><p>假设 apiserver 可能有故障，可以执行以下命令以查看其日志 </p></li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs -f kube-apiserver-demo-master<span class="_">-a</span>-1 -n kube-system</span><br></pre></td></tr></table></figure><h3 id="查看-kubelet-的日志"><a href="#查看-kubelet-的日志" class="headerlink" title="查看 kubelet 的日志"></a>查看 kubelet 的日志</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -u kubelet</span><br></pre></td></tr></table></figure><h2 id="集群故障的常见原因"><a href="#集群故障的常见原因" class="headerlink" title="集群故障的常见原因"></a>集群故障的常见原因</h2><p>一部分 kubernetes 集群常见的故障原因以及应对办法：</p><p>可能的 Root causes：</p><ul><li>虚拟机（或所在物理机）停机</li><li>集群内部发生网络不通的情况，或者集群和用户之间网络不通</li><li>Kubernetes 系统组件崩溃</li><li>数据丢失，或持久化存储不可用</li></ul><p>具体的故障场景有：</p><ul><li>Apiserver 所在虚拟机 shotdown 或者 apiserver 崩溃<ul><li>导致的结果：<ul><li>不能创建、停止、更新 Pod、Service、Deployment等</li><li>已有的 Pod 和 Service 仍然能够正常工作，除非该 Pod 或 Service 需要调用 Kubernetes 的接口，例如 Kubernetes Dashboard 和 Kuboard</li></ul></li></ul></li><li>Apiserver 的后端数据丢失<ul><li>导致的结果：<ul><li>apiserver 将不能再启动</li><li>已有的 Pod 和 Service 仍然能够正常工作，除非该 Pod 或 Service 需要调用 Kubernetes 的接口，例如 Kubernetes Dashboard 和 Kuboard</li><li>需要手工恢复（或重建） apiserver 的数据才能启动 apiserver</li></ul></li></ul></li><li>其他 Master 组件崩溃<ul><li>导致的结果和 apiserver 相同</li></ul></li><li>个别节点（虚拟机或物理机）停机<ul><li>导致的结果<ul><li>该节点上的所有 Pod 不再运行</li></ul></li></ul></li><li>网络分片<ul><li>导致的结果<ul><li>区域A认为区域B中的节点已死机；区域B认为区域A中的 apiserver 已死机（假设apiserver在区域A）</li></ul></li></ul></li><li>kubelet 软件故障<ul><li>导致的结果<ul><li>已崩溃的 Kubelet 不能在该节点上再创建新的 Pod</li><li>kubelet 有可能错误地删除了 Pod</li><li>节点被标记为 <code>unhealthy</code></li><li>Deployment/ReplicationController 在其他节点创建新的 Pod</li></ul></li></ul></li><li>集群管理员的人为错误<ul><li>导致的结果<ul><li>丢失 Pod、Service 等</li><li>丢失 apiserver 的数据</li><li>用户不能访问接口，等等</li></ul></li></ul></li></ul><h2 id="应对办法："><a href="#应对办法：" class="headerlink" title="应对办法："></a>应对办法：</h2><ul><li>Action： 为 apiserver + etcd 使用 IaaS 供应商提供的稳定可靠的持久化存储<ul><li>应对问题： Apiserver 的后端数据丢失</li></ul></li><li>Action： 使用高可用配置<ul><li>应对问题：Apiserver 所在虚拟机 shotdown 或者 apiserver 崩溃</li><li>应对问题：其他 Master 组件崩溃</li><li>应对问题：个别节点（虚拟机或物理机）停机</li></ul></li><li>Action：周期性的为 apiserver 的 etcd 所使用的数据卷创建磁盘快照（Snapshot）<ul><li>应对问题：Apiserver 的后端数据丢失</li><li>应对问题：集群管理员的人为错误</li><li>应对问题：kubelet 软件故障</li></ul></li><li>Action：使用Deployment/StatefulSet/DaemonSet 等控制器，而不是直接创建 Pod<ul><li>应对问题：个别节点（虚拟机或物理机）停机，或者 kubelet 软件故障</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Kubernetes 出现问题排查&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-Security Context</title>
    <link href="http://yoursite.com/2020/04/25/K8s-Security-Context/"/>
    <id>http://yoursite.com/2020/04/25/K8s-Security-Context/</id>
    <published>2020-04-25T06:25:08.000Z</published>
    <updated>2020-04-26T06:37:34.237Z</updated>
    
    <content type="html"><![CDATA[<p> Security Context 概述及配置</p><a id="more"></a> <h1 id="Security-Context-概述"><a href="#Security-Context-概述" class="headerlink" title="Security Context 概述"></a>Security Context 概述</h1><p>Security Context（安全上下文）用来限制容器对宿主节点的可访问范围，以避免容器非法操作宿主节点的系统级别的内容，使得节点的系统或者节点上其他容器组受到影响。</p><p>Security Context可以按照如下几种方式设定：</p><ul><li>访问权限控制：是否可以访问某个对象（例如文件）是基于 userID（UID）和 groupID（GID）的</li><li>Security Enhanced Linux (SELinux)：为对象分配Security标签</li><li>以 privileged（特权）模式运行</li><li>Linux Capabilities：为容器组（或容器）分配一部分特权，而不是 root 用户的所有特权</li><li>AppArmor：自 Kubernetes v1.4 以来，一直处于 beta 状态</li><li>Seccomp：过滤容器中进程的系统调用（system call）</li><li>AllowPrivilegeEscalation（允许特权扩大）：此项配置是一个布尔值，定义了一个进程是否可以比其父进程获得更多的特权，直接效果是，容器的进程上是否被设置 no_new_privs 标记。当出现如下情况时，AllowPrivilegeEscalation 的值始终为 true：<ul><li>容器以 privileged 模式运行</li><li>容器拥有 CAP_SYS_ADMIN 的 Linux Capability</li></ul></li></ul><h1 id="为Pod设置Security-Context"><a href="#为Pod设置Security-Context" class="headerlink" title="为Pod设置Security Context"></a>为Pod设置Security Context</h1><p>在 Pod 的定义中增加 <code>securityContext</code> 字段，即可为 Pod 指定 Security 相关的设定。 <code>securityContext</code> 字段是一个 PodSecurityContext 对象。通过该字段指定的内容将对该 Pod 中所有的容器生效。 </p><h2 id="Pod示例："><a href="#Pod示例：" class="headerlink" title="Pod示例："></a>Pod示例：</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">security-context-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">runAsUser:</span> <span class="number">1000</span></span><br><span class="line">    <span class="attr">runAsGroup:</span> <span class="number">3000</span></span><br><span class="line">    <span class="attr">fsGroup:</span> <span class="number">2000</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sec-ctx-vol</span></span><br><span class="line">    <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sec-ctx-demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">[</span> <span class="string">"sh"</span><span class="string">,</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"sleep 1h"</span> <span class="string">]</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sec-ctx-vol</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/data/demo</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>在上面的例子中：</p><ul><li><p>spec.securityContext.runAsUser 字段指定了该 Pod 中所有容器的进程都以UserID 1000 的身份运行，spec.securityContext.runAsGroup 字段指定了该 Pod 中所有容器的进程都以GroupID 3000 的身份运行</p></li><li><ul><li>如果该字段被省略，容器进程的GroupID为 root(0)</li><li>容器中创建的文件，其所有者为 userID 1000，groupID 3000</li></ul></li><li><p>spec.securityContext.fsGroup 字段指定了该 Pod 的 fsGroup 为 2000</p></li><li><ul><li>数据卷 （本例中，对应挂载点 /data/demo 的数据卷为 sec-ctx-demo） 的所有者以及在该数据卷下创建的任何文件，其 GroupID 为 2000</li></ul></li></ul><h2 id="执行Pod示例"><a href="#执行Pod示例" class="headerlink" title="执行Pod示例"></a>执行Pod示例</h2><ul><li>创建 Pod</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl apply -f security-context-1.yaml </span></span><br><span class="line">pod/security-context-demo created</span><br></pre></td></tr></table></figure><ul><li>验证 Pod 已运行</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl get pod security-context-demo</span></span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">security-context-demo   1/1     Running   2          159m</span><br></pre></td></tr></table></figure><ul><li>进入容器的命令行界面</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl exec -it security-context-demo -- sh</span></span><br></pre></td></tr></table></figure><ul><li>在该命令行界面中，查看正在运行的进程，所有的进程都以 user 1000 的身份运行（由 runAsUser 指定）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/ $ ps</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 1000      0:00 sleep 1h</span><br><span class="line">    6 1000      0:00 sh</span><br><span class="line">   11 1000      0:00 ps</span><br></pre></td></tr></table></figure><ul><li>切换到目录 /data，并查看目录中的文件列表，/data/demo 目录的 groupID 为 2000（由 fsGroup 指定）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/ $ <span class="built_in">cd</span> /data/</span><br><span class="line">/data $ ls -l</span><br><span class="line">total 0</span><br><span class="line">drwxrwsrwx    2 root     2000             6 Apr 25 06:42 demo</span><br></pre></td></tr></table></figure><ul><li>在命令行界面中，切换到目录 /data/demo，并创建一个文件，testfile 的 groupID 为 2000 （由 FSGroup 指定），输出结果如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/ $ <span class="built_in">cd</span> /data/demo/</span><br><span class="line">/data/demo $ <span class="built_in">echo</span> hello &gt; testfile</span><br><span class="line">/data/demo $ ls -l</span><br><span class="line">total 4</span><br><span class="line">-rw-r--r--    1 1000     2000             6 Apr 25 09:19 testfile</span><br></pre></td></tr></table></figure><ul><li>在命令行界面中执行 id 命令，输出结果如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/data/demo $ id</span><br><span class="line">uid=1000 gid=3000 groups=2000</span><br></pre></td></tr></table></figure><ul><li>gid 为 3000，与 <code>runAsGroup</code> 字段所指定的一致</li><li>如果 <code>runAsGroup</code> 字段被省略，则 gid 取值为 0（即 root），此时容器中的进程将可以操作 root Group 的文件</li></ul><h1 id="为容器设置Security-Context"><a href="#为容器设置Security-Context" class="headerlink" title="为容器设置Security Context"></a>为容器设置Security Context</h1><p> 容器的定义中包含 <code>securityContext</code> 字段，该字段接受 SecurityContext 对象。通过指定该字段，可以为容器设定安全相关的配置，当该字段的配置与 Pod 级别的 <code>securityContext</code> 配置相冲突时，容器级别的配置将覆盖 Pod 级别的配置。容器级别的 <code>securityContext</code> 不影响 Pod 中的数据卷。 </p><p>下面的示例中的 Pod 包含一个 Container，且 Pod 和 Container 都有定义 <code>securityContext</code> 字段： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">security-context-demo-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">runAsUser:</span> <span class="number">1000</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sec-ctx-demo-2</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">[</span> <span class="string">"sh"</span><span class="string">,</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"sleep 1h"</span> <span class="string">]</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">2000</span></span><br><span class="line">      <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><ul><li>执行命令以创建 Pod </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0426]<span class="comment"># kubectl apply -f security-context-demo-2.yaml </span></span><br><span class="line">pod/security-context-demo-2 created</span><br></pre></td></tr></table></figure><ul><li>执行命令以验证容器已运行</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0426]<span class="comment"># kubectl get pod security-context-demo-2</span></span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">security-context-demo-2   1/1     Running   0          8s</span><br></pre></td></tr></table></figure><ul><li>执行命令进入容器的命令行界面： </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0426]<span class="comment"># kubectl exec -it security-context-demo-2 -- sh</span></span><br><span class="line">/ $ ps</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 2000      0:00 sleep 1h</span><br><span class="line">    6 2000      0:00 sh</span><br><span class="line">   11 2000      0:00 ps</span><br></pre></td></tr></table></figure><p>注意： 容器的进程以 userID 2000 的身份运行。该取值由 <code>spec.containers[*].securityContext.runAsUser</code> 容器组中的字段定义。Pod 中定义的 <code>spec.securityContext.runAsUser</code> 取值 1000 被覆盖。</p><h1 id="为容器设置SELinux标签"><a href="#为容器设置SELinux标签" class="headerlink" title="为容器设置SELinux标签"></a>为容器设置SELinux标签</h1><p>Pod 或容器定义的 <code>securityContext</code> 中 <code>seLinuxOptions</code> 字段是一个 SELinuxOptions 对象，该字段可用于为容器指定 SELinux 标签。如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line">  <span class="attr">seLinuxOptions:</span></span><br><span class="line">    <span class="attr">level:</span> <span class="string">"s0:c123,c456"</span></span><br></pre></td></tr></table></figure><p> 为容器指定 SELinux 标签时，宿主节点的 SELinux 模块必须加载。 </p><h1 id="关于数据卷"><a href="#关于数据卷" class="headerlink" title="关于数据卷"></a>关于数据卷</h1><p>Pod 的 securityContext 作用于 Pod 中所有的容器，同时对 Pod 的数据卷也同样生效。具体来说，<code>fsGroup</code> 和 <code>seLinuxOptions</code> 将被按照如下方式应用到 Pod 中的数据卷：</p><ul><li><code>fsGroup</code>：对于支持 ownership 管理的数据卷，通过 <code>fsGroup</code> 指定的 GID 将被设置为该数据卷的 owner，并且可被 <code>fsGroup</code> 写入。</li><li><code>seLinuxOptions</code>：对于支持 SELinux 标签的数据卷，将按照 <code>seLinuxOptions</code> 的设定重新打标签，以使 Pod 可以访问数据卷内容。通常只需要设置 <code>seLinuxOptions</code> 中 <code>level</code> 这一部分内容。该设定为 Pod 中所有容器及数据卷设置 Multi-Category Security (MCS) 标签。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Security Context 概述及配置&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-Secret概述</title>
    <link href="http://yoursite.com/2020/04/25/K8s-Secret%E6%A6%82%E8%BF%B0/"/>
    <id>http://yoursite.com/2020/04/25/K8s-Secret%E6%A6%82%E8%BF%B0/</id>
    <published>2020-04-25T01:41:52.000Z</published>
    <updated>2020-04-25T06:24:00.437Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes Secret 对象可以用来储存敏感信息，例如：密码、OAuth token、ssh 密钥等</p> <a id="more"></a> <h1 id="一、Secret概述"><a href="#一、Secret概述" class="headerlink" title="一、Secret概述"></a>一、Secret概述</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Kubernetes <code>Secret</code> 对象可以用来储存敏感信息，例如：密码、OAuth token、ssh 密钥等。如果不使用 <code>Secret</code>，此类信息可能被放置在 Pod 定义中或者容器镜像中。将此类敏感信息存储到 <code>Secret</code> 中，可以更好地：</p><ul><li>控制其使用</li><li>降低信息泄露的风险</li></ul><p>用户可以直接创建 Secret，Kubernetes 系统也会创建一些 Secret。</p><p>Secret有如下几种使用方式：</p><ul><li>作为 Pod 的数据卷挂载</li><li>作为 Pod 的环境变量</li><li>kubelet 在抓取容器镜像时，作为 docker 镜像仓库的用户名密码</li></ul><h2 id="内建Secret"><a href="#内建Secret" class="headerlink" title="内建Secret"></a>内建Secret</h2><p>Service Account 将自动创建 Secret</p><p>Kubernetes 自动创建包含访问 Kubernetes APIServer 身份信息的 Secret，并自动修改 Pod 使其引用这类 Secret。</p><p>如果需要，可以禁用或者自定义自动创建并使用 Kubernetes APIServer 身份信息的特性。然而，如果期望安全地访问 Kubernetes APIServer，应该使用默认的 Secret 创建使用过程。</p><h2 id="自建Secret"><a href="#自建Secret" class="headerlink" title="自建Secret"></a>自建Secret</h2><p>可以使用如下方式创建自己的 Secret：</p><ul><li>使用 kubectl 创建 Secret</li><li>手动创建 Secret</li><li>使用 Generator 创建 Secret</li></ul><h2 id="解码和编辑"><a href="#解码和编辑" class="headerlink" title="解码和编辑"></a>解码和编辑</h2><p>Kubenetes 中，Secret 使用 base64 编码存储，您可以将其解码获得对应信息的原文，创建 Secret 之后，您也可以再次编辑Secret</p><h1 id="二、创建Secret（使用kubectl）"><a href="#二、创建Secret（使用kubectl）" class="headerlink" title="二、创建Secret（使用kubectl）"></a>二、创建Secret（使用kubectl）</h1><p> 假设某个 Pod 需要访问数据库。在您执行 kubectl 命令所在机器的当前目录，创建文件 <code>./username.txt</code> 文件和 <code>./password.txt</code> 暂存数据库的用户名和密码，后续我们根据这两个文件配置 kubernetes secrets。 </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -n <span class="string">'admin'</span> &gt; ./username.txt</span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">'1f2d1e2e67df'</span> &gt; ./password.txt</span><br></pre></td></tr></table></figure><p>在 Kubernetes APIServer 中创建 Secret 对象，并将这两个文件中的内容存储到该 Secret 对象中，执行命令，输出结果如下所示： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt </span></span><br><span class="line">secret/db-user-pass created</span><br></pre></td></tr></table></figure><ul><li><p>上述命令的执行效果与此命令执行效果相同： <code>kubectl create secret generic db-user-pass –from-literal=username=admin –from-literal=password=1f2d1e2e67df</code></p></li><li><p>如果密码中包含特殊字符需要转码（例如 <code>$</code>、<code>*</code>、<code>\</code>、<code>!</code>），请使用 <code>\</code> 进行转码。例如：实际密码为 <code>S!B\*d$zDsb</code>，kubectl 命令应该写成 <code>kubectl create secret generic dev-db-secret –from-literal=username=devuser –from-literal=password=S\!B\\*d\$zDsb</code>。如果通过文件创建（–from-file），则无需对文件中的密码进行转码。</p><p>检查 Secret 的创建结果，输出信息如下所示： </p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl get secrets</span></span><br><span class="line">NAME                  TYPE                                  DATA   AGE</span><br><span class="line">db-user-pass          Opaque                                2      57s</span><br><span class="line">default-token-xws5p   kubernetes.io/service-account-token   3      28d</span><br><span class="line">nginxsecret           Opaque                                2      15d</span><br></pre></td></tr></table></figure><p> 查看 Secret 详情，输出信息如下所示： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl describe secrets db-user-pass</span></span><br><span class="line">Name:         db-user-pass</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Type:  Opaque</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line"></span><br><span class="line">password.txt:  12 bytes</span><br><span class="line">username.txt:  5 bytes</span><br></pre></td></tr></table></figure><p>默认情况下，<code>kubectl get</code> 和 <code>kubectl describe</code> 命令都避免展示 Secret 的内容。这种做法可以避免密码被偷窥，或者被存储到终端的日志中 </p><h1 id="三、创建Secret（手动）"><a href="#三、创建Secret（手动）" class="headerlink" title="三、创建Secret（手动）"></a>三、创建Secret（手动）</h1><p>可以在 yaml 文件中定义好 Secret，然后通过 <code>kubectl apply -f</code> 命令创建。通过如下两种方式在 yaml 文件中定义 Secret：</p><ul><li><strong>data</strong>：使用 data 字段时，取值的内容必须是 base64 编码的</li><li><strong>stringData</strong>：使用 stringData 时，更为方便，您可以直接将取值以明文的方式写在 yaml 文件中</li></ul><h2 id="在-yaml-中定义-data"><a href="#在-yaml-中定义-data" class="headerlink" title="在 yaml 中定义 data"></a>在 yaml 中定义 data</h2><ul><li>假设要保存 <code>username=admin</code> 和 <code>password=1f2d1e2e67df</code> 到 Secret 中，要先将数据的值转化为 base64 编码，执行如下命令：</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -n <span class="string">'admin'</span> | base64</span><br><span class="line">YWRtaW4=</span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">'1f2d1e2e67df'</span> | base64</span><br><span class="line">MWYyZDFlMmU2N2Rm</span><br></pre></td></tr></table></figure><p>创建 secret.yaml 文件，内容如下所示：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">YWRtaW4=</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">MWYyZDFlMmU2N2Rm</span></span><br></pre></td></tr></table></figure><p>执行命令创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f .&#x2F;secret.yaml</span><br><span class="line">secret &quot;mysecret&quot; created</span><br></pre></td></tr></table></figure><h2 id="在-yaml-中定义-stringData"><a href="#在-yaml-中定义-stringData" class="headerlink" title="在 yaml 中定义 stringData"></a>在 yaml 中定义 stringData</h2><p>假如并不想先将用户名和密码转换为 base64 编码之后再创建 Secret，则，可以通过定义 stringData 来达成，此时 stringData 中的取值部分将被 apiserver 自动进行 base64 编码之后再保存。</p><ul><li>创建文件 secret.yaml，内容如下所示：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">1f2d1e2e67df</span></span><br></pre></td></tr></table></figure><p>执行命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f .&#x2F;secret.yaml</span><br><span class="line">secret &quot;mysecret&quot; created</span><br></pre></td></tr></table></figure><p>执行命令 <code>kubectl get -f ./secret.yaml -o yaml</code> 输出结果如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">MWYyZDFlMmU2N2Rm</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">YWRtaW4=</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubectl.kubernetes.io/last-applied-configuration:</span> <span class="string">|</span></span><br><span class="line">      <span class="string">&#123;"apiVersion":"v1","kind":"Secret","metadata":&#123;"annotations":&#123;&#125;,"name":"mysecret","namespace":"default"&#125;,"stringData":&#123;"password":"1f2d1e2e67df","username":"admin"&#125;,"type":"Opaque"&#125;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">"2019-09-23T14:16:56Z"</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">"10318365"</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/default/secrets/mysecret</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="number">24602031</span><span class="string">-e18d-467a-b7fe-0962af8ec8b8</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br></pre></td></tr></table></figure><h2 id="同时定义了-data-和-stringData"><a href="#同时定义了-data-和-stringData" class="headerlink" title="同时定义了 data 和 stringData"></a>同时定义了 data 和 stringData</h2><p>如果同时定义了 data 和 stringData，对于两个对象中key 重复的字段，最终将采纳 stringData 中的 value </p><p>创建文件 secret.yaml，该文件同时定义了 data 和 stringData，内容如下所示：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">YWRtaW4=</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">administrator</span></span><br></pre></td></tr></table></figure><p>执行命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f .&#x2F;secret.yaml</span><br><span class="line">secret &quot;mysecret&quot; created</span><br></pre></td></tr></table></figure><p> 执行命令 <code>kubectl get -f ./secret.yaml -o yaml</code> 输出结果如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="number">0425</span><span class="string">]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">-f</span>  <span class="string">secret.yaml</span> <span class="string">-o</span> <span class="string">yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">YWRtaW5pc3RyYXRvcg==</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubectl.kubernetes.io/last-applied-configuration:</span> <span class="string">|</span></span><br><span class="line">      <span class="string">&#123;"apiVersion":"v1","data":&#123;"username":"YWRtaW4="&#125;,"kind":"Secret","metadata":&#123;"annotations":&#123;&#125;,"name":"mysecret","namespace":"default"&#125;,"stringData":&#123;"username":"administrator"&#125;,"type":"Opaque"&#125;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">"2020-04-25T05:46:21Z"</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">"1544704"</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/api/v1/namespaces/default/secrets/mysecret</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">1b1d10a7-87e2-40a4-8101-693aad84eca2</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br></pre></td></tr></table></figure><p> 此处 <code>YWRtaW5pc3RyYXRvcg==</code> 解码后的值是 <code>administrator</code> </p><h2 id="将配置文件存入-Secret"><a href="#将配置文件存入-Secret" class="headerlink" title="将配置文件存入 Secret"></a>将配置文件存入 Secret</h2><p>假设某个应用程序需要从一个配置文件中读取敏感信息，此时，可以将该文件的内容存入 Secret，再通过数据卷的形式挂载到容器。</p><p>例如，应用程序需要读取如下配置文件内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apiUrl: &quot;https:&#x2F;&#x2F;my.api.com&#x2F;api&#x2F;v1&quot;</span><br><span class="line">username: user</span><br><span class="line">password: password</span><br></pre></td></tr></table></figure><p>可以使用下面的 secret.yaml 创建 Secret</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">config.yaml:</span> <span class="string">|-</span></span><br><span class="line">    <span class="attr">apiUrl:</span> <span class="string">"https://my.api.com/api/v1"</span></span><br><span class="line">    <span class="attr">username:</span> <span class="string">user</span></span><br><span class="line">    <span class="attr">password:</span> <span class="string">password</span></span><br></pre></td></tr></table></figure><p>执行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ./secret.yaml</span><br><span class="line">secret <span class="string">"mysecret"</span> created</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl get -f secret.yaml -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  config.yaml: YXBpVXJsOiAiaHR0cHM6Ly9teS5hcGkuY29tL2FwaS92MSIKdXNlcm5hbWU6IHVzZXIKcGFzc3dvcmQ6IHBhc3N3b3Jk</span><br><span class="line">  username: <span class="string">""</span></span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubectl.kubernetes.io/last-applied-configuration: |</span><br><span class="line">      &#123;<span class="string">"apiVersion"</span>:<span class="string">"v1"</span>,<span class="string">"kind"</span>:<span class="string">"Secret"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"mysecret"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"stringData"</span>:&#123;<span class="string">"config.yaml"</span>:<span class="string">"apiUrl: \"https://my.api.com/api/v1\"\nusername: user\npassword: password"</span>&#125;,<span class="string">"type"</span>:<span class="string">"Opaque"</span>&#125;</span><br><span class="line">  creationTimestamp: <span class="string">"2020-04-25T05:46:21Z"</span></span><br><span class="line">  name: mysecret</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: <span class="string">"1545171"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/default/secrets/mysecret</span><br><span class="line">  uid: 1b1d10a7-87e2-40a4-8101-693aad84eca2</span><br><span class="line"><span class="built_in">type</span>: Opaque</span><br></pre></td></tr></table></figure><h1 id="四、解码和编辑Secret"><a href="#四、解码和编辑Secret" class="headerlink" title="四、解码和编辑Secret"></a>四、解码和编辑Secret</h1><h2 id="解码Secret"><a href="#解码Secret" class="headerlink" title="解码Secret"></a>解码Secret</h2><p>Secret 中的信息可以通过 <code>kubectl get secret</code> 命令获取。</p><p>执行命令 <code>kubectl get secret mysecret -o yaml</code> 可获取所创建的 Secret，输出信息如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl get secret mysecret -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  config.yaml: <span class="string">""</span></span><br><span class="line">  password: MWYyZDFlMmU2N2Rm</span><br><span class="line">  username: YWRtaW4=</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubectl.kubernetes.io/last-applied-configuration: |</span><br><span class="line">      &#123;<span class="string">"apiVersion"</span>:<span class="string">"v1"</span>,<span class="string">"kind"</span>:<span class="string">"Secret"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"name"</span>:<span class="string">"mysecret"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"stringData"</span>:&#123;<span class="string">"password"</span>:<span class="string">"1f2d1e2e67df"</span>,<span class="string">"username"</span>:<span class="string">"admin"</span>&#125;,<span class="string">"type"</span>:<span class="string">"Opaque"</span>&#125;</span><br><span class="line">  creationTimestamp: <span class="string">"2020-04-25T05:46:21Z"</span></span><br><span class="line">  name: mysecret</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: <span class="string">"1546981"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/default/secrets/mysecret</span><br><span class="line">  uid: 1b1d10a7-87e2-40a4-8101-693aad84eca2</span><br><span class="line"><span class="built_in">type</span>: Opaque</span><br></pre></td></tr></table></figure><p> 执行命令 <code>echo &#39;MWYyZDFlMmU2N2Rm&#39; | base64 --decode</code> 可解码密码字段，输出结果如下： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># echo 'MWYyZDFlMmU2N2Rm' | base64 --decode</span></span><br><span class="line">1f2d1e2e67df</span><br></pre></td></tr></table></figure><p> 执行命令 <code>echo &#39;YWRtaW4=&#39; | base64 --decode</code> 可解码用户名字段，输出结果如下： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># echo 'YWRtaW4=' | base64 --decode</span></span><br><span class="line">admin</span><br></pre></td></tr></table></figure><h2 id="编辑Secret"><a href="#编辑Secret" class="headerlink" title="编辑Secret"></a>编辑Secret</h2><p>执行命令 <code>kubectl edit secrets mysecret</code> 可以编辑已经创建的 Secret，该命令将打开一个类似于 <code>vi</code> 的文本编辑器，可以直接编辑已经进行 base64 编码的字段，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0425]<span class="comment"># kubectl edit secrets mysecret</span></span><br><span class="line">Edit cancelled, no changes made.</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes Secret 对象可以用来储存敏感信息，例如：密码、OAuth token、ssh 密钥等&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-污点和容忍</title>
    <link href="http://yoursite.com/2020/04/24/K8s-%E6%B1%A1%E7%82%B9%E5%92%8C%E5%AE%B9%E5%BF%8D/"/>
    <id>http://yoursite.com/2020/04/24/K8s-%E6%B1%A1%E7%82%B9%E5%92%8C%E5%AE%B9%E5%BF%8D/</id>
    <published>2020-04-24T07:00:09.000Z</published>
    <updated>2020-04-25T01:39:57.208Z</updated>
    
    <content type="html"><![CDATA[<p> 污点和容忍</p><a id="more"></a> <h1 id="K8s-污点和容忍"><a href="#K8s-污点和容忍" class="headerlink" title="K8s-污点和容忍"></a>K8s-污点和容忍</h1><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>Pod 中存在属性 Node selector / Node affinity，用于将 Pod 指定到合适的节点。</p><p>相对的，节点中存在属性 <code>污点 taints</code>，使得节点可以排斥某些 Pod。</p><p>污点和容忍（taints and tolerations）成对工作，以确保 Pod 不会被调度到不合适的节点上。</p><ul><li>可以为节点增加污点（taints，一个节点可以有 0-N 个污点）</li><li>可以为 Pod 增加容忍（toleration，一个 Pod 可以有 0-N 个容忍）</li><li>如果节点上存在污点，则该节点不会接受任何不能容忍（tolerate）该污点的 Pod</li></ul><h2 id="向节点添加污点"><a href="#向节点添加污点" class="headerlink" title="向节点添加污点"></a>向节点添加污点</h2><ul><li><p>执行 <code>kubectl taint</code> 命令，可向节点添加污点，如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes node1 key=value:NoSchedule</span><br></pre></td></tr></table></figure><p>该命令为节点 <code>node1</code> 添加了一个污点。污点是一个键值对，在本例中，污点的键为 <code>key</code>，值为 <code>value</code>，污点效果为 <code>NoSchedule</code>。此污点意味着 Kubernetes 不会向该节点调度任何 Pod，除非该 Pod 有一个匹配的容忍（toleration）</p></li><li><p>执行如下命令可以将本例中的污点移除：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes node1 key:NoSchedule-</span><br></pre></td></tr></table></figure></li></ul><h2 id="向-Pod-添加容忍"><a href="#向-Pod-添加容忍" class="headerlink" title="向 Pod 添加容忍"></a>向 Pod 添加容忍</h2><p>PodSpec 中有一个 <code>tolerations</code> 字段，可用于向 Pod 添加容忍。下面的两个例子中定义的容忍都可以匹配上例中的污点，包含这些容忍的 Pod 也都可以被调度到 <code>node1</code> 节点上：</p><p>容忍1：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">"key"</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">"Equal"</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">"value"</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure><p>容忍2：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">"key"</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">"Exists"</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure><p>下面这个 Pod 的例子中，使用了容忍：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">env:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">"example-key"</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">"Exists"</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">"NoSchedule"</span></span><br></pre></td></tr></table></figure><h2 id="污点与容忍的匹配"><a href="#污点与容忍的匹配" class="headerlink" title="污点与容忍的匹配"></a>污点与容忍的匹配</h2><p>当满足如下条件时，Kubernetes 认为容忍和污点匹配：</p><ul><li><p>键（key）相同</p></li><li><p>效果（effect）相同</p></li><li><p>污点的 operator 为：</p><ul><li><code>Exists</code> （此时污点中不应该指定 <code>value</code>）</li><li>或者 <code>Equal</code> （此时容忍的 <code>value</code> 应与污点的 <code>value</code> 相同）</li></ul></li><li><p>如果不指定 <code>operator</code>，则其默认为 <code>Equal</code></p></li></ul><p>一个节点上可以有多个污点，同时一个 Pod 上可以有多个容忍。Kubernetes 使用一种类似于过滤器的方法来处理多个节点和容忍：</p><ul><li>对于节点的所有污点，检查 Pod 上是否有匹配的容忍，如果存在匹配的容忍，则忽略该污点；</li><li>剩下的不可忽略的污点将对该 Pod 起作用</li></ul><p>例如：</p><ul><li>如果存在至少一个不可忽略的污点带有效果 <code>NoSchedule</code>，则 Kubernetes 不会将 Pod 调度到该节点上</li><li>如果没有不可忽略的污点带有效果 <code>NoSchedule</code>，但是至少存在一个不可忽略的污点带有效果 <code>PreferNoSchedule</code>，则 Kubernetes 将尽量避免将该 Pod 调度到此节点</li><li>如果存在至少一个忽略的污点带有效果 NoExecute，则：<ul><li>假设 Pod 已经在该节点上运行，Kubernetes 将从该节点上驱逐（evict）该 Pod</li><li>假设 Pod 尚未在该节点上运行，Kubernetes 将不会把 Pod 调度到该节点</li></ul></li></ul><h1 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h1><p>污点和容忍使用起来非常灵活，可以用于：</p><ul><li>避免 Pod 被调度到某些特定的节点</li><li>从节点上驱逐本不应该在该节点运行的 Pod</li></ul><p>具体的场景可能有：</p><ul><li><strong>专属的节点：</strong> 如果您想将一组节点专门用于特定的场景，您可以为这些节点添加污点（例如 <code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>）然后向对应的 Pod 添加容忍。带有这些容忍的 Pod 将可以使用这一组专属节点，同时也可以使用集群中的其他节点。如果您想进一步限制这些 Pod 只能使用这一组节点，那么您应该为这一组节点添加一个标签（例如 dedicated=groupName），并为这一组 Pod 添加 node affinity（或 node selector）以限制这些 Pod 只能调度到这一组节点上。</li><li><strong>带有特殊硬件的节点：</strong> 集群中，如果某一组节点具备特殊的硬件（例如 GPU），此时非常有必要将那些不需要这类硬件的 Pod 从这组节点上排除掉，以便需要这类硬件的 Pod 可以得到资源。此时您可以为这类节点添加污点（例如：<code>kubectl taint nodes nodename special=true:NoSchedule</code> 或者 <code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>）并为需要这类硬件的 Pod 添加匹配的容忍。</li><li><strong>基于污点的驱逐</strong> 当节点出现问题时，可以使用污点以 Pod 为单位从节点上驱逐 Pod。</li></ul><h1 id="基于污点的驱逐（TaintBasedEviction）"><a href="#基于污点的驱逐（TaintBasedEviction）" class="headerlink" title="基于污点的驱逐（TaintBasedEviction）"></a>基于污点的驱逐（TaintBasedEviction）</h1><p>如果有 NoExecute 的污点效果，该效果将对已经运行在节点上的 Pod 施加如下影响：</p><ul><li>不容忍该污点的 Pod 将立刻被驱逐</li><li>容忍该污点的 Pod 在未指定 <code>tolerationSeconds</code> 的情况下，将继续在该节点上运行</li><li>容忍该污点的 Pod 在指定了 <code>tolerationSeconds</code> 的情况下，将在指定时间超过时从节点上驱逐</li></ul><blockquote><p><code>tolerationSeconds</code> 字段可以理解为 Pod 容忍该污点的 <code>耐心</code>：</p><ul><li>超过指定的时间，则达到 Pod 忍耐的极限，Pod 离开所在节点</li><li>不指定 <code>tolerationSeconds</code>，则认为 Pod 对该污点的容忍是无期限的</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 污点和容忍&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-配置</title>
    <link href="http://yoursite.com/2020/04/22/K8s-%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2020/04/22/K8s-%E9%85%8D%E7%BD%AE/</id>
    <published>2020-04-22T08:32:39.000Z</published>
    <updated>2020-04-24T06:58:12.808Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 的配置信息管理</p><a id="more"></a> <h1 id="K8s-配置相关"><a href="#K8s-配置相关" class="headerlink" title="K8s-配置相关"></a>K8s-配置相关</h1><h1 id="一、使用ConfigMap配置您的应用程序"><a href="#一、使用ConfigMap配置您的应用程序" class="headerlink" title="一、使用ConfigMap配置您的应用程序"></a>一、使用ConfigMap配置您的应用程序</h1><p> Kubernetes ConfigMap 可以将配置信息和容器镜像解耦，以使得容器化的应用程序可移植。 </p><h1 id="二、管理容器的计算资源"><a href="#二、管理容器的计算资源" class="headerlink" title="二、管理容器的计算资源"></a>二、管理容器的计算资源</h1><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>在 Kubernetes 中创建工作负载时，您可以为 Pod 中的每一个容器指定其所需要的内存（RAM）大小和 CPU 数量。如果这些信息被指定了，Kubernetes 调度器可以更好的决定将 Pod 调度到哪一个节点。对于容器来说，其所需要的资源也将依据其指定的数值得到保证</p><h3 id="资源类型及计量"><a href="#资源类型及计量" class="headerlink" title="资源类型及计量"></a>资源类型及计量</h3><p>当计算资源的时候，主要是指 CPU 和 内存。CPU 的计量单位是内核的单元数，内存的计量单位是 byte 字节数。应用程序可以按量请求、分配、消耗计算资源</p><h4 id="CPU-的计量"><a href="#CPU-的计量" class="headerlink" title="CPU 的计量"></a>CPU 的计量</h4><p>Kubernetes 中，0.5 代表请求半个 CPU 资源。表达式 0.1 等价于 表达式 100m。在 API Server 中，表达式 0.1 将被转换成 100m，精度低于 1m 的请求是不受支持的。 CPU 的计量代表的是绝对值，而非相对值，例如，请求了 0.1 个 CPU，无论节点是 单核、双核、48核，得到的 CPU 资源都是 0.1 核。 </p><h4 id="内存的计量"><a href="#内存的计量" class="headerlink" title="内存的计量"></a>内存的计量</h4><p>内存的计量单位是 byte 字节。可以使用一个整数来表达内存的大小，也可以使用后缀来表示（E、P、T、G、M、K）。也可以使用 2 的幂数来表示内存大小，其后缀为（Ei、Pi、Ti、Gi、Mi、Ki）。例如，下面的几个表达方式所表示的内存大小大致相等：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">128974848, 129e6, 129M, 123Mi</span><br></pre></td></tr></table></figure><h3 id="容器组及容器的计算资源请求及限制"><a href="#容器组及容器的计算资源请求及限制" class="headerlink" title="容器组及容器的计算资源请求及限制"></a>容器组及容器的计算资源请求及限制</h3><p>Kubernetes 中，可以为容器指定计算资源的请求数量 request 和限制数量 limit。尽管资源的请求/限制数量只能在容器上指定，我们仍然经常讨论容器组的资源请求/限制数量。容器组对某一个类型的资源请求/限制数量是该容器组中所有工作容器对该资源请求/限制数量的求和。</p><h3 id="带有资源请求的容器组是如何调度的"><a href="#带有资源请求的容器组是如何调度的" class="headerlink" title="带有资源请求的容器组是如何调度的"></a>带有资源请求的容器组是如何调度的</h3><p>当创建 Pod 时（直接创建，或者通过控制器创建），Kubernetes 调度程序选择一个节点去运行该 Pod。每一个节点都有一个最大可提供的资源数量：CPU 数量和内存大小。调度程序将确保：对于每一种资源类型，已调度的 Pod 对该资源的请求之和小于该节点最大可供使用资源数量。</p><p>尽管某个节点实际使用的CPU、内存数量非常低，如果新加入一个 Pod 使得该节点上对 CPU 或内存请求的数量之和大于了该节点最大可供使用 CPU 或内存数量，则调度程序不会将该 Pod 分配到该节点。Kubernetes 这样做可以避免在日常的流量高峰时段，节点上出现资源短缺的情况。 </p><h3 id="带有资源限制的容器组是如何运行的"><a href="#带有资源限制的容器组是如何运行的" class="headerlink" title="带有资源限制的容器组是如何运行的"></a>带有资源限制的容器组是如何运行的</h3><p>Kubelet 启动容器组的容器时，将 CPU、内存的最大使用限制作为参数传递给容器引擎。</p><p>以 Docker 容器引擎为例：</p><ul><li>容器的 cpu 请求将转换成 docker 要求的格式，并以 <code>--cpu-shares</code> 标志传递到 <code>docker run</code> 命令</li><li>容器的 cpu 限制将也将转换成 millicore 表达式并乘以 100。结果数字是每 100ms 的周期内，该容器可以使用的 CPU 份额</li><li>容器的内存限制将转换成一个整数，并使用 <code>--memory</code> 标志传递到 <code>docker run</code> 命令</li></ul><p>如下情况可能会发生：</p><ul><li>如果某个容器超出了其内存限制，它可能将被终止。如果 restartPolicy 为 Always 或 OnFailure，kubelet 将重启该容器</li><li>如果某个容器超出了其内存申请（仍低于其内存限制），当节点超出内存使用时，该容器仍然存在从节点驱逐的可能性</li><li>短时间内容器有可能能够超出其 CPU 使用限制运行。kubernetes 并不会终止这些超出 CPU 使用限制的容器</li></ul><h1 id="三、将容器组调度到指定的节点"><a href="#三、将容器组调度到指定的节点" class="headerlink" title="三、将容器组调度到指定的节点"></a>三、将容器组调度到指定的节点</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>在 Kubernetes，可以限定 Pod 只能在特定的节点上运行，或者优先选择在特定的节点上运行。通常您并不需要这样做，而应该交由 kubernetes 调度程序根据资源使用情况自动地为 Pod 分配节点。但是少数情况下，这种限定仍然是必要的，例如：</p><ul><li>确保某些 Pod 被分配到具有固态硬盘的节点</li><li>将相互通信频繁的两个 Pod 分配到同一个高可用区的节点</li></ul><p>Kubernetes 一共提供了四种方法，可以将 Pod 调度到指定的节点上，这些方法从简便到复杂的顺序如下：</p><ul><li>指定节点 nodeName</li><li>节点选择器 nodeSelector（Kubernetes 推荐用法）</li><li>Node isolation/restriction</li><li>Affinity and anti-affinity</li></ul><h2 id="指定节点-nodeName"><a href="#指定节点-nodeName" class="headerlink" title="指定节点 nodeName"></a>指定节点 nodeName</h2><p>nodeName 是四种方法中最简单的一个，但是因为它的局限性，也是使用最少的。nodeName 是 PodSpec 当中的一个字段。如果该字段非空，调度程序直接将其指派到 nodeName 对应的节点上运行。</p><p>通过 nodeName 限定 Pod 所运行的节点有如下局限性：</p><ul><li>如果 nodeName 对应的节点不存在，Pod 将不能运行</li><li>如果 nodeName 对应的节点没有足够的资源，Pod 将运行失败，可能的原因有：OutOfmemory /OutOfcpu</li><li>集群中的 nodeName 通常是变化的（新的集群中可能没有该 nodeName 的节点，指定的 nodeName 的节点可能从集群中移除）</li></ul><h2 id="节点选择器-nodeSelector"><a href="#节点选择器-nodeSelector" class="headerlink" title="节点选择器 nodeSelector"></a>节点选择器 nodeSelector</h2><p>nodeSelector 是 PodSpec 中的一个字段。指定了一组名值对。节点的 labels 中必须包含 Pod 的 nodeSelector 中所有的名值对，该节点才可以运行此 Pod。最普遍的用法中， nodeSelector 只包含一个名值对。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 的配置信息管理&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>搭建NFS Server</title>
    <link href="http://yoursite.com/2020/04/22/%E6%90%AD%E5%BB%BANFS-Server/"/>
    <id>http://yoursite.com/2020/04/22/%E6%90%AD%E5%BB%BANFS-Server/</id>
    <published>2020-04-22T07:40:54.000Z</published>
    <updated>2020-04-25T06:18:54.521Z</updated>
    
    <content type="html"><![CDATA[<p>搭建 NFS 服务器</p> <a id="more"></a> <h1 id="搭建NFS-Server"><a href="#搭建NFS-Server" class="headerlink" title="搭建NFS Server"></a>搭建NFS Server</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Kubernetes 对 Pod 进行调度时，以当时集群中各节点的可用资源作为主要依据，自动选择某一个可用的节点，并将 Pod 分配到该节点上。在这种情况下，Pod 中容器数据的持久化如果存储在所在节点的磁盘上，就会产生不可预知的问题，例如，当 Pod 出现故障，Kubernetes 重新调度之后，Pod 所在的新节点上，并不存在上一次 Pod 运行时所在节点上的数据。</p><p>为了使 Pod 在任何节点上都能够使用同一份持久化存储数据，我们需要使用网络存储的解决方案为 Pod 提供数据卷。常用的网络存储方案有：NFS/cephfs/glusterfs。</p><h2 id="配置要求"><a href="#配置要求" class="headerlink" title="配置要求"></a>配置要求</h2><ul><li>两台 linux 服务器，centos 7<ul><li>一台用作 nfs server</li><li>另一台用作 nfs 客户端</li></ul></li></ul><h2 id="配置NFS服务器"><a href="#配置NFS服务器" class="headerlink" title="配置NFS服务器"></a>配置NFS服务器</h2><p>执行以下命令安装 nfs 服务器所需的软件包</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>执行命令 <code>vim /etc/exports</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;root&#x2F;nfs_root&#x2F; *(insecure,rw,sync,no_root_squash)</span><br></pre></td></tr></table></figure><p>执行以下命令，启动 nfs 服务</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建共享目录，如果要使用自己的目录，请替换本文档中所有的 /root/nfs_root/</span></span><br><span class="line">mkdir /root/nfs_root</span><br><span class="line"></span><br><span class="line">systemctl <span class="built_in">enable</span> rpcbind</span><br><span class="line">systemctl <span class="built_in">enable</span> nfs-server</span><br><span class="line"></span><br><span class="line">systemctl start rpcbind</span><br><span class="line">systemctl start nfs-server</span><br><span class="line">exportfs -r</span><br></pre></td></tr></table></figure><p>检查配置是否生效</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exportfs</span><br><span class="line"><span class="comment"># 输出结果如下所示</span></span><br><span class="line">/root/nfs_root /root/nfs_root</span><br></pre></td></tr></table></figure><h2 id="在客户端测试NFS"><a href="#在客户端测试NFS" class="headerlink" title="在客户端测试NFS"></a>在客户端测试NFS</h2><ul><li>服务器端防火墙开放111、662、875、892、2049的 tcp / udp 允许，否则远端客户无法连接。</li></ul><p>执行以下命令安装 nfs 客户端所需的软件包</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>执行以下命令检查 nfs 服务器端是否有设置共享目录</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># showmount -e $(nfs服务器的IP)</span></span><br><span class="line">showmount -e 172.17.216.82</span><br><span class="line"><span class="comment"># 输出结果如下所示</span></span><br><span class="line">Export list <span class="keyword">for</span> 172.17.216.82:</span><br><span class="line">/root/nfs_root *</span><br></pre></td></tr></table></figure><p>执行以下命令挂载 nfs 服务器上的共享目录到本机路径 <code>/root/nfsmount</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir /root/nfsmount</span><br><span class="line"><span class="comment"># mount -t nfs $(nfs服务器的IP):/root/nfs_root /root/nfsmount</span></span><br><span class="line">mount -t nfs 172.17.216.82:/root/nfs_root /root/nfsmount</span><br><span class="line"><span class="comment"># 写入一个测试文件</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello nfs server"</span> &gt; /root/nfsmount/test.txt</span><br></pre></td></tr></table></figure><p>在 nfs 服务器上执行以下命令，验证文件写入成功</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /root/nfs_root/test.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;搭建 NFS 服务器&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-存储卷PersistentVolume</title>
    <link href="http://yoursite.com/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/"/>
    <id>http://yoursite.com/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/</id>
    <published>2020-04-17T02:58:16.000Z</published>
    <updated>2020-04-22T07:53:40.804Z</updated>
    
    <content type="html"><![CDATA[<p> PV &amp; PVC</p><a id="more"></a> <h1 id="存储卷PersistentVolume"><a href="#存储卷PersistentVolume" class="headerlink" title="存储卷PersistentVolume"></a>存储卷PersistentVolume</h1><p>PersistentVolume（PV 存储卷）是集群中的一块存储空间，由集群管理员管理、或者由 Storage Class（存储类）自动管理。PV（存储卷）和 node（节点）一样，是集群中的资源（kubernetes 集群由存储资源和计算资源组成）。</p><p>PersistentVolumeClaim（存储卷声明）是一种类型的 Volume（数据卷），PersistentVolumeClaim（存储卷声明）引用的 PersistentVolume（存储卷）有自己的生命周期，该生命周期独立于任何使用它的容器组。PersistentVolume（存储卷）描述了如何提供存储的细节信息（NFS、cephfs等存储的具体参数）。</p><p>PersistentVolumeClaim（PVC 存储卷声明）代表用户使用存储的请求。Pod 容器组消耗 node 计算资源，PVC 存储卷声明消耗 PersistentVolume 存储资源。Pod 容器组可以请求特定数量的计算资源（CPU / 内存）；PersistentVolumeClaim 可以请求特定大小/特定访问模式（只能被单节点读写/可被多节点只读/可被多节点读写）的存储资源。</p><p>根据应用程序的特点不同，其所需要的存储资源也存在不同的要求，例如读写性能等。集群管理员必须能够提供关于 PersistentVolume（存储卷）的更多选择，无需用户关心存储卷背后的实现细节。为了解决这个问题，Kubernetes 引入了 StorageClass（存储类）的概念</p><h2 id="存储卷和存储卷声明的关系"><a href="#存储卷和存储卷声明的关系" class="headerlink" title="存储卷和存储卷声明的关系"></a>存储卷和存储卷声明的关系</h2><p>存储卷和存储卷声明的关系如下图所示：</p><ul><li>PersistentVolume 是集群中的存储资源，通常由集群管理员创建和管理</li><li>StorageClass 用于对 PersistentVolume 进行分类，如果正确配置，StorageClass 也可以根据 PersistentVolumeClaim 的请求动态创建 Persistent Volume</li><li>PersistentVolumeClaim 是使用该资源的请求，通常由应用程序提出请求，并指定对应的 StorageClass 和需求的空间大小</li><li>PersistentVolumeClaim 可以做为数据卷的一种，被挂载到容器组/容器中使用</li></ul><p><img src="/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/1.jpg" alt></p><h2 id="存储卷声明的管理过程"><a href="#存储卷声明的管理过程" class="headerlink" title="存储卷声明的管理过程"></a>存储卷声明的管理过程</h2><p>PersistantVolume 和 PersistantVolumeClaim 的管理过程描述如下：</p><blockquote><p>下图主要描述的是 PV 和 PVC 的管理过程，因为绘制空间的问题，将挂载点与Pod关联了，实际结构应该如上图所示：</p><ul><li>Pod 中添加数据卷，数据卷关联PVC</li><li>Pod 中包含容器，容器挂载数据卷</li></ul></blockquote><p><img src="/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/2.png" alt></p><h3 id="1-提供-Provisioning"><a href="#1-提供-Provisioning" class="headerlink" title="1.提供 Provisioning"></a>1.提供 Provisioning</h3><p>有两种方式为 PersistentVolumeClaim 提供 PersistentVolume : 静态、动态</p><ul><li><p><strong>静态提供 Static</strong></p><p>集群管理员实现创建好一系列 PersistentVolume，它们包含了可供集群中应用程序使用的关于实际存储的具体信息。</p></li><li><p><strong>动态提供 Dynamic</strong></p><p>在配置有合适的 StorageClass（存储类）且 PersistentVolumeClaim 关联了该 StorageClass 的情况下，kubernetes 集群可以为应用程序动态创建 PersistentVolume。</p></li></ul><h3 id="2-绑定-Binding"><a href="#2-绑定-Binding" class="headerlink" title="2.绑定 Binding"></a>2.绑定 Binding</h3><p>假设用户创建了一个 PersistentVolumeClaim 存储卷声明，并指定了需求的存储空间大小以及访问模式。Kubernets master 将立刻为其匹配一个 PersistentVolume 存储卷，并将存储卷声明和存储卷绑定到一起。如果一个 PersistentVolume 是动态提供给一个新的 PersistentVolumeClaim，Kubernetes master 会始终将其绑定到该 PersistentVolumeClaim。除此之外，应用程序将被绑定一个不小于（可能大于）其 PersistentVolumeClaim 中请求的存储空间大小的 PersistentVolume。一旦绑定，PersistentVolumeClaim 将拒绝其他 PersistentVolume 的绑定关系。PVC 与 PV 之间的绑定关系是一对一的映射。</p><p>PersistentVolumeClaim 将始终停留在 <strong><em>未绑定 unbound</em></strong> 状态，直到有合适的 PersistentVolume 可用。举个例子：集群中已经存在一个 50Gi 的 PersistentVolume，同时有一个 100Gi 的 PersistentVolumeClaim，在这种情况下，该 PVC 将一直处于 <strong><em>未绑定 unbound</em></strong> 状态，直到管理员向集群中添加了一个 100Gi 的 PersistentVolume。</p><h3 id="3-使用-Using"><a href="#3-使用-Using" class="headerlink" title="3.使用 Using"></a>3.使用 Using</h3><p>对于 Pod 容器组来说，PersistentVolumeClaim 存储卷声明是一种类型的 Volume 数据卷。Kubernetes 集群将 PersistentVolumeClaim 所绑定的 PersistentVolume 挂载到容器组供其使用。</p><h3 id="4-使用中保护-Storage-Object-in-Use-Protection"><a href="#4-使用中保护-Storage-Object-in-Use-Protection" class="headerlink" title="4.使用中保护 Storage Object in Use Protection"></a>4.使用中保护 Storage Object in Use Protection</h3><ul><li>使用中保护的目的是确保正在被容器组使用的 PersistentVolumeClaim 以及其绑定的 PersistentVolume 不能被系统删除，以避免可能的数据丢失。</li><li>如果用户删除一个正在使用中的 PersistentVolumeClaim，则该 PVC 不会立即被移除掉，而是推迟到该 PVC 不在被任何容器组使用时才移除；同样的如果管理员删除了一个已经绑定到 PVC 的 PersistentVolume，则该 PV 也不会立刻被移除掉，而是推迟到其绑定的 PVC 被删除后才移除掉</li></ul><h3 id="5-回收-Reclaiming"><a href="#5-回收-Reclaiming" class="headerlink" title="5.回收 Reclaiming"></a>5.回收 Reclaiming</h3><p>当用户不在需要其数据卷时，可以删除掉其 PersistentVolumeClaim，此时其对应的 PersistentVolume 将被集群回收并再利用。Kubernetes 集群根据 PersistentVolume 中的 reclaim policy（回收策略）决定在其被回收时做对应的处理。当前支持的回收策略有：Retained（保留）、Recycled（重复利用）、Deleted（删除）</p><ul><li><p><strong>保留 Retain</strong></p><p>保留策略需要集群管理员手工回收该资源。当绑定的 PersistentVolumeClaim 被删除后，PersistentVolume 仍然存在，并被认为是”已释放“。但是此时该存储卷仍然不能被其他 PersistentVolumeClaim 绑定，因为前一个绑定的 PersistentVolumeClaim 对应容器组的数据还在其中。集群管理员可以通过如下步骤回收该 PersistentVolume：</p><ul><li>删除该 PersistentVolume。PV 删除后，其数据仍然存在于对应的外部存储介质中（nfs、cefpfs、glusterfs 等）</li><li>手工删除对应存储介质上的数据</li><li>手工删除对应的存储介质，您也可以创建一个新的 PersistentVolume 并再次使用该存储介质</li></ul></li><li><p><strong>删除 Delete</strong></p><p>删除策略将从 kubernete 集群移除 PersistentVolume 以及其关联的外部存储介质（云环境中的 AWA EBS、GCE PD、Azure Disk 或 Cinder volume）。</p></li><li><p><strong>再利用 Recycle</strong></p><ul><li>再利用策略将在 PersistentVolume 回收时，执行一个基本的清除操作（rm -rf /thevolume/*），并使其可以再次被新的 PersistentVolumeClaim 绑定。</li><li>集群管理员也可以自定义一个 recycler pod template，用于执行清除操作。</li></ul></li></ul><h2 id="存储卷类型"><a href="#存储卷类型" class="headerlink" title="存储卷类型"></a>存储卷类型</h2><p>Kubernetes 支持 20 种存储卷类型，如下所示：</p><ul><li>非持久性存储<ul><li>emptyDir</li><li>HostPath (只在单节点集群上用做测试目的)</li></ul></li><li>网络连接性存储<ul><li>SAN：iSCSI、ScaleIO Volumes、FC (Fibre Channel)</li><li>NFS：nfs，cfs</li></ul></li><li>分布式存储<ul><li>Glusterfs</li><li>RBD (Ceph Block Device)</li><li>CephFS</li><li>Portworx Volumes</li><li>Quobyte Volumes</li></ul></li><li>云端存储<ul><li>GCEPersistentDisk</li><li>AWSElasticBlockStore</li><li>AzureFile</li><li>AzureDisk</li><li>Cinder (OpenStack block storage)</li><li>VsphereVolume</li><li>StorageOS</li></ul></li><li>自定义存储<ul><li>FlexVolume</li></ul></li><li>不推荐<ul><li>Flocker</li></ul></li></ul><h1 id="节点相关的数据卷限制"><a href="#节点相关的数据卷限制" class="headerlink" title="节点相关的数据卷限制"></a>节点相关的数据卷限制</h1><p>类似于 Google、Amazon、Microsoft 这样的云供应商，通常都会限定单个节点可挂载的数据卷的最大数量。Kubernetes 必须遵守这些限定，否则，当 Pod 调度上某节点上时，可能会因为不能实现数据卷挂载而启动不了。 </p><h2 id="自定义限制"><a href="#自定义限制" class="headerlink" title="自定义限制"></a>自定义限制</h2><p>修改此限制值的步骤如下：</p><ul><li>设置环境变量 <code>KUBE_MAX_PD_VOLS</code> 的取值</li><li>重启调度器 kube-scheduler</li></ul><p>建议不要将此数值设置得比默认值更大。在修改之前，请认真查询云供应商的相关文档，确保节点机器可以支持设置的限制取值。</p><p>该限定对整个集群生效，因此，将影响到集群中的所有节点。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; PV &amp;amp; PVC&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-数据卷Volume</title>
    <link href="http://yoursite.com/2020/04/16/K8s-%E6%95%B0%E6%8D%AE%E5%8D%B7Volume/"/>
    <id>http://yoursite.com/2020/04/16/K8s-%E6%95%B0%E6%8D%AE%E5%8D%B7Volume/</id>
    <published>2020-04-16T08:09:30.000Z</published>
    <updated>2020-04-17T02:55:30.993Z</updated>
    
    <content type="html"><![CDATA[<p>数据卷Volume及挂载</p> <a id="more"></a> <h1 id="一、数据卷Volume"><a href="#一、数据卷Volume" class="headerlink" title="一、数据卷Volume"></a>一、数据卷Volume</h1><h2 id="数据卷概述"><a href="#数据卷概述" class="headerlink" title="数据卷概述"></a>数据卷概述</h2><p>Kubernetes Volume（数据卷）主要解决了如下两方面问题：</p><ul><li>数据持久性：通常情况下，容器运行起来之后，写入到其文件系统的文件暂时性的。当容器崩溃后，kubelet 将会重启该容器，此时原容器运行后写入的文件将丢失，因为容器将重新从镜像创建。</li><li>数据共享：同一个 Pod（容器组）中运行的容器之间，经常会存在共享文件/文件夹的需求</li></ul><p>Docker 里同样也存在一个 volume（数据卷）的概念，但是 docker 对数据卷的管理相对 kubernetes 而言要更少一些。在 Docker 里，一个 Volume（数据卷）仅仅是宿主机（或另一个容器）文件系统上的一个文件夹。Docker 并不管理 Volume（数据卷）的生命周期。</p><p>在 Kubernetes 里，Volume（数据卷）存在明确的生命周期（与包含该数据卷的容器组相同）。因此，Volume（数据卷）的生命周期比同一容器组中任意容器的生命周期要更长，不管容器重启了多少次，数据都能被保留下来。当然，如果容器组退出了，数据卷也就自然退出了。此时，根据容器组所使用的 Volume（数据卷）类型不同，数据可能随数据卷的退出而删除，也可能被真正持久化，并在下次容器组重启时仍然可以使用。</p><p>从根本上来说，一个 Volume（数据卷）仅仅是一个可被容器组中的容器访问的文件目录（也许其中包含一些数据文件）。这个目录是怎么来的，取决于该数据卷的类型（不同类型的数据卷使用不同的存储介质）。</p><p>使用 Volume（数据卷）时，我们需要先在容器组中定义一个数据卷，并将其挂载到容器的挂载点上。容器中的一个进程所看到（可访问）的文件系统是由容器的 docker 镜像和容器所挂载的数据卷共同组成的。Docker 镜像将被首先加载到该容器的文件系统，任何数据卷都被在此之后挂载到指定的路径上。Volume（数据卷）不能被挂载到其他数据卷上，或者通过引用其他数据卷。同一个容器组中的不同容器各自独立地挂载数据卷，即同一个容器组中的两个容器可以将同一个数据卷挂载到各自不同的路径上。</p><p>我们现在通过下图来理解 容器组、容器、挂载点、数据卷、存储介质（nfs、PVC、ConfigMap）等几个概念之间的关系：</p><ul><li>一个容器组可以包含多个数据卷、多个容器</li><li>一个容器通过挂载点决定某一个数据卷被挂载到容器中的什么路径</li><li>不同类型的数据卷对应不同的存储介质</li></ul><h2 id="数据卷的常用类型"><a href="#数据卷的常用类型" class="headerlink" title="数据卷的常用类型"></a>数据卷的常用类型</h2><h3 id="emptyDir"><a href="#emptyDir" class="headerlink" title="emptyDir"></a>emptyDir</h3><ul><li><p><strong>描述</strong></p><p>emptyDir类型的数据卷在容器组被创建时分配给该容器组，并且直到容器组被移除，该数据卷才被释放。该数据卷初始分配时，始终是一个空目录。同一容器组中的不同容器都可以对该目录执行读写操作，并且共享其中的数据，（尽管不同的容器可能将该数据卷挂载到容器中的不同路径）。当容器组被移除时，emptyDir数据卷中的数据将被永久删除</p><p>容器崩溃时，kubelet 并不会删除容器组，而仅仅是将容器重启，因此 emptyDir 中的数据在容器崩溃并重启后，仍然是存在的。</p></li><li><p><strong>适用场景</strong></p><ul><li>空白的初始空间，例如合并/排序算法中，临时将数据存在磁盘上</li><li>长时间计算中存储检查点（中间结果），以便容器崩溃时，可以从上一次存储的检查点（中间结果）继续进行，而不是从头开始</li><li>作为两个容器的共享存储，使得第一个内容管理的容器可以将生成的页面存入其中，同时由一个 webserver 容器对外提供这些页面</li><li>默认情况下，emptyDir 数据卷被存储在 node（节点）的存储介质（机械硬盘、SSD、或者网络存储）上。此外，您可以设置 emptyDir.medium 字段为 “Memory”，此时 Kubernetes 将挂载一个 tmpfs（基于 RAM 的文件系统）。tmpfs 的读写速度非常快，但是与磁盘不一样，tmpfs 在节点重启后将被清空，且您向该 emptyDir 写入文件时，将消耗对应容器的内存限制。</li></ul></li></ul><h3 id="nfs"><a href="#nfs" class="headerlink" title="nfs"></a>nfs</h3><ul><li><p><strong>描述</strong></p><p>nfs 类型的数据卷可以加载 NFS（Network File System）到容器组/容器。容器组被移除时，将仅仅 umount（卸载）NFS 数据卷，NFS 中的数据仍将被保留。</p><ul><li>可以在加载 NFS 数据卷前就在其中准备好数据；</li><li>可以在不同容器组之间共享数据；</li><li>可以被多个容器组加载并同时读写；</li></ul></li><li><p><strong>适用场景</strong></p><ul><li>存储日志文件</li><li>MySQL的data目录（建议只在测试环境中）</li><li>用户上传的临时文件</li></ul></li></ul><h3 id="cephfs"><a href="#cephfs" class="headerlink" title="cephfs"></a>cephfs</h3><ul><li><p><strong>描述</strong></p><p>cephfs 数据卷可以挂载一个外部 CephFS 卷到容器组中。对于 kubernetes 而言，cephfs 与 nfs 的管理方式和行为完全相似，适用场景也相同。不同的仅仅是背后的存储介质。</p></li><li><p><strong>适用场景</strong></p><p>同 nfs 数据卷</p></li></ul><h3 id="configMap"><a href="#configMap" class="headerlink" title="configMap"></a>configMap</h3><ul><li><p><strong>描述</strong></p><p>ConfigMap 提供了一种向容器组注入配置信息的途径。ConfigMap 中的数据可以被 Pod（容器组）中的容器作为一个数据卷挂载。</p><p>在数据卷中引用 ConfigMap 时：</p><ul><li>可以直接引用整个 ConfigMap 到数据卷，此时 ConfigMap 中的每一个 key 对应一个文件名，value 对应该文件的内容</li><li>也可以只引用 ConfigMap 中的某一个名值对，此时可以将 key 映射成一个新的文件名</li></ul><p>将 ConfigMap 数据卷挂载到容器时，如果该挂载点指定了 <strong><em>数据卷内子路径</em></strong> （subPath），则该 ConfigMap 被改变后，该容器挂载的内容仍然不变。</p></li><li><p><strong>适用场景</strong></p><ul><li>使用 ConfigMap 中的某一 key 作为文件名，对应 value 作为文件内容，替换 nginx 容器中的 /etc/nginx/conf.d/default.conf 配置文件。</li></ul></li></ul><h3 id="secret"><a href="#secret" class="headerlink" title="secret"></a>secret</h3><ul><li><p><strong>描述</strong></p><p>secret 数据卷可以用来注入敏感信息（例如密码）到容器组。可以将敏感信息存入 kubernetes secret 对象，并通过 Volume（数据卷）以文件的形式挂载到容器组（或容器）。secret 数据卷使用 tmpfs（基于 RAM 的文件系统）挂载。</p><p>将 Secret 数据卷挂载到容器时，如果该挂载点指定了 <strong><em>数据卷内子路径</em></strong> （subPath），则该 Secret 被改变后，该容器挂载的内容仍然不变。</p></li><li><p><strong>适用场景</strong></p><ul><li>将 HTTPS 证书存入 kubernets secret，并挂载到 /etc/nginx/conf.d/myhost.crt、/etc/nginx/conf.d/myhost.pem 路径，用来配置 nginx 的 HTTPS 证书</li></ul></li></ul><h3 id="PersistentVolumeClaim"><a href="#PersistentVolumeClaim" class="headerlink" title="PersistentVolumeClaim"></a>PersistentVolumeClaim</h3><ul><li><p><strong>描述</strong></p><p>persistentVolumeClaim 数据卷用来挂载 PersistentVolume 存储卷。</p><p> PersistentVolume 存储卷为用户提供了一种在无需关心具体所在云环境的情况下”声明“ 所需持久化存储的方式。</p></li></ul><h1 id="二、数据卷-挂载"><a href="#二、数据卷-挂载" class="headerlink" title="二、数据卷-挂载"></a>二、数据卷-挂载</h1><p>挂载是指将定义在 Pod 中的数据卷关联到容器，同一个 Pod 中的同一个数据卷可以被挂载到该 Pod 中的多个容器上。</p><h2 id="数据卷内子路径"><a href="#数据卷内子路径" class="headerlink" title="数据卷内子路径"></a>数据卷内子路径</h2><p>同一个 Pod 的不同容器间共享数据卷。使用 <code>volumeMounts.subPath</code> 属性，可以使容器在挂载数据卷时指向数据卷内部的一个子路径，而不是直接指向数据卷的根路径。</p><p>下面的例子中，一个 LAMP（Linux Apache Mysql PHP）应用的 Pod 使用了一个共享数据卷，HTML 内容映射到数据卷的 <code>html</code> 目录，数据库的内容映射到了 <code>mysql</code> 目录： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-lamp-site</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">    <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">mysql</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MYSQL_ROOT_PASSWORD</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">"rootpasswd"</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/lib/mysql</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">        <span class="attr">subPath:</span> <span class="string">mysql</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">php</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">php:7.0-apache</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/www/html</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">        <span class="attr">subPath:</span> <span class="string">html</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">my-lamp-site-data</span></span><br></pre></td></tr></table></figure><h3 id="通过环境变量指定数据卷内子路径"><a href="#通过环境变量指定数据卷内子路径" class="headerlink" title="通过环境变量指定数据卷内子路径"></a>通过环境变量指定数据卷内子路径</h3><p>使用 <code>volumeMounts.subPathExpr</code> 字段，可以通过容器的环境变量指定容器内路径。使用此特性时，必须启用 <code>VolumeSubpathEnvExpansion</code>（自 Kubernetes v1.15 开始，是默认启用的。） </p><p>如下面的例子，该 Pod 使用 <code>subPathExpr</code> 在 hostPath 数据卷 <code>/var/log/pods</code> 中创建了一个目录 <code>pod1</code>（该参数来自于Pod的名字）。此时，宿主机目录 <code>/var/log/pods/pod1</code> 挂载到了容器的 <code>/logs</code> 路径： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container1</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAME</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">[</span> <span class="string">"sh"</span><span class="string">,</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt"</span> <span class="string">]</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir1</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/logs</span></span><br><span class="line">      <span class="attr">subPathExpr:</span> <span class="string">$(POD_NAME)</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir1</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/log/pods</span></span><br></pre></td></tr></table></figure><h2 id="容器内路径"><a href="#容器内路径" class="headerlink" title="容器内路径"></a>容器内路径</h2><p><code>mountPath</code> 数据卷被挂载到容器的路径，不能包含 <code>:</code></p><h2 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h2><p>容器对挂载的数据卷是否具备读写权限，如果 <code>readOnly</code> 为 <code>true</code>，则只读，否则可以读写（为 <code>false</code> 或者不指定）。默认为 <code>false</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;数据卷Volume及挂载&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>Kubernetes网络模型</title>
    <link href="http://yoursite.com/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-04-14T06:55:20.000Z</published>
    <updated>2020-04-16T07:54:50.876Z</updated>
    
    <content type="html"><![CDATA[<p>Container-to-Container 的网络</p><p>Pod-to-Pod 的网络</p><p>Pod-to-Service 的网络</p><p>Internet-to-Service 的网络</p><a id="more"></a> <h1 id="Kubernetes网络模型"><a href="#Kubernetes网络模型" class="headerlink" title="Kubernetes网络模型"></a>Kubernetes网络模型</h1><h2 id="一、Kubernetes基本概念"><a href="#一、Kubernetes基本概念" class="headerlink" title="一、Kubernetes基本概念"></a>一、Kubernetes基本概念</h2><p>Kubernetes 基于少数几个核心概念，不断完善，提供了非常丰富和实用的功能。本章节罗列了这些核心概念，并简要的做了概述，以便更好地支持后面的讨论。熟悉 Kubernetes 的读者可跳过这个章节。</p><h3 id="Kubernetes-API-Server"><a href="#Kubernetes-API-Server" class="headerlink" title="Kubernetes API Server"></a>Kubernetes API Server</h3><p>操作 Kubernetes 的方式，是调用 Kubernetes API Server（kube-apiserver）的 API 接口。kubectl、kubernetes dashboard、kuboard 都是通过调用 kube-apiserver 的接口实现对 kubernetes 的管理。API server 最终将集群状态的数据存储在 <a href="https://github.com/coreos/etcd" target="_blank" rel="noopener">etcd</a> 中。</p><h3 id="控制器Controller"><a href="#控制器Controller" class="headerlink" title="控制器Controller"></a>控制器Controller</h3><p>控制器（Controller）是 Kubernetes 中最核心的抽象概念。在用户通过 kube-apiserver 声明了期望的状态以后，控制器通过不断监控 apiserver 中的当前状态，并对当前状态与期望状态之间的差异做出反应，以确保集群的当前状态不断地接近用户声明的期望状态。这个过程实现在一个循环中，参考如下伪代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">while <span class="literal">true</span>:</span><br><span class="line">  X = currentState()</span><br><span class="line">  Y = desiredState()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> X == Y:</span><br><span class="line">    <span class="keyword">return</span>  # Do nothing</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    do(tasks to get to Y)</span><br></pre></td></tr></table></figure><p>例如，当你通过 API Server 创建一个新的 Pod 对象时，Kubernetes调度器（是一个控制器）注意到此变化，并做出将该 Pod 运行在集群中哪个节点的决定。然后，通过 API Server 修改 Pod 对象的状态。此时，对应节点上的kubelet（是一个控制器）注意到此变化，并将在其所在节点运行该 Pod，设置需要的网络，使 Pod 在集群内可以访问。此处，两个控制器针对不同的状态变化做出反应，以使集群的当前状态与用户指定的期望状态匹配。</p><h3 id="容器组Pod"><a href="#容器组Pod" class="headerlink" title="容器组Pod"></a>容器组Pod</h3><p>Pod 是 Kubernetes 中的最小可部署单元。一个 Pod 代表了集群中运行的一个工作负载，可以包括一个或多个 docker 容器、挂载需要的存储，并拥有唯一的 IP 地址。Pod 中的多个容器将始终在同一个节点上运行。</p><h3 id="节点Node"><a href="#节点Node" class="headerlink" title="节点Node"></a>节点Node</h3><p>节点是Kubernetes集群中的一台机器，可以是物理机，也可以是虚拟机。</p><h2 id="二、Kubernetes网络模型"><a href="#二、Kubernetes网络模型" class="headerlink" title="二、Kubernetes网络模型"></a>二、Kubernetes网络模型</h2><p>关于 Pod 如何接入网络这件事情，Kubernetes 做出了明确的选择。具体来说，Kubernetes 要求所有的网络插件实现必须满足如下要求：</p><ul><li>所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射（network address translation）</li><li>所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射</li><li>Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个</li></ul><p>在这些限制条件下，需要解决如下四种完全不同的网络使用场景的问题：</p><ol><li>Container-to-Container 的网络</li><li>Pod-to-Pod 的网络</li><li>Pod-to-Service 的网络</li><li>Internet-to-Service 的网络</li></ol><h2 id="三、Container-to-Container的网络"><a href="#三、Container-to-Container的网络" class="headerlink" title="三、Container-to-Container的网络"></a>三、Container-to-Container的网络</h2><p>Linux系统中，每一个进程都在一个 network namespace 中进行通信，network namespace 提供了一个逻辑上的网络堆栈（包含自己的路由、防火墙规则、网络设备）。换句话说，network namespace 为其中的所有进程提供了一个全新的网络堆栈。 </p><p>Linux 用户可以使用 <code>ip</code> 命令创建 network namespace。</p><p>例如，下面的命令创建了一个新的 network namespace 名称为 <code>ns1</code>：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns add ns1</span><br></pre></td></tr></table></figure><p>当创建 network namespace 时，同时将在 <code>/var/run/netns</code> 下创建一个挂载点（mount point）用于存储该 namespace 的信息。</p><p>执行 <code>ls /var/run/netns</code> 命令，或执行 <code>ip</code> 命令，可以查看所有的 network namespace： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls /var/run/netns</span><br><span class="line">ns1</span><br><span class="line">$ ip netns</span><br><span class="line">ns1</span><br></pre></td></tr></table></figure><p>默认情况下，Linux 将所有的进程都分配到 root network namespace，以使得进程可以访问外部网络，如下图所示： </p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/1.png" alt></p><p>在 Kubernetes 中，Pod 是一组 docker 容器的集合，这一组 docker 容器将共享一个 network namespace。Pod 中所有的容器都：</p><ul><li>使用该 network namespace 提供的同一个 IP 地址以及同一个端口空间</li><li>可以通过 localhost 直接与同一个 Pod 中的另一个容器通信</li></ul><p>Kubernetes 为每一个 Pod 都创建了一个 network namespace。具体做法是，把一个 Docker 容器当做 “Pod Container” 用来获取 network namespace，在创建 Pod 中新的容器时，都使用 docker run 的 <code>--network:container</code> 功能来加入该 network namespace。</p><p>如下图所示，每一个 Pod 都包含了多个 docker 容器（<code>ctr*</code>），这些容器都在同一个共享的 network namespace 中：</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/2.png" alt></p><p>此外，Pod 中可以定义数据卷，Pod 中的容器都可以共享这些数据卷，并通过挂载点挂载到容器内部不同的路径。</p><h2 id="四、Pod-to-Pod的网络"><a href="#四、Pod-to-Pod的网络" class="headerlink" title="四、Pod-to-Pod的网络"></a>四、Pod-to-Pod的网络</h2><p>在 Kubernetes 中，每一个 Pod 都有一个真实的 IP 地址，并且每一个 Pod 都可以使用此 IP 地址与 其他 Pod 通信。Pod-to-Pod 通信中使用真实 IP ，不管两个 Pod 是在同一个节点上，还是集群中的不同节点上。</p><p>从 Pod 的视角来看，Pod 是在其自身所在的 network namespace 与同节点上另外一个 network namespace 进程通信。在Linux上，不同的 network namespace 可以通过 <strong>Virtual Ethernet Device</strong> 或 <strong><em>veth pair</em></strong> (两块跨多个名称空间的虚拟网卡)进行通信。为连接 pod 的 network namespace，可以将 <strong><em>veth pair</em></strong> 的一段指定到 root network namespace，另一端指定到 Pod 的 network namespace。每一组 <strong><em>veth pair</em></strong> 类似于一条网线，连接两端，并可以使流量通过。节点上有多少个 Pod，就会设置多少组 <strong><em>veth pair</em></strong>。下图展示了 veth pair 连接 Pod 到 root namespace 的情况：</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/3.png" alt></p><p>此时，每个Pod 都有了自己的 network namespace，从 Pod 的角度来看，他们都有自己的以太网卡以及 IP 地址，并且都连接到了节点的 root network namespace。为了让 Pod 可以互相通过 root network namespace 通信，通过使用 network bridge（网桥）。</p><p>Linux Ethernet bridge 是一个虚拟的 Layer 2 网络设备，可用来连接两个或多个网段（network segment）。网桥的工作原理是，在源于目标之间维护一个转发表（forwarding table），通过检查通过网桥的数据包的目标地址（destination）和该转发表来决定是否将数据包转发到与网桥相连的另一个网段。桥接代码通过网络中具备唯一性的网卡MAC地址来判断是否桥接或丢弃数据。</p><p>网桥实现了 <code>ARP</code> 协议，以发现链路层与 IP 地址绑定的 MAC 地址。当网桥收到数据帧时，网桥将该数据帧广播到所有连接的设备上（除了发送者以外），对该数据帧做出相应的设备被记录到一个查找表中（lookup table）。后续网桥再收到发向同一个 IP 地址的流量时，将使用查找表（lookup table）来找到对应的 MAC 地址，并转发数据包。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/4.png" alt></p><h3 id="数据包的传递：Pod-to-Pod，同节点"><a href="#数据包的传递：Pod-to-Pod，同节点" class="headerlink" title="数据包的传递：Pod-to-Pod，同节点"></a>数据包的传递：Pod-to-Pod，同节点</h3><p>在 network namespace 将每一个 Pod 隔离到各自的网络堆栈的情况下，虚拟以太网设备（virtual Ethernet device）将每一个 namespace 连接到 root namespace，网桥将 namespace 又连接到一起，此时，Pod 可以向同一节点上的另一个 Pod 发送网络报文了。下图演示了同节点上，网络报文从一个Pod传递到另一个Pod的情况。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/5.gif" alt></p><p>Pod1 发送一个数据包到其自己的默认以太网设备 <code>eth0</code>。</p><ol><li>对 Pod1 来说，<code>eth0</code> 通过虚拟以太网设备（veth0）连接到 root namespace</li><li>网桥 <code>cbr0</code> 中为 <code>veth0</code> 配置了一个网段。一旦数据包到达网桥，网桥使用 <code>ARP</code> 协议解析出其正确的目标网段 <code>veth1</code></li><li>网桥 <code>cbr0</code> 将数据包发送到 <code>veth1</code></li><li>数据包到达 <code>veth1</code> 时，被直接转发到 Pod2 的 network namespace 中的 <code>eth0</code> 网络设备。</li></ol><p>在整个数据包传递过程中，每一个 Pod 都只和 <code>localhost</code> 上的 <code>eth0</code> 通信，且数包被路由到正确的 Pod 上。</p><p>Kubernetes 的网络模型规定，在跨节点的情况下 Pod 也必须可以通过 IP 地址访问。也就是说，Pod 的 IP 地址必须始终对集群中其他 Pod 可见；且从 Pod 内部和从 Pod 外部来看，Pod 的IP地址都是相同的。</p><h3 id="数据包的传递：Pod-to-Pod，跨节点"><a href="#数据包的传递：Pod-to-Pod，跨节点" class="headerlink" title="数据包的传递：Pod-to-Pod，跨节点"></a>数据包的传递：Pod-to-Pod，跨节点</h3><p>Kubernetes 网络模型要求 Pod 的 IP 在整个网络中都可访问，但是并不指定如何实现这一点。实际上，这是所使用网络插件相关的，但是，仍然有一些模式已经被确立了。</p><p>通常，集群中每个节点都被分配了一个 CIDR 网段，指定了该节点上的 Pod 可用的 IP 地址段。一旦发送到该 CIDR 网段的流量到达节点，就由节点负责将流量继续转发给对应的 Pod。下图展示了两个节点之间的数据报文传递过程。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/6.gif" alt></p><p>图中，目标 Pod（以绿色高亮）与源 Pod（以蓝色高亮）在不同的节点上，数据包传递过程如下：</p><ol><li>数据包从 Pod1 的网络设备 <code>eth0</code>，该设备通过 <code>veth0</code> 连接到 root namespace</li><li>数据包到达 root namespace 中的网桥 <code>cbr0</code></li><li>网桥上执行 ARP 将会失败，因为与网桥连接的所有设备中，没有与该数据包匹配的 MAC 地址。一旦 ARP 失败，网桥会将数据包发送到默认路由（root namespace 中的 <code>eth0</code> 设备）。此时，数据包离开节点进入网络</li><li>假设网络可以根据各节点的CIDR网段，将数据包路由到正确的节点</li><li>数据包进入目标节点的 root namespace（VM2 上的 <code>eth0</code>）后，通过网桥路由到正确的虚拟网络设备（<code>veth1</code>）</li><li>最终，数据包通过 <code>veth1</code> 发送到对应 Pod 的 <code>eth0</code>，完成了数据包传递的过程</li></ol><p>通常来说，每个节点知道如何将数据包分发到运行在该节点上的 Pod。一旦一个数据包到达目标节点，数据包的传递方式与同节点上不同Pod之间数据包传递的方式就是一样的了。</p><p>此处，我们直接跳过了如何配置网络，以使得数据包可以从一个节点路由到匹配的节点。这些是与具体的网络插件实现相关的。</p><p><code>Container Network Interface(CNI) plugin</code> 提供了一组通用 API 用来连接容器与外部网络。具体到容器化应用开发者来说，只需要了解在整个集群中，可以通过 Pod 的 IP 地址直接访问 Pod；网络插件是如何做到跨节点的数据包传递这件事情对容器化应用来说是透明的。</p><h2 id="五、Pod-to-Service的网络"><a href="#五、Pod-to-Service的网络" class="headerlink" title="五、Pod-to-Service的网络"></a>五、Pod-to-Service的网络</h2><p>Pod 可以通过 IP 地址之间传递数据包，但是，Pod 的 IP 地址并非是固定不变的，随着 Pod 的重新调度（例如水平伸缩、应用程序崩溃、节点重启等），Pod 的 IP 地址将会出现又消失。此时，Pod 的客户端无法得知该访问哪一个 IP 地址。Kubernetes 中，Service 的概念用于解决此问题。 </p><p>一个 Kubernetes Service 管理了一组 Pod 的状态，可以追踪一组 Pod 的 IP 地址的动态变化过程。一个 Service 拥有一个 IP 地址，并且充当了一组 Pod 的 IP 地址的“虚拟 IP 地址”。任何发送到 Service 的 IP 地址的数据包将被负载均衡到该 Service 对应的 Pod 上。在此情况下，Service 关联的 Pod 可以随时间动态变化，客户端只需要知道 Service 的 IP 地址即可（该地址不会发生变化）。</p><p>从效果上来说，Kubernetes 自动为 Service 创建和维护了集群内部的分布式负载均衡，可以将发送到 Service IP 地址的数据包分发到 Service 对应的健康的 Pod 上。</p><h3 id="netfilter-and-iptables"><a href="#netfilter-and-iptables" class="headerlink" title="netfilter and iptables"></a>netfilter and iptables</h3><p>Kubernetes 利用 Linux 内建的网络框架 - <code>netfilter</code> 来实现负载均衡。Netfilter 是由 Linux 提供的一个框架，可以通过自定义 handler 的方式来实现多种网络相关的操作。Netfilter 提供了许多用于数据包过滤、网络地址转换、端口转换的功能，通过这些功能，自定义的 handler 可以在网络上转发数据包、禁止数据包发送到敏感的地址等。</p><p><code>iptables</code> 是一个 user-space 应用程序，可以提供基于决策表的规则系统，以使用 netfilter 操作或转换数据包。在 Kubernetes 中，kube-proxy 控制器监听 apiserver 中的变化，并配置 iptables 规则。当 Service 或 Pod 发生变化时（例如 Service 被分配了 IP 地址，或者新的 Pod 被关联到 Service），kube-proxy 控制器将更新 iptables 规则，以便将发送到 Service 的数据包正确地路由到其后端 Pod 上。iptables 规则将监听所有发向 Service 的虚拟 IP 的数据包，并将这些数据包转发到该Service 对应的一个随机的可用 Pod 的 IP 地址，同时 iptables 规则将修改数据包的目标 IP 地址（从 Service 的 IP 地址修改为选中的 Pod 的 IP 地址）。当 Pod 被创建或者被终止时，iptables 的规则也被对应的修改。换句话说，iptables 承担了从 Service IP 地址到实际 Pod IP 地址的负载均衡的工作。</p><p>在返回数据包的路径上，数据包从目标 Pod 发出，此时，iptables 规则又将数据包的 IP 头从 Pod 的 IP 地址替换为 Service 的 IP 地址。从请求的发起方来看，就好像始终只是在和 Service 的 IP 地址通信一样。</p><h3 id="IPVS"><a href="#IPVS" class="headerlink" title="IPVS"></a>IPVS</h3><p>Kubernetes v1.11 开始，提供了另一个选择用来实现集群内部的负载均衡：IPVS。</p><p>IPVS（IP Virtual Server）也是基于 netfilter 构建的，在 Linux 内核中实现了传输层的负载均衡。</p><p>IPVS 被合并到 LVS（Linux Virtual Server）当中，充当一组服务器的负载均衡器。</p><p>IPVS 可以转发 TCP / UDP 请求到实际的服务器上，使得一组实际的服务器看起来像是只通过一个单一 IP 地址访问的服务一样。IPVS 的这个特点天然适合与用在 Kubernetes Service 的这个场景下。</p><p>当声明一个 Kubernetes Service 时，可以指定是使用 iptables 还是 IPVS 来提供集群内的负载均衡工作。</p><p>IPVS 是转为负载均衡设计的，并且使用更加有效率的数据结构（hash tables），相较于 iptables，可以支持更大数量的网络规模。当创建使用 IPVS 形式的 Service 时，Kubernetes 执行了如下三个操作：</p><ul><li>在节点上创建一个 dummy IPVS interface</li><li>将 Service 的 IP 地址绑定到该 dummy IPVS interface</li><li>为每一个 Service IP 地址创建 IPVS 服务器</li></ul><p>将来，IPVS 有可能成为 kubernetes 中默认的集群内负载均衡方式。这个改变将只影响到集群内的负载均衡，以 iptables 为例子，所有讨论对 IPVS 是同样适用。</p><h3 id="数据包的传递：Pod-to-Service"><a href="#数据包的传递：Pod-to-Service" class="headerlink" title="数据包的传递：Pod-to-Service"></a>数据包的传递：Pod-to-Service</h3><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/7.gif" alt></p><p>在 Pod 和 Service 之间路由数据包时，数据包的发起和以前一样：</p><ol><li>数据包首先通过 Pod 的 <code>eth0</code> 网卡发出</li><li>数据包经过虚拟网卡 <code>veth0</code> 到达网桥 <code>cbr0</code></li><li>网桥上的 APR 协议查找不到该 Service，所以数据包被发送到 root namespace 中的默认路由 - <code>eth0</code></li><li>此时，在数据包被 <code>eth0</code> 接受之前，数据包将通过 iptables 过滤。iptables 使用其规则（由 kube-proxy 根据 Service、Pod 的变化在节点上创建的 iptables 规则）重写数据包的目标地址（从 Service 的 IP 地址修改为某一个具体 Pod 的 IP 地址）</li><li>数据包现在的目标地址是 Pod 4，而不是 Service 的虚拟 IP 地址。iptables 使用 Linux 内核的 <code>conntrack</code> 工具包来记录具体选择了哪一个 Pod，以便可以将未来的数据包路由到同一个 Pod。简而言之，iptables 直接在节点上完成了集群内负载均衡的功能。数据包后续如何发送到 Pod 上，其路由方式与Pod-to-Pod的网络中相同。</li></ol><h3 id="数据包的传递：Service-to-Pod"><a href="#数据包的传递：Service-to-Pod" class="headerlink" title="数据包的传递：Service-to-Pod"></a>数据包的传递：Service-to-Pod</h3><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/8.gif" alt></p><ol><li>接收到此请求的 Pod 将会发送返回数据包，其中标记源 IP 为接收请求 Pod 自己的 IP，目标 IP 为最初发送对应请求的 Pod 的 IP</li><li>当数据包进入节点后，数据包将经过 iptables 的过滤，此时记录在 <code>conntrack</code> 中的信息将被用来修改数据包的源地址（从接收请求的 Pod 的 IP 地址修改为 Service 的 IP 地址）</li><li>然后，数据包将通过网桥、以及虚拟网卡 <code>veth0</code></li><li>最终到达 Pod 的网卡 <code>eth0</code></li></ol><h3 id="使用DNS"><a href="#使用DNS" class="headerlink" title="使用DNS"></a>使用DNS</h3><p>Kubernetes 也可以使用 DNS，以避免将 Service 的 cluster IP 地址硬编码到应用程序当中。Kubernetes DNS 是 Kubernetes 上运行的一个普通的 Service。每一个节点上的 <code>kubelet</code> 都使用该 DNS Service 来执行 DNS 名称的解析。集群中每一个 Service（包括 DNS Service 自己）都被分配了一个 DNS 名称。DNS 记录将 DNS 名称解析到 Service 的 ClusterIP 或者 Pod 的 IP 地址。SRV 记录用来指定 Service 的已命名端口。</p><p>DNS Pod 由三个不同的容器组成：</p><ul><li><code>kubedns</code>：观察 Kubernetes master 上 Service 和 Endpoints 的变化，并维护内存中的 DNS 查找表</li><li><code>dnsmasq</code>：添加 DNS 缓存，以提高性能</li><li><code>sidecar</code>：提供一个健康检查端点，可以检查 <code>dnsmasq</code> 和 <code>kubedns</code> 的健康状态</li></ul><p>DNS Pod 被暴露为 Kubernetes 中的一个 Service，该 Service 及其 ClusterIP 在每一个容器启动时都被传递到容器中（环境变量及 /etc/resolves），因此，每一个容器都可以正确的解析 DNS。DNS 条目最终由 <code>kubedns</code> 解析，<code>kubedns</code> 将 DNS 的所有信息都维护在内存中。<code>etcd</code> 中存储了集群的所有状态，<code>kubedns</code> 在必要的时候将 <code>etcd</code> 中的 key-value 信息转化为 DNS 条目信息，以重建内存中的 DNS 查找表。</p><p>CoreDNS 的工作方式与 <code>kubedns</code> 类似，但是通过插件化的架构构建，因而灵活性更强。自 Kubernetes v1.11 开始，CoreDNS 是 Kubernetes 中默认的 DNS 实现。</p><h2 id="六、Internet-to-Service的网络"><a href="#六、Internet-to-Service的网络" class="headerlink" title="六、Internet-to-Service的网络"></a>六、Internet-to-Service的网络</h2><ul><li>从集群内部访问互联网</li><li>从互联网访问集群内部</li></ul><h3 id="出方向-从集群内部访问互联网"><a href="#出方向-从集群内部访问互联网" class="headerlink" title="出方向 - 从集群内部访问互联网"></a>出方向 - 从集群内部访问互联网</h3><p>将网络流量从集群内的一个节点路由到公共网络是与具体网络以及实际网络配置紧密相关的。为了更加具体地讨论此问题，本文将使用 AWS VPC 来讨论其中的具体问题。</p><p>在 AWS，Kubernetes 集群在 VPC 内运行，在此处，每一个节点都被分配了一个内网地址（private IP address）可以从 Kubernetes 集群内部访问。为了使访问外部网络，通常会在 VPC 中添加互联网网关（Internet Gateway），以实现如下两个目的：</p><ul><li>作为 VPC 路由表中访问外网的目标地址</li><li>提供网络地址转换（NAT Network Address Translation），将节点的内网地址映射到一个外网地址，以使外网可以访问内网上的节点</li></ul><p>在有互联网网关（Internet Gateway）的情况下，虚拟机可以任意访问互联网。但是，存在一个小问题：Pod 有自己的 IP 地址，且该 IP 地址与其所在节点的 IP 地址不一样，并且，互联网网关上的 NAT 地址映射只能够转换节点（虚拟机）的 IP 地址，因为网关不知道每个节点（虚拟机）上运行了哪些 Pod （互联网网关不知道 Pod 的存在）。那么 Kubernetes 是如何使用 iptables 解决此问题的。</p><h4 id="数据包的传递：Node-to-Internet"><a href="#数据包的传递：Node-to-Internet" class="headerlink" title="数据包的传递：Node-to-Internet"></a>数据包的传递：Node-to-Internet</h4><p>下图中：</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/9.gif" alt></p><ol><li>数据包从 Pod 的 network namespace 发出</li><li>通过 <code>veth0</code> 到达虚拟机的 root network namespace</li><li>由于网桥上找不到数据包目标地址对应的网段，数据包将被网桥转发到 root network namespace 的网卡 <code>eth0</code>。在数据包到达 <code>eth0</code> 之前，iptables 将过滤该数据包。</li><li>在此处，数据包的源地址是一个 Pod，如果仍然使用此源地址，互联网网关将拒绝此数据包，因为其 NAT 只能识别与节点（虚拟机）相连的 IP 地址。因此，需要 iptables 执行源地址转换（source NAT），这样子，对互联网网关来说，该数据包就是从节点（虚拟机）发出的，而不是从 Pod 发出的</li><li>数据包从节点（虚拟机）发送到互联网网关</li><li>互联网网关再次执行源地址转换（source NAT），将数据包的源地址从节点（虚拟机）的内网地址修改为网关的外网地址，最终数据包被发送到互联网</li></ol><p>在回路径上，数据包沿着相同的路径反向传递，源地址转换（source NAT）在对应的层级上被逆向执行。</p><h3 id="入方向-从互联网访问Kubernetes"><a href="#入方向-从互联网访问Kubernetes" class="headerlink" title="入方向 - 从互联网访问Kubernetes"></a>入方向 - 从互联网访问Kubernetes</h3><ol><li>Service LoadBalancer</li><li>Ingress Controller</li></ol><h4 id="4-层：LoadBalancer"><a href="#4-层：LoadBalancer" class="headerlink" title="4 层：LoadBalancer"></a>4 层：LoadBalancer</h4><p>当创建 Kubernetes Service 时，可以指定其类型为 LoadBalancer。 LoadBalancer 的实现由 cloud controller 提供，cloud controller 可以调用云供应商 IaaS 层的接口，为 Kubernetes Service 创建负载均衡器（如果是自建 Kubernetes 集群，可以使用 NodePort 类型的 Service，并手动创建负载均衡器）。用户可以将请求发送到负载均衡器来访问 Kubernetes 中的 Service。</p><p>在 AWS，负载均衡器可以将网络流量分发到其目标服务器组（即 Kubernetes 集群中的所有节点）。一旦数据包到达节点，Service 的 iptables 规则将确保其被转发到 Service 的一个后端 Pod。</p><h4 id="数据包的传递：LoadBalancer-to-Service"><a href="#数据包的传递：LoadBalancer-to-Service" class="headerlink" title="数据包的传递：LoadBalancer-to-Service"></a>数据包的传递：LoadBalancer-to-Service</h4><p>接下来了解一下 Layer 4 的入方向访问具体是如何做到的：</p><ol><li>Loadbalancer 类型的 Service 创建后，cloud controller 将为其创建一个负载均衡器</li><li>负载均衡器只能直接和节点（虚拟机沟通），不知道 Pod 的存在，当数据包从请求方（互联网）到达 LoadBalancer 之后，将被分发到集群的节点上</li><li>节点上的 iptables 规则将数据包转发到合适的 Pod 上</li></ol><p>从 Pod 到请求方的相应数据包将包含 Pod 的 IP 地址，但是请求方需要的是负载均衡器的 IP 地址。iptables 和 <code>conntrack</code> 被用来重写返回路径上的正确的 IP 地址。</p><p>下图描述了一个负载均衡器和三个集群节点：</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/10.gif" alt></p><ol><li>请求数据包从互联网发送到负载均衡器</li><li>负载均衡器将数据包随机分发到其中的一个节点（虚拟机），此处，我们假设数据包被分发到了一个没有对应 Pod 的节点（VM2）上</li><li>在 VM2 节点上，kube-proxy 在节点上安装的 iptables 规则会将该数据包的目标地址判定到对应的 Pod 上（集群内负载均衡将生效）</li><li>iptables 完成 NAT 映射，并将数据包转发到目标 Pod</li></ol><h4 id="7-层：Ingress控制器"><a href="#7-层：Ingress控制器" class="headerlink" title="7 层：Ingress控制器"></a>7 层：Ingress控制器</h4><p>Layer 7 网络入方向访问在网络堆栈的 HTTP/HTTPS 协议层面工作，并且依赖于 KUbernetes Service。要实现 Layer 7 网络入方向访问，首先需要将 Service 指定为 <code>NodtePort</code> 类型，此时 Kubernetes master 将会为该 Service 分配一个<strong>节点端口</strong>，每一个节点上的 iptables 都会将此端口上的请求转发到 Service 的后端 Pod 上。此时，Service-to-Pod 的路由与数据包的传递：Service-to-Pod的描述相同。</p><p>接下来，创建一个 Kubernetes Ingress 对象可以将该 Service 发布到互联网。Ingress 是一个高度抽象的 HTTP 负载均衡器，可以将 HTTP 请求映射到 Kubernetes Service。在不同的 Kubernetes 集群中，Ingress 的具体实现可能是不一样的。与 Layer 4 的网络负载均衡器相似，HTTP 负载均衡器只理解节点的 IP 地址（而不是 Pod 的 IP 地址），因此，也同样利用了集群内部通过 iptables 实现的负载均衡特性。</p><p>在 AWS 中，ALB Ingress 控制器使用 Amazon 的 Layer 7 Application Load Balancer实现了 Kubernetes Ingress 的功能。下图展示了 AWS 上 Ingress 控制器的细节，也展示了网络请求是如何从 ALB 路由到 Kubernetes 集群的。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/11.png" alt></p><ol><li>ALB Ingress Controller 创建后，将监听 Kubernetes API 上关于 Ingress 的事件。当发现匹配的 Ingress 对象时，Ingress Controller 开始创建 AWS 资源</li><li>AWS 使用 Application Load Balancer（ALB）来满足 Ingress 对象的要求，并使用 Target Group 将请求路由到目标节点</li><li>ALB Ingress Controller 为 Kubernetes Ingress 对象中用到的每一个 Kubernetes Service 创建一个 AWS Target Group</li><li>Listener 是一个 ALB 进程，由 ALB Ingress Controller 根据 Ingress 的注解（annotations）创建，监听 ALB 上指定的协议和端口，并接收外部的请求</li><li>ALB Ingress Controller 还根据 Kubernetes Ingress 中的路径定义，创建了 Target Group Rule，确保指定路径上的请求被路由到合适的 Kubernetes Service</li></ol><h4 id="数据包的传递：Ingress-to-Service"><a href="#数据包的传递：Ingress-to-Service" class="headerlink" title="数据包的传递：Ingress-to-Service"></a>数据包的传递：Ingress-to-Service</h4><p>Ingress-to-Service 的数据包传递与 LoadBalancer-to-Service 的数据包传递非常相似。核心差别是：</p><ul><li>Ingress 能够解析 URL 路径（可基于路径进行路由）</li><li>Ingress 连接到 Service 的 NodePort</li></ul><p>下图展示了 Ingress-to-Service 的数据包传递过程。</p><p><img src="/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/12.gif" alt></p><ol><li>创建 Ingress 之后，cloud controller 将会为其创建一个新的 Ingress Load Balancer</li><li>由于 Load Balancer 并不知道 Pod 的 IP 地址，当路由到达 Ingress Load Balancer 之后，会被转发到集群中的节点上（Service的节点端口）</li><li>节点上的 iptables 规则将数据包转发到合适的 Pod</li><li>Pod 接收到数据包</li></ol><p>从 Pod 返回的响应数据包将包含 Pod 的 IP 地址，但是请求客户端需要的是 Ingress Load Balancer 的 IP 地址。iptables 和 <code>conntrack</code> 被用来重写返回路径上的 IP 地址。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Container-to-Container 的网络&lt;/p&gt;
&lt;p&gt;Pod-to-Pod 的网络&lt;/p&gt;
&lt;p&gt;Pod-to-Service 的网络&lt;/p&gt;
&lt;p&gt;Internet-to-Service 的网络&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-网络策略</title>
    <link href="http://yoursite.com/2020/04/10/K8s-%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/"/>
    <id>http://yoursite.com/2020/04/10/K8s-%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/</id>
    <published>2020-04-10T08:59:31.000Z</published>
    <updated>2020-04-14T06:50:29.028Z</updated>
    
    <content type="html"><![CDATA[<p> Network Policies 网络策略</p><a id="more"></a> <h1 id="Network-Policies"><a href="#Network-Policies" class="headerlink" title="Network Policies"></a>Network Policies</h1><p>Kubernetes 中，Network Policy（网络策略）定义了一组 Pod 是否允许相互通信，或者与网络中的其他端点 endpoint 通信。</p><p><code>NetworkPolicy</code> 对象使用标签选择Pod，并定义规则指定选中的Pod可以执行什么样的网络通信</p><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>Network Policy 由网络插件实现，因此，使用的网络插件必须能够支持 <code>NetworkPolicy</code> 才可以使用此特性。如果仅仅是创建了一个 Network Policy 对象，但使用的网络插件并不支持此特性，所创建的 Network Policy 对象是不生效的。</p><h2 id="solated-Non-isolated-Pods"><a href="#solated-Non-isolated-Pods" class="headerlink" title="solated/Non-isolated Pods"></a>solated/Non-isolated Pods</h2><p>默认情况下，Pod 都是非隔离的（non-isolated），可以接受来自任何请求方的网络请求。</p><p>如果一个 NetworkPolicy 的标签选择器选中了某个 Pod，则该 Pod 将变成隔离的（isolated），并将拒绝任何不被 NetworkPolicy 许可的网络连接。（名称空间中其他未被 NetworkPolicy 选中的 Pod 将认可接受来自任何请求方的网络请求。）</p><p>Network Police 不会相互冲突，而是相互叠加的。如果多个 NetworkPolicy 选中了同一个 Pod，则该 Pod 可以接受这些 NetworkPolicy 当中任何一个 NetworkPolicy 定义的（入口/出口）规则，是所有NetworkPolicy规则的并集，因此，NetworkPolicy 的顺序并不重要，因为不会影响到最终的结果。</p><h2 id="NetworkPolicy对象"><a href="#NetworkPolicy对象" class="headerlink" title="NetworkPolicy对象"></a>NetworkPolicy对象</h2><p> 一个 NetworkPolicy 的 Example 如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-network-policy</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">role:</span> <span class="string">db</span></span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">172.17</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">        <span class="attr">except:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">172.17</span><span class="number">.1</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">project:</span> <span class="string">myproject</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">podSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">role:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">10.0</span><span class="number">.0</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">5978</span></span><br></pre></td></tr></table></figure><ul><li><p>基本信息： 同其他的 Kubernetes 对象一样，<code>NetworkPolicy</code> 需要 <code>apiVersion</code>、<code>kind</code>、<code>metadata</code> 字段</p></li><li><p>spec： <code>NetworkPolicy</code> 的 <code>spec</code> 字段包含了定义网络策略的主要信息：</p></li><li><ul><li>podSelector： 同名称空间中，符合此标签选择器 <code>.spec.podSelector</code> 的 Pod 都将应用这个 NetworkPolicy。上面的 Example中的 podSelector 选择了 role=db 的 Pod。如果该字段为空，则将对名称空间中所有的 Pod 应用这个 NetworkPolicy</li><li>policyTypes： .spec.policyTypes 是一个数组类型的字段，该数组中可以包含 Ingress、Egress 中的一个，也可能两个都包含。该字段标识了此 NetworkPolicy 是否应用到 入方向的网络流量、出方向的网络流量、或者两者都有。如果不指定 policyTypes 字段，该字段默认将始终包含 Ingress，当 NetworkPolicy 中包含出方向的规则时，Egress 也将被添加到默认值。</li><li>ingress： ingress 是一个数组，代表入方向的白名单规则。每一条规则都将允许与 from 和 ports 匹配的入方向的网络流量发生。例子中的 ingress 包含了一条规则，允许的入方向网络流量必须符合如下条件：</li></ul></li><li><ul><li><ul><li>Pod 的监听端口为 6379</li><li>请求方可以是如下三种来源当中的任意一种：</li></ul></li></ul></li><li><ul><li><ul><li><ul><li>ipBlock 为 172.17.0.0/16 网段，但是不包括 172.17.1.0/24 网段</li><li>namespaceSelector 标签选择器，匹配标签为 project=myproject</li><li>podSelector 标签选择器，匹配标签为 role=frontend</li></ul></li></ul></li></ul></li><li><ul><li>egress： egress 是一个数组，代表出方向的白名单规则。每一条规则都将允许与 to 和 ports 匹配的出方向的网络流量发生。例子中的 egress 允许的出方向网络流量必须符合如下条件：</li></ul></li><li><ul><li><ul><li>目标端口为 5978</li><li>目标 ipBlock  10.0.0.0/24网段</li></ul></li></ul></li></ul><p>因此，例子中的 NetworkPolicy 对网络流量做了如下限制：</p><ol><li>隔离了 default 名称空间中带有 role=db 标签的所有 Pod 的入方向网络流量和出方向网络流量</li><li>Ingress规则（入方向白名单规则）：</li></ol><ul><li><ul><li>当请求方是如下三种来源当中的任意一种时，允许访问 default 名称空间中所有带 role=db 标签的 Pod 的 6379 端口：</li></ul></li><li><ul><li><ul><li>ipBlock 为 172.17.0.0/16 网段，但是不包括 172.17.1.0/24 网段</li><li>namespaceSelector 标签选择器，匹配标签为 project=myproject</li><li>podSelector 标签选择器，匹配标签为 role=frontend</li></ul></li></ul></li></ul><ol><li>Egress rules（出方向白名单规则）：</li></ol><ul><li><ul><li>当如下条件满足时，允许出方向的网络流量：</li></ul></li><li><ul><li><ul><li>目标端口为 5978</li><li>目标 ipBlock 为 10.0.0.0/24 网段</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Network Policies 网络策略&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>通过Ingress访问应用程序</title>
    <link href="http://yoursite.com/2020/04/10/%E9%80%9A%E8%BF%87Ingress%E8%AE%BF%E9%97%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"/>
    <id>http://yoursite.com/2020/04/10/%E9%80%9A%E8%BF%87Ingress%E8%AE%BF%E9%97%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/</id>
    <published>2020-04-10T07:05:26.000Z</published>
    <updated>2020-04-10T08:32:27.837Z</updated>
    
    <content type="html"><![CDATA[<p> 用 Ingress 访问应用程序</p><a id="more"></a> <h1 id="通过-Ingress-访问应用程序"><a href="#通过-Ingress-访问应用程序" class="headerlink" title="通过 Ingress 访问应用程序"></a>通过 Ingress 访问应用程序</h1><h2 id="什么是-Ingress？"><a href="#什么是-Ingress？" class="headerlink" title="什么是 Ingress？"></a>什么是 Ingress？</h2><p>通常情况下，Service 和 Pod 的 IP 仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到 Service 在 Node 上暴露的 NodePort 上，然后再由 kube-proxy 通过边缘路由器 (edge router) 将其转发给相关的 Pod 或者丢弃。而 Ingress 就是为进入集群的请求提供路由规则的集合。</p><p>Ingress 可以给 Service 提供集群外部访问的 URL、负载均衡、SSL 终止、HTTP 路由等。为了配置这些 Ingress 规则，集群管理员需要部署一个 Ingress Controller，它监听 Ingress 和 Service 的变化，并根据规则配置负载均衡并提供访问入口。</p><h3 id="Pod-与-Ingress-的关系"><a href="#Pod-与-Ingress-的关系" class="headerlink" title="Pod 与 Ingress 的关系"></a>Pod 与 Ingress 的关系</h3><ul><li>通过label-selector相关联</li><li>通过Ingress Controller实现Pod的负载均衡</li></ul><p>-支持TCP/UDP 4层和HTTP 7层</p><p><img src="/2020/04/10/%E9%80%9A%E8%BF%87Ingress%E8%AE%BF%E9%97%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/1156961-20181225141932348-858339416.png" alt> </p><h3 id="Ingress-组成："><a href="#Ingress-组成：" class="headerlink" title="Ingress 组成："></a>Ingress 组成：</h3><ul><li>ingress controller：将新加入的Ingress转化成Nginx的配置文件并使之生效；</li><li>ingress服务：将Nginx的配置抽象成一个Ingress对象，每添加一个新的服务只需写一个新的Ingress的yaml文件即可；</li></ul><h3 id="Ingress-工作原理："><a href="#Ingress-工作原理：" class="headerlink" title="Ingress 工作原理："></a>Ingress 工作原理：</h3><p>ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，再写到nginx-ingress-control的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。</p><h2 id="实战：通过-Ingress-使的应用程序在互联网可用"><a href="#实战：通过-Ingress-使的应用程序在互联网可用" class="headerlink" title="实战：通过 Ingress 使的应用程序在互联网可用"></a>实战：通过 Ingress 使的应用程序在互联网可用</h2><p> <strong>创建文件 nginx-deployment.yaml</strong> </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br></pre></td></tr></table></figure><p> <strong>创建文件 nginx-service.yaml</strong> </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-port</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">32600</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br></pre></td></tr></table></figure><p> <strong>创建文件 nginx-ingress.yaml</strong> </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-ingress-for-nginx</span>  <span class="comment"># Ingress 的名字，仅用于标识</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span>                      <span class="comment"># Ingress 中定义 L7 路由规则</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">www.test.com</span>   <span class="comment"># 根据 virtual hostname 进行路由（请使用您自己的域名）</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span>                  <span class="comment"># 按路径进行路由</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">nginx-service</span>  <span class="comment"># 指定后端的 Service 为之前创建的 nginx-service</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p> <strong>执行命令</strong> </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx-deployment.yaml</span><br><span class="line">kubectl apply -f nginx-service.yaml</span><br><span class="line">kubectl apply -f nginx-ingress.yaml</span><br></pre></td></tr></table></figure><p> <strong>检查执行结果</strong> </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ingress]<span class="comment"># kubectl get ingress -o wide</span></span><br><span class="line">NAME                   HOSTS               ADDRESS   PORTS   AGE</span><br><span class="line">my-ingress-for-nginx   www.test.com             80      39m</span><br><span class="line"></span><br><span class="line"><span class="comment">#从互联网访问</span></span><br><span class="line">[root@k8s-master ingress]<span class="comment"># curl www.test.com</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 用 Ingress 访问应用程序&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-Service连接应用程序</title>
    <link href="http://yoursite.com/2020/04/08/K8s-Service%E8%BF%9E%E6%8E%A5%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"/>
    <id>http://yoursite.com/2020/04/08/K8s-Service%E8%BF%9E%E6%8E%A5%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/</id>
    <published>2020-04-08T08:00:56.000Z</published>
    <updated>2020-04-10T03:36:38.870Z</updated>
    
    <content type="html"><![CDATA[<p> 创建/访问/Service，保护 Service 安全，暴露 Service；</p><a id="more"></a> <h1 id="Service连接应用程序"><a href="#Service连接应用程序" class="headerlink" title="Service连接应用程序"></a>Service连接应用程序</h1><h2 id="Kubernetes-的网络模型"><a href="#Kubernetes-的网络模型" class="headerlink" title="Kubernetes 的网络模型"></a>Kubernetes 的网络模型</h2><p>通常，Docker 使用一种 <code>host-private</code> 的联网方式，在此情况下，只有两个容器都在同一个节点（主机）上时，一个容器才可以通过网络连接另一个容器。为了使 Docker 容器可以跨节点通信，必须在宿主节点（主机）的 IP 地址上分配端口，并将该端口接收到的网络请求转发（或代理）到容器中。这意味着，用户必须非常小心地为容器分配宿主节点（主机）的端口号，或者端口号可以自动分配。</p><p>在一个集群中，多个开发者之间协调分配端口号是非常困难的。Kubernetes 认为集群中的两个 Pod 应该能够互相通信，无论他们各自在哪个节点上。每一个 Pod 都被分配自己的 <strong>“cluster-private-IP”</strong>，因此，无需在 Pod 间建立连接，或者将容器的端口映射到宿主机的端口。因此：</p><ul><li>Pod 中的任意容器可以使用 localhost 直连同 Pod 中另一个容器的端口</li><li>集群中的任意 Pod 可以使用另一的 Pod 的 <strong>cluster-private-IP</strong> 直连对方的端口，（无需 NAT 映射）</li></ul><h2 id="在集群中部署-Pod"><a href="#在集群中部署-Pod" class="headerlink" title="在集群中部署 Pod"></a>在集群中部署 Pod</h2><p> 创建文件 <code>run-my-nginx.yaml</code>，文件内容如下 </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p> 执行以下命令，部署 Pod 并检查运行情况： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl apply -f  run-my-nginx.yaml </span><br><span class="line">deployment.apps&#x2F;my-nginx created</span><br><span class="line"></span><br><span class="line">[root@k8s-master 0408]# kubectl get pods -l run&#x3D;my-nginx -o wide</span><br><span class="line">NAME                        READY   STATUS         RESTARTS   AGE   IP               NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">my-nginx-75897978cd-6vfrl   0&#x2F;1     ErrImagePull   0          39s   10.244.235.206   k8s-master    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">my-nginx-75897978cd-p2tcd   1&#x2F;1     Running        0          39s   10.244.44.235    k8s-node-02   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>执行命令 <code>kubectl get pods -l run=my-nginx -o yaml | grep podIP</code>， 检查 Pod 的 IP 地址，输出结果如下： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl get pods -l run&#x3D;my-nginx -o yaml | grep podIP</span><br><span class="line">      cni.projectcalico.org&#x2F;podIP: 10.244.235.206&#x2F;32</span><br><span class="line">    podIP: 10.244.235.206</span><br><span class="line">    podIPs:</span><br><span class="line">      cni.projectcalico.org&#x2F;podIP: 10.244.44.235&#x2F;32</span><br><span class="line">    podIP: 10.244.44.235</span><br><span class="line">    podIPs:</span><br></pre></td></tr></table></figure><p>在集群中的任意节点上，您可以执行 <code>curl 10.244.235.206 或</code>curl 10.244.44.235` 获得 nginx 的响应。此时：</p><ul><li>容器并没有使用节点上的 80 端口</li><li>没有使用 NAT 规则对容器端口进行映射</li></ul><p>这意味着，您可以</p><ul><li>在同一节点上使用 80 端口运行多个 nginx Pod</li><li>在集群的任意节点/Pod 上使用 nginx Pod 的 clusterIP 访问 nginx 的 80 端口</li></ul><p>同 Docker 一样，Kubernets 中，仍然可以将 Pod 的端口映射到宿主节点的网络地址上（使用 nodePort），但是使用 Kubernetes 的网络模型时，这类需求已经大大减少了。</p><h1 id="创建-Service"><a href="#创建-Service" class="headerlink" title="创建 Service"></a>创建 Service</h1><p>Pod 因为故障或其他原因终止后，Deployment Controller 将创建一个新的 Pod 以替代该 Pod，但是 IP 地址将发生变化。Kubernetes Service 解决了这样的问题。</p><p>Kubernetes Service：</p><ul><li>定义了集群中一组 Pod 的逻辑集合，该集合中的 Pod 提供了相同的功能</li><li>被创建后，获得一个唯一的 IP 地址（ClusterIP）。直到该 Service 被删除，此地址不会发生改变</li><li>Pod 可以直接连接 Service IP 地址上的端口，且发送到该 IP 地址的网络请求被自动负载均衡分发到 Service 所选取的 Pod 集合中</li></ul><p>执行命令 <code>kubectl expose deployment/my-nginx</code> 可以为上面的两个 nginx Pod 创建 Service，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl expose deployment&#x2F;my-nginx</span><br><span class="line">service&#x2F;my-nginx exposed</span><br></pre></td></tr></table></figure><p> 该命令等价于 <code>kubectl apply -f nginx-svc.yaml</code>，其中 nginx-svc.yaml 文件的内容如下所示： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br></pre></td></tr></table></figure><p>该 yaml 文件将创建一个 Service：</p><ul><li>该 Service 通过 label selector 选取包含 <code>run: my-nginx</code> 标签的 Pod 作为后端 Pod</li><li>该 Service 暴露一个端口 80（<code>spec.ports[*].port</code>）</li><li>该 Service 将 80 端口上接收到的网络请求转发到后端 Pod 的 80 （<code>spec.ports[*].targetPort</code>）端口上，支持负载均衡</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl get svc my-nginx</span><br><span class="line">NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">my-nginx   ClusterIP   10.97.72.97   &lt;none&gt;        80&#x2F;TCP    28m</span><br></pre></td></tr></table></figure><p>Service 的后端 Pod 实际上通过 <code>Endpoints</code> 来暴露。Kubernetes 会持续检查 Service 的 label selector <code>spec.selector</code>，并将符合条件的 Pod 更新到与 Service 同名（my-nginx）的 Endpoints 对象。如果 Pod 终止了，该 Pod 将被自动从 Endpoints 中移除，新建的 Pod 将自动被添加到该 Endpoint。 </p><p>执行命令 <code>kubectl describe svc my-nginx</code>，输出结果如下，请注意 Endpoints 中的 IP 地址与上面获得的 Pod 地址相同： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl describe svc my-nginx</span><br><span class="line">Name:              my-nginx</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          run&#x3D;my-nginx</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.97.72.97</span><br><span class="line">Port:              &lt;unset&gt;  80&#x2F;TCP</span><br><span class="line">TargetPort:        80&#x2F;TCP</span><br><span class="line">Endpoints:         10.244.154.209:80,10.244.44.236:80</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none</span><br></pre></td></tr></table></figure><p> 执行命令 <code>kubectl get ep my-nginx</code>，输出结果如下： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl get ep my-nginx</span><br><span class="line">NAME       ENDPOINTS                            AGE</span><br><span class="line">my-nginx   10.244.154.209:80,10.244.44.236:80   38m</span><br></pre></td></tr></table></figure><p> 此时，可以在集群的任意节点上执行 <code>curl 10.0.162.149:80</code>，通过 Service 的 ClusterIP:Port 访问 nginx。 </p><p> Service 的 IP 地址是虚拟地址。 </p><h1 id="访问-Service"><a href="#访问-Service" class="headerlink" title="访问 Service"></a>访问 Service</h1><p>Kubernetes 支持两种方式发现服务：</p><ul><li>环境变量</li><li>DNS</li></ul><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>针对每一个有效的 Service，kubelet 在创建 Pod 时，向 Pod 添加一组环境变量。这种做法引发了一个 Pod 和 Service 的顺序问题。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]<span class="comment"># kubectl exec my-nginx-75897978cd-4t7zb  -- printenv | grep SERVICE</span></span><br><span class="line">MY_SERVICE_PORT_80_TCP_PORT=80</span><br><span class="line">MY_SERVICE_PORT=tcp://10.100.240.247:80</span><br><span class="line">MY_SERVICE_SERVICE_HOST=10.100.240.247</span><br><span class="line">MY_SERVICE_PORT_80_TCP=tcp://10.100.240.247:80</span><br><span class="line">KUBERNETES_SERVICE_PORT_HTTPS=443</span><br><span class="line">KUBERNETES_SERVICE_PORT=443</span><br><span class="line">NGINX_DEP_SERVICE_PORT_80_80=80</span><br><span class="line">MY_SERVICE_PORT_80_TCP_ADDR=10.100.240.247</span><br><span class="line">MYAPP_SERVICE_PORT_80_80=80</span><br><span class="line">MY_SERVICE_SERVICE_PORT=80</span><br><span class="line">MYAPP_SERVICE_HOST=10.102.48.254</span><br><span class="line">MYAPP_SERVICE_PORT=80</span><br><span class="line">NGINX_DEP_SERVICE_HOST=10.98.115.52</span><br><span class="line">NGINX_DEP_SERVICE_PORT=80</span><br><span class="line">MY_SERVICE_PORT_80_TCP_PROTO=tcp</span><br><span class="line">KUBERNETES_SERVICE_HOST=10.96.0.1</span><br></pre></td></tr></table></figure><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><p>Kubernetes 提供了一个 DNS cluster addon，可自动为 Service 分配 DNS name。该 addon 已经默认安装。</p><p>执行命令 <code>kubectl get services kube-dns --namespace=kube-system</code> 查看该 addon 在集群上是否可用，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl get services kube-dns --namespace&#x3D;kube-system</span><br><span class="line">NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53&#x2F;UDP,53&#x2F;TCP,9153&#x2F;TCP   13d</span><br></pre></td></tr></table></figure><p>此时，可以从集群中任何 Pod 中按 Service 的名称访问该 Service。</p><ul><li>执行命令 <code>kubectl run curl --image=radial/busyboxplus:curl -i --tty</code> 获得 busyboxplus 容器的命令行终端，该命令输出结果如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]<span class="comment"># kubectl run curl --image=radial/busyboxplus:curl -i --tty</span></span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed <span class="keyword">in</span> a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">If you don<span class="string">'t see a command prompt, try pressing enter.</span></span><br></pre></td></tr></table></figure><ul><li>执行命令 <code>nslookup my-nginx</code>，输出结果如下所示： </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[ root@curl-69c656fd45-848f6:/ ]$ nslookup my-nginx</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-nginx</span><br><span class="line">Address 1: 10.97.72.97 my-nginx.default.svc.cluster.local</span><br></pre></td></tr></table></figure><ul><li><p>执行命令 <code>curl my-nginx:80</code>，可获得 Nginx 的响应。 </p></li><li><p>执行命令 <code>kubectl delete deployment curl</code> 可删除刚才创建的 <code>curl</code> 测试容器 </p></li></ul><h1 id="保护-Service-的安全"><a href="#保护-Service-的安全" class="headerlink" title="保护 Service 的安全"></a>保护 Service 的安全</h1><p>在将该 Service 公布到互联网时，可能需要确保该通信渠道是安全的。为此：</p><ul><li>准备 https 证书（购买，或者自签名）</li><li>将该 nginx 服务配置好，并使用该 https 证书</li><li>配置 Secret，以使得其他 Pod 可以使用该证书</li></ul><p>配置 nginx 使用自签名证书：</p><ul><li>创建密钥对 </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]<span class="comment"># openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/nginx.key -out /tmp/nginx.crt -subj "/CN=my-nginx/O=my-nginx"</span></span><br><span class="line">Generating a 2048 bit RSA private key</span><br><span class="line">.............................................+++</span><br><span class="line">.............................................+++</span><br><span class="line">writing new private key to <span class="string">'/tmp/nginx.key'</span></span><br></pre></td></tr></table></figure><ul><li>将密钥对转换为 base64 编码</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /tmp/nginx.crt | base64</span><br><span class="line">cat /tmp/nginx.key | base64</span><br></pre></td></tr></table></figure><ul><li>创建一个如下格式的 nginxsecrets.yaml 文件，使用前面命令输出的 base64 编码替换其中的内容（base64编码内容不能换行） </li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"v1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"Secret"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">"nginxsecret"</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">"default"</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">nginx.crt:</span> <span class="string">""</span></span><br><span class="line">  <span class="attr">nginx.key:</span> <span class="string">""</span></span><br></pre></td></tr></table></figure><ul><li>使用该文件创建 Secrets </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建 Secrets</span></span><br><span class="line">[root@k8s-master 0408]<span class="comment"># kubectl apply -f 1.yaml </span></span><br><span class="line">secret/nginxsecret created</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看 Secrets</span></span><br><span class="line">[root@k8s-master 0408]<span class="comment"># kubectl get secrets</span></span><br><span class="line">NAME                  TYPE                                  DATA   AGE</span><br><span class="line">default-token-xws5p   kubernetes.io/service-account-token   3      13d</span><br><span class="line">nginxsecret           Opaque                                2      38s</span><br></pre></td></tr></table></figure><ul><li>修改 nginx 部署，使 nginx 使用 Secrets 中的 https 证书，修改 Service，使其暴露 80 端口和 443端口。nginx-secure-app.yaml 文件如下所示： </li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">        <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">nginxsecret</span>       <span class="comment">#####</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginxhttps</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">bprashanth/nginxhttps:1.0</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">443</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/ssl</span>      <span class="comment">#####</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">secret-volume</span></span><br></pre></td></tr></table></figure><p>关于 nginx-secure-app.yaml</p><ul><li>该文件同时包含了 Deployment 和 Service 的定义</li><li>nginx server 监听 HTTP 80 端口和 HTTPS 443 端口的请求， nginx Service 同时暴露了这两个端口</li><li>nginx 容器可以通过 <code>/etc/nginx/ssl</code> 访问到 https 证书，https 证书存放在 Secrets 中，且必须在 Pod 创建之前配置好。</li></ul><p>执行命令使该文件生效：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete deployments,svc my-nginx</span><br><span class="line">kubectl create -f ./nginx-secure-app.yaml</span><br></pre></td></tr></table></figure><p>此时，可以从任何节点访问该 nginx server</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o yaml | grep -i podip</span><br><span class="line">    podIP: 10.244.3.5</span><br><span class="line">node $ curl -k https://10.244.3.5</span><br><span class="line">...</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br></pre></td></tr></table></figure><p> 创建 curlpod.yaml 文件，内容如下： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">curl-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">curlpod</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">curlpod</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">        <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">nginxsecret</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">curlpod</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">while</span> <span class="literal">true</span><span class="string">;</span> <span class="string">do</span> <span class="string">sleep</span> <span class="number">1</span><span class="string">;</span> <span class="string">done</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">radial/busyboxplus:curl</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/ssl</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">secret-volume</span></span><br></pre></td></tr></table></figure><ul><li>执行命令，完成 curlpod 的部署 </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0408]# kubectl apply -f curlpod.yaml </span><br><span class="line">deployment.apps&#x2F;curl-deployment created</span><br><span class="line"></span><br><span class="line">[root@k8s-master 0408]# kubectl get pods -l app&#x3D;curlpod</span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">curl-deployment-f8c5c685b-cwqrp   1&#x2F;1     Running   0          17s</span><br><span class="line"></span><br><span class="line">#执行 curl，访问 nginx 的 https 端口</span><br><span class="line">[root@k8s-master 0408]# kubectl exec curl-deployment-f8c5c685b-cwqrp -- curl https:&#x2F;&#x2F;my-nginx --cacert &#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;nginx.crt</span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--</span><br></pre></td></tr></table></figure><h1 id="暴露-Service"><a href="#暴露-Service" class="headerlink" title="暴露 Service"></a>暴露 Service</h1><p>在应用程序中，可能有一部分功能需要通过 Service 发布到一个外部的 IP 地址上。Kubernetes 支持如下两种方式：</p><ul><li>NodePort</li><li>LoadBalancer<ul><li>需要云环境支持</li></ul></li></ul><p><strong>执行命令查看service暴露的外部端口</strong></p><ul><li><pre><code>[root@k8s-master 0408]# kubectl get svc my-nginx -o yaml | grep nodePort -C 5spec:  clusterIP: 10.107.66.92  externalTrafficPolicy: Cluster  ports:  - name: http    nodePort: 30407      ###        port: 80    protocol: TCP    targetPort: 80  - name: https    nodePort: 30534      ###    port: 443    protocol: TCP    targetPort: 443      selector:    run: my-nginx<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">假设某一节点的公网 IP 地址为 23.251.152.56，可以在任意一台可上网的机器执行命令 &#96;curl https:&#x2F;&#x2F;23.251.152.56:30407 -k&#96;。输出结果为：</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;text</span><br><span class="line">...</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br></pre></td></tr></table></figure></code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 创建/访问/Service，保护 Service 安全，暴露 Service；&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s-Service</title>
    <link href="http://yoursite.com/2020/04/08/K8s-Service/"/>
    <id>http://yoursite.com/2020/04/08/K8s-Service/</id>
    <published>2020-04-08T05:30:32.000Z</published>
    <updated>2020-04-08T07:59:36.978Z</updated>
    
    <content type="html"><![CDATA[<p> Service的功能及作用</p><a id="more"></a> <h1 id="一、Service概述"><a href="#一、Service概述" class="headerlink" title="一、Service概述"></a>一、Service概述</h1><h2 id="为何需要-Service"><a href="#为何需要-Service" class="headerlink" title="为何需要 Service"></a>为何需要 Service</h2><p>Kubernetes 中 Pod 是随时可以消亡的（节点故障、容器内应用程序错误等原因）。如果使用 Deployment运行应用程序，Deployment 将会在 Pod 消亡后再创建一个新的 Pod 以维持所需要的副本数。每一个 Pod 有自己的 IP 地址，然而，对于 Deployment 而言，对应 Pod 集合是动态变化的。</p><p>这个现象导致了如下问题：</p><ul><li>如果某些 Pod（假设是 ‘backends’）为另外一些 Pod（假设是 ‘frontends’）提供接口，在 ‘backends’ 中的 Pod 集合不断变化（IP 地址也跟着变化）的情况下，’frontends’ 中的 Pod 如何才能知道应该将请求发送到哪个 IP 地址？</li></ul><p>Service 存在的意义，就是为了解决这个问题</p><h2 id="Kubernetes-Service"><a href="#Kubernetes-Service" class="headerlink" title="Kubernetes Service"></a>Kubernetes Service</h2><p>Kubernetes 中 Service 是一个 API 对象，通过 kubectl + YAML，定义一个 Service，可以将符合 Service 指定条件的 Pod 作为可通过网络访问的服务提供给服务调用者。</p><p>Service 是 Kubernetes 中的一种服务发现机制：</p><ul><li>Pod 有自己的 IP 地址</li><li>Service 被赋予一个唯一的 dns name</li><li>Service 通过 label selector 选定一组 Pod</li><li>Service 实现负载均衡，可将请求均衡分发到选定这一组 Pod 中</li></ul><p>例如，假设有一个无状态的图像处理后端程序运行了 3 个 Pod 副本。这些副本是相互可替代的（前端程序调用其中任何一个都可以）。在后端程序的副本集中的 Pod 经常变化（销毁、重建、扩容、缩容等）的情况下，前端程序不应该关注这些变化。</p><p>Kubernetes 通过引入 Service 的概念，将前端与后端解耦</p><h1 id="二、Service详细描述"><a href="#二、Service详细描述" class="headerlink" title="二、Service详细描述"></a>二、Service详细描述</h1><h2 id="创建-Service"><a href="#创建-Service" class="headerlink" title="创建 Service"></a>创建 Service</h2><p>Kubernetes Servies 是一个 RESTFul 接口对象，可通过 yaml 文件创建。</p><p>例如，假设您有一组 Pod：</p><ul><li>每个 Pod 都监听 9376 TCP 端口</li><li>每个 Pod 都有标签 app=MyApp</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">MyApp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>上述 YAML 文件可用来创建一个 Service：</p><ul><li>名字为 <code>my-service</code></li><li>目标端口为 TCP 9376</li><li>选取所有包含标签 app=MyApp 的 Pod</li></ul><h3 id="关于-Service："><a href="#关于-Service：" class="headerlink" title="关于 Service："></a>关于 Service：</h3><ul><li><p>Kubernetes 将为该 Service 分配一个 IP 地址（ClusterIP 或 集群内 IP），供 Service Proxy 使用。</p></li><li><p>Kubernetes 将不断扫描符合该 selector 的 Pod，并将最新的结果更新到与 Service 同名 <code>my-service</code> 的 Endpoint 对象中。</p></li><li><p>Service 从自己的 IP 地址和 <code>port</code> 端口接收请求，并将请求映射到符合条件的 Pod 的 <code>targetPort</code>。为了方便，默认 <code>targetPort</code> 的取值 与 <code>port</code> 字段相同 </p></li><li><p>Pod 的定义中，Port 可能被赋予了一个名字，可以在 Service 的 <code>targetPort</code> 字段引用这些名字，而不是直接写端口号。这种做法可以使得将来修改后端程序监听的端口号，而无需影响到前端程序。</p></li><li><p>Service 的默认传输协议是 TCP，也可以使用其他支持的传输协议。</p></li><li><p>Kubernetes Service 中，可以定义多个端口，不同的端口可以使用相同或不同的传输协议。</p></li></ul><h2 id="创建-Service（无-label-selector）"><a href="#创建-Service（无-label-selector）" class="headerlink" title="创建 Service（无 label selector）"></a>创建 Service（无 label selector）</h2><p>Service 通常用于提供对 Kubernetes Pod 的访问，但是也可以将其用于任何其他形式的后端。例如：</p><ul><li>在生产环境中使用一个 Kubernetes 外部的数据库集群，在测试环境中使用 Kubernetes 内部的 数据库</li><li>将 Service 指向另一个名称空间中的 Service，或者另一个 Kubernetes 集群中的 Service</li><li>将程序迁移到 Kubernetes，但是根据迁移路径，只将一部分后端程序运行在 Kubernetes 中</li></ul><p>在上述这些情况下，可以定义一个没有 Pod Selector 的 Service。例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>因为该 Service 没有 selector，相应的 Endpoint 对象就无法自动创建。可以手动创建一个 Endpoint 对象，以便将该 Service 映射到后端服务真实的 IP 地址和端口： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">192.0</span><span class="number">.2</span><span class="number">.42</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>注意：</p><ul><li>对于 Service 的访问者来说，Service 是否有 label selector 都是一样的。在上述例子中，Service 将请求路由到 Endpoint 192.0.2.42:9376 (TCP)。 </li><li>Endpoint 中的 IP 地址不可以是集群中其他 Service 的 ClusterIP。</li></ul><h2 id="Kubernetes-支持三种-proxy-mode（代理模式）"><a href="#Kubernetes-支持三种-proxy-mode（代理模式）" class="headerlink" title="Kubernetes 支持三种 proxy mode（代理模式）"></a>Kubernetes 支持三种 proxy mode（代理模式）</h2><h3 id="1-User-space-代理模式"><a href="#1-User-space-代理模式" class="headerlink" title="1.User space 代理模式"></a>1.User space 代理模式</h3><p><strong>在 user space proxy mode 下：</strong></p><ul><li>kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件</li><li>kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 打开一个随机端口</li><li>kube-proxy 安装 iptables 规则，将发送到该 Service 的 ClusterIP（虚拟 IP）/ Port 的请求重定向到该随机端口</li><li>任何发送到该随机端口的请求将被代理转发到该 Service 的后端 Pod 上（kube-proxy 从 Endpoint 信息中获得可用 Pod）</li><li>kube-proxy 在决定将请求转发到后端哪一个 Pod 时，默认使用 round-robin（轮询）算法，并会考虑到 Service 中的 <code>SessionAffinity</code> 的设定</li></ul><p>如下图所示：</p><p> <img src="https://kuboard.cn/assets/img/services-userspace-overview.7dfebdc9.svg" alt="Kubernetes教程：Service user space"> </p><h3 id="2-Iptables-代理模式-默认模式"><a href="#2-Iptables-代理模式-默认模式" class="headerlink" title="2.Iptables 代理模式 默认模式"></a>2.Iptables 代理模式 <strong>默认模式</strong></h3><p><strong>在 iptables proxy mode 下：</strong></p><ul><li><p>kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件</p></li><li><p>kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 安装 iptable 规则</p></li><li><p>iptables 将发送到 Service 的 ClusterIP / Port 的请求重定向到 Service 的后端 Pod 上</p><ul><li>对于 Service 中的每一个 Endpoint，kube-proxy 安装一个 iptable 规则</li><li>默认情况下，kube-proxy 随机选择一个 Service 的后端 Pod</li></ul><p><img src="https://kuboard.cn/assets/img/services-iptables-overview.fc39e9e4.svg" alt="Kubernetes教程：Service iptables proxy"> </p></li></ul><p><strong>iptables proxy mode 的优点：</strong></p><ul><li>更低的系统开销：在 linux netfilter 处理请求，无需在 userspace 和 kernel space 之间切换</li><li>更稳定</li></ul><p><strong>与 user space mode 的差异：</strong></p><ul><li>使用 iptables mode 时，如果第一个 Pod 没有响应，则创建连接失败</li><li>使用 user space mode 时，如果第一个 Pod 没有响应，kube-proxy 会自动尝试连接另外一个后端 Pod</li></ul><p>可以配置 Pod 就绪检查（readiness probe）确保后端 Pod 正常工作，此时，在 iptables 模式下 kube-proxy 将只使用健康的后端 Pod，从而避免了 kube-proxy 将请求转发到已经存在问题的 Pod 上。</p><h3 id="3-IPVS-代理模式"><a href="#3-IPVS-代理模式" class="headerlink" title="3.IPVS 代理模式"></a>3.IPVS 代理模式</h3><p><strong>在 IPVS proxy mode 下：</strong></p><ul><li><p>kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件</p></li><li><p>kube-proxy 根据监听到的事件，调用 netlink 接口，创建 IPVS 规则；并且将 Service/Endpoint 的变化同步到 IPVS 规则中</p></li><li><p>当访问一个 Service 时，IPVS 将请求重定向到后端 Pod</p><p><img src="https://kuboard.cn/assets/img/services-ipvs-overview.88a8453f.svg" alt="Kubernetes教程：Service IPVS proxy"> </p></li></ul><p><strong>IPVS 模式的优点</strong></p><p>IPVS proxy mode 基于 netfilter 的 hook 功能，与 iptables 代理模式相似，但是 IPVS 代理模式使用 hash table 作为底层的数据结构，并在 kernel space 运作。这就意味着</p><ul><li>IPVS 代理模式可以比 iptables 代理模式有更低的网络延迟，在同步代理规则时，也有更高的效率</li><li>与 user space 代理模式 / iptables 代理模式相比，IPVS 模式可以支持更大的网络流量</li></ul><p><strong>注意：</strong></p><ul><li>如果要使用 IPVS 模式，必须在启动 kube-proxy 前为节点的 linux 启用 IPVS</li><li>kube-proxy 以 IPVS 模式启动时，如果发现节点的 linux 未启用 IPVS，则退回到 iptables 模式</li></ul><h3 id="代理模式总结"><a href="#代理模式总结" class="headerlink" title="代理模式总结"></a>代理模式总结</h3><p>在所有的代理模式中，发送到 Service 的 IP:Port 的请求将被转发到一个合适的后端 Pod，而无需调用者知道任何关于 Kubernetes/Service/Pods 的细节。</p><p>Service 中额外字段的作用：</p><ul><li><pre><code>service.spec.sessionAffinity<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  - 默认值为 &quot;None&quot;</span><br><span class="line">  - 如果设定为 &quot;ClientIP&quot;，则同一个客户端的连接将始终被转发到同一个 Pod</span><br><span class="line"></span><br><span class="line">-</span><br></pre></td></tr></table></figure>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  - 默认值为 10800 （3 小时）</span><br><span class="line">  - 设定会话保持的持续时间</span><br><span class="line"></span><br><span class="line">## 多端口的Service</span><br><span class="line"></span><br><span class="line">Kubernetes 中，可以在一个 Service 对象中定义多个端口，但必须为每个端口定义一个名字。如下所示：</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: my-service</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: MyApp</span><br><span class="line">  ports:</span><br><span class="line">    - name: http</span><br><span class="line">      protocol: TCP</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 9376</span><br><span class="line">    - name: https</span><br><span class="line">      protocol: TCP</span><br><span class="line">      port: 443</span><br><span class="line">      targetPort: 9377</span><br></pre></td></tr></table></figure></code></pre></li></ul><h2 id="使用自定义的-IP-地址"><a href="#使用自定义的-IP-地址" class="headerlink" title="使用自定义的 IP 地址"></a>使用自定义的 IP 地址</h2><p>创建 Service 时，如果指定 <code>.spec.clusterIP</code> 字段，可以使用自定义的 Cluster IP 地址。该 IP 地址必须是 APIServer 中配置字段 <code>service-cluster-ip-range</code> CIDR 范围内的合法 IPv4 或 IPv6 地址，否则不能创建成功。</p><h2 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h2><p>Kubernetes 支持两种主要的服务发现模式：</p><ul><li>环境变量</li><li>DNS</li></ul><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>kubelet 查找有效的 Service，并针对每一个 Service，向其所在节点上的 Pod 注入一组环境变量。支持的环境变量有：</p><ul><li>{SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT<ul><li>Service name 被转换为大写</li><li>小数点 <code>.</code> 被转换为下划线 <code>_</code> </li></ul></li></ul><p>如果要在 Pod 中使用基于环境变量的服务发现方式，必须先创建 Service，再创建调用 Service 的 Pod。否则，Pod 中不会有该 Service 对应的环境变量。 </p><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><p>CoreDNS 监听 Kubernetes API 上创建和删除 Service 的事件，并为每一个 Service 创建一条 DNS 记录。集群中所有的 Pod 都可以使用 DNS Name 解析到 Service 的 IP 地址。 </p><h2 id="虚拟-IP-的实现"><a href="#虚拟-IP-的实现" class="headerlink" title="虚拟 IP 的实现"></a>虚拟 IP 的实现</h2><h3 id="避免冲突"><a href="#避免冲突" class="headerlink" title="避免冲突"></a>避免冲突</h3><p>Kubernetes 的一个设计哲学是：尽量避免非人为错误产生的可能性。就设计 Service 而言，Kubernetes 应该将选择的端口号与其他人选择的端口号隔离开。为此，Kubernetes 为每一个 Service 分配一个该 Service 专属的 IP 地址。</p><p>为了确保每个 Service 都有一个唯一的 IP 地址，kubernetes 在创建 Service 之前，先更新 etcd 中的一个全局分配表，如果更新失败（例如 IP 地址已被其他 Service 占用），则 Service 不能成功创建。</p><p>Kubernetes 使用一个后台控制器检查该全局分配表中的 IP 地址的分配是否仍然有效，并且自动清理不再被 Service 使用的 IP 地址。</p><h3 id="Service-的-IP-地址"><a href="#Service-的-IP-地址" class="headerlink" title="Service 的 IP 地址"></a>Service 的 IP 地址</h3><p>Pod 的 IP 地址路由到一个确定的目标，然而 Service 的 IP 地址则不同，通常背后并不对应一个唯一的目标。 kube-proxy 使用 iptables （Linux 中的报文处理逻辑）来定义虚拟 IP 地址。当客户端连接到该虚拟 IP 地址时，它们的网络请求将自动发送到一个合适的 Endpoint。Service 对应的环境变量和 DNS 实际上反应的是 Service 的虚拟 IP 地址（和端口）。</p><h4 id="Userspace"><a href="#Userspace" class="headerlink" title="Userspace"></a>Userspace</h4><p>以上面提到的图像处理程序为例。当后端 Service 被创建时，Kubernetes master 为其分配一个虚拟 IP 地址（假设是 10.0.0.1），并假设 Service 的端口是 1234。集群中所有的 kube-proxy 都实时监听者 Service 的创建和删除。Service 创建后，kube-proxy 将打开一个新的随机端口，并设定 iptables 的转发规则（以便将该 Service 虚拟 IP 的网络请求全都转发到这个新的随机端口上），并且 kube-proxy 将开始接受该端口上的连接。</p><p>当一个客户端连接到该 Service 的虚拟 IP 地址时，iptables 的规则被触发，并且将网络报文重定向到 kube-proxy 自己的随机端口上。kube-proxy 接收到请求后，选择一个后端 Pod，再将请求以代理的形式转发到该后端 Pod。</p><p>这意味着 Service 可以选择任意端口号，而无需担心端口冲突。客户端可以直接连接到一个 IP:port，无需关心最终在使用哪个 Pod 提供服务。</p><h4 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h4><p>仍然以上面提到的图像处理程序为例。当后端 Service 被创建时，Kubernetes master 为其分配一个虚拟 IP 地址（假设是 10.0.0.1），并假设 Service 的端口是 1234。集群中所有的 kube-proxy 都实时监听者 Service 的创建和删除。Service 创建后，kube-proxy 设定了一系列的 iptables 规则（这些规则可将虚拟 IP 地址映射到 per-Service 的规则）。per-Service 规则进一步链接到 per-Endpoint 规则，并最终将网络请求重定向（使用 destination-NAT）到后端 Pod。</p><p>当一个客户端连接到该 Service 的虚拟 IP 地址时，iptables 的规则被触发。一个后端 Pod 将被选中（基于 session affinity 或者随机选择），且网络报文被重定向到该后端 Pod。与 userspace proxy 不同，网络报文不再被复制到 userspace，kube-proxy 也无需处理这些报文，且报文被直接转发到后端 Pod。</p><p>在使用 node-port 或 load-balancer 类型的 Service 时，以上的代理处理过程是相同的。</p><h4 id="IPVS"><a href="#IPVS" class="headerlink" title="IPVS"></a>IPVS</h4><p>在一个大型集群中（例如，存在 10000 个 Service）iptables 的操作将显著变慢。IPVS 的设计是基于 in-kernel hash table 执行负载均衡。因此，使用 IPVS 的 kube-proxy 在 Service 数量较多的情况下仍然能够保持好的性能。同时，基于 IPVS 的 kube-proxy 可以使用更复杂的负载均衡算法（最少连接数、基于地址的、基于权重的等）</p><h2 id="支持的传输协议"><a href="#支持的传输协议" class="headerlink" title="支持的传输协议"></a>支持的传输协议</h2><ul><li>TCP</li><li>UDP</li><li>HTTP</li><li>Proxy Protocol</li><li>SCTP</li></ul><h1 id="三、发布Service"><a href="#三、发布Service" class="headerlink" title="三、发布Service"></a>三、发布Service</h1><h2 id="Service-类型"><a href="#Service-类型" class="headerlink" title="Service 类型"></a>Service 类型</h2><p>Kubernetes 中可以通过不同方式发布 Service，通过 <code>ServiceType</code> 字段指定，该字段的默认值是 <code>ClusterIP</code>，可选值有：</p><ul><li><strong>ClusterIP</strong>:  默认值。通过集群内部的一个 IP 地址暴露 Service，只在集群内部可以访问</li><li><strong>NodePort</strong>:  通过每一个节点上的的静态端口（NodePort）暴露 Service，同时自动创建 ClusterIP 类型的访问方式<ul><li>在集群内部通过 $(ClusterIP): $(Port) 访问</li><li>在集群外部通过 $(NodeIP): $(NodePort) 访问</li></ul></li><li><strong>LoadBalancer</strong>:  通过云服务供应商（AWS、Azure、GCE 等）的负载均衡器在集群外部暴露 Service，同时自动创建 NodePort 和 ClusterIP 类型的访问方式<ul><li>在集群内部通过 $(ClusterIP): $(Port) 访问</li><li>在集群外部通过 $(NodeIP): $(NodePort) 访问</li><li>在集群外部通过 $(LoadBalancerIP): $(Port) 访问</li></ul></li><li><strong>ExternalName</strong>: 将 Service 映射到 <code>externalName</code> 指定的地址（例如：foo.bar.example.com），返回值是一个 CNAME 记录。不使用任何代理机制。</li></ul><h2 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h2><p>ClusterIP 是 ServiceType 的默认值</p><h2 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h2><p>对于 <code>NodePort</code> 类型的 Service，Kubernetes 为其分配一个节点端口（对于同一 Service，在每个节点上的节点端口都相同），该端口的范围在初始化 apiserver 时可通过参数 <code>--service-node-port-range</code> 指定（默认是：30000-32767）。节点将该端口上的网络请求转发到对应的 Service 上。可通过 Service 的 <code>.spec.ports[*].nodePort</code> 字段查看该 Service 分配到的节点端口号。</p><p>在启动 kube-proxy 时使用参数 <code>--nodeport-address</code> 可指定阶段端口可以绑定的 IP 地址段。该参数接收以逗号分隔的 CIDR 作为参数值（例如：10.0.0.0/8,192.0.2.0/25），kube-proxy 将查找本机符合该 CIDR 的 IP 地址，并将节点端口绑定到符合的 IP 地址上。</p><p>NodePort 类型的 Service 可通过如下方式访问：</p><ul><li>在集群内部通过 $(ClusterIP): $(Port) 访问</li><li>在集群外部通过 $(NodeIP): $(NodePort) 访问</li></ul><h2 id="LoadBalancer"><a href="#LoadBalancer" class="headerlink" title="LoadBalancer"></a>LoadBalancer</h2><p>在支持外部负载均衡器的云环境中（例如 GCE、AWS、Azure 等），将 <code>.spec.type</code> 字段设置为 <code>LoadBalancer</code>，Kubernetes 将为该Service 自动创建一个负载均衡器，负载均衡器的创建操作异步完成。</p><h2 id="ExternalName"><a href="#ExternalName" class="headerlink" title="ExternalName"></a>ExternalName</h2><p>ExternalName 类型的 Service 映射到一个外部的 DNS name，而不是一个 pod label selector。可通过 <code>spec.externalName</code> 字段指定外部 DNS name。</p><h2 id="External-IP"><a href="#External-IP" class="headerlink" title="External IP"></a>External IP</h2><p>如果有外部 IP 路由到 Kubernetes 集群的一个或多个节点，Kubernetes Service 可以通过这些 <code>externalIPs</code> 进行访问。<code>externalIP</code> 需要由集群管理员在 Kubernetes 之外配置。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Service的功能及作用&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>控制器-CronJob</title>
    <link href="http://yoursite.com/2020/04/08/%E6%8E%A7%E5%88%B6%E5%99%A8-CronJob/"/>
    <id>http://yoursite.com/2020/04/08/%E6%8E%A7%E5%88%B6%E5%99%A8-CronJob/</id>
    <published>2020-04-08T02:20:54.000Z</published>
    <updated>2020-04-08T05:31:25.505Z</updated>
    
    <content type="html"><![CDATA[<p> <strong>控制器CronJob：</strong> 周期性任务控制，不需要持续后台运行 </p> <a id="more"></a> <h1 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h1><p> CronJob 按照预定的时间计划（schedule）创建 Job。一个 CronJob 对象类似于 crontab (cron table) 文件中的一行记录。该对象根据 Cron 格式定义的时间计划，周期性地创建 Job 对象。 </p><p> CronJob 只负责按照时间计划的规定创建 Job 对象，由 Job 来负责管理具体 Pod 的创建和执行。 </p><h1 id="使用CronJob执行自动任务"><a href="#使用CronJob执行自动任务" class="headerlink" title="使用CronJob执行自动任务"></a>使用CronJob执行自动任务</h1><p>CronJob可以用来执行基于时间计划的定时任务，类似于Linux/Unix系统中的crontable。</p><p>CronJob 执行周期性的重复任务时非常有用，例如备份数据、发送邮件等。CronJob 也可以用来指定将来某个时间点执行单个任务，例如将某项任务定时到系统负载比较低的时候执行。</p><p>CronJob 也存在某些限制，例如，在某些情况下，一个 CronJob 可能会创建多个 Job。因此，Job 必须是幂等的。</p><h2 id="创建CronJob"><a href="#创建CronJob" class="headerlink" title="创建CronJob"></a>创建CronJob</h2><p>下面例子中的 CronJob 每分钟，打印一次当前时间并输出 hello world 信息。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronJob</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">schedule:</span> <span class="string">"*/1 * * * *"</span></span><br><span class="line">  <span class="attr">jobTemplate:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line">            <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">            <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">date;</span> <span class="string">echo</span> <span class="string">Hello</span> <span class="string">from</span> <span class="string">the</span> <span class="string">Kubernetes</span> <span class="string">cluster</span></span><br><span class="line">          <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br></pre></td></tr></table></figure><p> 执行命令以创建 CronJob： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]# kubectl create -f cronjob.yaml </span><br><span class="line">cronjob.batch&#x2F;hello created</span><br></pre></td></tr></table></figure><p>或者直接用kubectl run 命令来创建CronJob</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run hello --schedule=<span class="string">"*/1 * * * *"</span> --restart=OnFailure --image=busybox -- /bin/sh -c <span class="string">"date; echo Hello from the Kubernetes cluster"</span></span><br></pre></td></tr></table></figure><p>查看已创建的CronJob</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]# kubectl get cronjob hello</span><br><span class="line">NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE</span><br><span class="line">hello   *&#x2F;1 * * * *   False     1        31s             16m</span><br></pre></td></tr></table></figure><p>输出结果显示，该 CronJob 在 <code>LAST SCHEDULE</code> 这个时间点成功创建了一个 Job。当前 <code>ACTIVE</code> Job 数为 0，意味着，该 Job 已经成功结束，或者已经失败。 </p><h2 id="删除CronJob"><a href="#删除CronJob" class="headerlink" title="删除CronJob"></a>删除CronJob</h2><p>当您不再需要某个 CronJob 时，可以使用命令将其删除 <code>kubectl delete cronjob</code>，在本例中，可以执行命令：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete cronjob hello</span><br><span class="line">cronjob.batch <span class="string">"hello"</span> deleted</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line">kubectl delete -f  cronjob.yaml</span><br></pre></td></tr></table></figure><p>删除 CronJob 时，将移除该 CronJob 创建的所有 Job 和 Pod，并且 CronJob 控制器将不会为其在创建任何新的 Job。 </p><h2 id="写CronJob-YAML"><a href="#写CronJob-YAML" class="headerlink" title="写CronJob YAML"></a>写CronJob YAML</h2><p>与其他所有 Kubernetes 对象一样，CronJob 对象需要 <code>apiVersion</code>、<code>kind</code>、<code>metadata</code> 这几个字段。CronJob 还需要 <code>.spec</code> 字段。</p><p>所有对 CronJob 对象作出的修改，尤其是 <code>.spec</code> 的修改，都只对修改之后新建的 Job 有效，已经创建的 Job 不会受到影响</p><h3 id="Schedule"><a href="#Schedule" class="headerlink" title="Schedule"></a>Schedule</h3><p><code>.spec.schedule</code> 是一个必填字段。类型为 Cron 格式的字符串，例如 <code>0 * * * *</code> 或者 <code>@hourly</code>，该字段定义了 CronJob 应该何时创建和执行 Job。</p><p>该字段同样支持 <code>vixie cron</code> step 值，指定 CronJob 每隔两个小时执行一次，可以有如下三种写法：</p><ul><li><code>0 0,2,4,5,6,8,12,14,16,17,20,22 * * *</code>）</li><li>使用 范围值 + Step 值的写法：<code>0 0-23/2 * * *</code></li><li>Step 也可以跟在一个星号后面，如 <code>0 */2 * * *</code></li></ul><p>问号 <code>?</code> 与 星号 <code>*</code> 的含义相同，代表着该字段不做限定</p><h3 id="Job-Template"><a href="#Job-Template" class="headerlink" title="Job Template"></a>Job Template</h3><p><code>.spec.jobTemplate</code> 字段是必填字段。该字段的结构与Job相同，只是不需要 <code>apiVersion</code> 和 <code>kind</code>。</p><h3 id="tarting-Deadline"><a href="#tarting-Deadline" class="headerlink" title="tarting Deadline"></a>tarting Deadline</h3><p><code>.spec.startingDeadlineSeconds</code> 为可选字段，代表着从计划的时间点开始，最迟多少秒之内必须启动 Job。如果超过了这个时间点，CronJob 就不会为其创建 Job，并将其记录为一次错过的执行次数。如果该字段未指定，则 Job 必须在指定的时间点执行。</p><p>CronJob 控制器将为每一个 CronJob 记录错过了多少次执行次数，如果错过的执行次数超过 100，则控制器将不会再为该 CronJob 创建新的 Job。如果 <code>.spec.startingDeadlineSeconds</code> 未指定，CronJob 控制器计算从 <code>.status.lastScheduleTime</code> 开始到现在为止总共错过的执行次数。</p><p>例如，某一个 CronJob 应该每分钟执行一次，<code>.status.lastScheduleTime</code> 的值是 上午5:00，假设现在已经是上午7:00。这意味着已经有 120 次执行时间点被错过，因此该 CronJob 将不再执行了。</p><p>如果 <code>.spec.startingDeadlineSeconds</code> 字段被设置为一个非空的值，则 CronJob 控制器计算将从 <code>.spec.startingDeadlineSeconds</code> 秒以前到现在这个时间段内错过的执行次数。</p><p>例如，假设该字段被设置为 <code>200</code>，控制器将只计算过去 200 秒内错过的执行次数。如果在过去 200 秒之内，有超过 100 次错过的执行次数，则 CronJob 将不再执行。</p><h3 id="Concurrency-Policy"><a href="#Concurrency-Policy" class="headerlink" title="Concurrency Policy"></a>Concurrency Policy</h3><p><code>.spec.concurrencyPolicy</code> 是选填字段，指定了如何控制该 CronJob 创建的 Job 的并发性，可选的值有：</p><ul><li><code>Allow</code>： 默认值，允许并发运行 Job</li><li><code>Forbid</code>： 不允许并发运行 Job；如果新的执行时间点到了，而上一个 Job 还未执行完，则 CronJob 将跳过新的执行时间点，保留仍在运行的 Job，且不会在此刻创建新的 Job</li><li><code>Replace</code>： 如果新的执行时间点到了，而上一个 Job 还未执行完，则 CronJob 将创建一个新的 Job 以替代正在执行的 Job</li></ul><p>TIP</p><p>Concurrency policy 只对由同一个 CronJob 创建的 Job 生效。如果有多个 CronJob，则他们各自创建的 Job 之间不会相互影响。</p><h3 id="Suspend"><a href="#Suspend" class="headerlink" title="Suspend"></a>Suspend</h3><p><code>.spec.suspend</code> 是选填字段。如果该字段设置为 <code>true</code>，所有的后续执行都将挂起，该字段不会影响到已经创建的 Job。默认值为 <code>false</code>。</p><p><strong>警告</strong></p><p>挂起（suspend）的时间段内，如果恰好存在有计划的执行时间点，则这些执行时间计划都被记录下来。如果不指定 <code>.spec.startingDeadlineSeconds</code>，并将 <code>.spec.suspend</code> 字段从 <code>true</code> 修改为 <code>false</code>，则挂起这段时间内的执行计划都将被立刻执行。</p><h3 id="Job-History-Limits"><a href="#Job-History-Limits" class="headerlink" title="Job History Limits"></a>Job History Limits</h3><p><code>.spec.successfulJobsHistoryLimit</code> 和 <code>.spec.failedJobsHistoryLimit</code> 字段是可选的。这些字段指定了 CronJob 应该保留多少个 completed 和 failed 的 Job 记录。</p><ul><li><code>.spec.successfulJobsHistoryLimit</code> 的默认值为 3</li><li><code>.spec.failedJobsHistoryLimit</code> 的默认值为 1</li></ul><p>如果将其设置为 <code>0</code>，则 CronJob 不会保留已经结束的 Job 的记录。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;strong&gt;控制器CronJob：&lt;/strong&gt; 周期性任务控制，不需要持续后台运行 &lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>控制器-Job</title>
    <link href="http://yoursite.com/2020/04/07/%E6%8E%A7%E5%88%B6%E5%99%A8-Job/"/>
    <id>http://yoursite.com/2020/04/07/%E6%8E%A7%E5%88%B6%E5%99%A8-Job/</id>
    <published>2020-04-07T09:01:49.000Z</published>
    <updated>2020-04-08T02:21:57.991Z</updated>
    
    <content type="html"><![CDATA[<p> <strong>控制器-Job：</strong> 只要完成就立即退出，不需要重启或重建。 </p> <a id="more"></a> <h1 id="控制器-Job"><a href="#控制器-Job" class="headerlink" title="控制器 - Job"></a>控制器 - Job</h1><p>Kubernetes中的 Job 对象将创建一个或多个 Pod，并确保指定数量的 Pod 可以成功执行到进程正常结束：</p><ul><li>当 Job 创建的 Pod 执行成功并正常结束时，Job 将记录成功结束的 Pod 数量</li><li>当成功结束的 Pod 达到指定的数量时，Job 将完成执行</li><li>删除 Job 对象时，将清理掉由 Job 创建的 Pod</li></ul><p>一个简单的例子是：创建一个 Job 对象用来确保一个 Pod 的成功执行并结束。在第一个 Pod 执行失败或者被删除（例如，节点硬件故障或机器重启）的情况下，该 Job 对象将创建一个新的 Pod 以重新执行。</p><p>当然，也可以使用 Job 对象并行执行多个 Pod。</p><h2 id="运行一个Job的例子"><a href="#运行一个Job的例子" class="headerlink" title="运行一个Job的例子"></a>运行一个Job的例子</h2><p>在下面这个 Job 的例子中，Pod 执行了一个跟 π 相关的计算，并打印出最终结果，该计算大约需要 10 秒钟执行结束。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">perl</span></span><br><span class="line">        <span class="attr">command:</span> <span class="string">["perl",</span>  <span class="string">"-Mbignum=bpi"</span><span class="string">,</span> <span class="string">"-wle"</span><span class="string">,</span> <span class="string">"print bpi(2000)"</span><span class="string">]</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">4</span></span><br></pre></td></tr></table></figure><p>创建Job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]# kubectl apply -f  job1.yaml </span><br><span class="line">job.batch&#x2F;pi created</span><br></pre></td></tr></table></figure><p>查看已创建的Job</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="number">0407</span><span class="string">]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">job</span></span><br><span class="line"><span class="string">NAME</span>   <span class="string">COMPLETIONS</span>   <span class="string">DURATION</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">pi</span>     <span class="number">0</span><span class="string">/1</span>           <span class="string">38s</span>        <span class="string">38s</span></span><br><span class="line"></span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="number">0407</span><span class="string">]#</span> <span class="string">kubectl</span> <span class="string">describe</span> <span class="string">job</span></span><br><span class="line"><span class="attr">Name:</span>           <span class="string">pi</span></span><br><span class="line"><span class="attr">Namespace:</span>      <span class="string">default</span></span><br><span class="line"><span class="attr">Selector:</span>       <span class="string">controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60</span></span><br><span class="line"><span class="attr">Labels:</span>         <span class="string">controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60</span></span><br><span class="line">                <span class="string">job-name=pi</span></span><br><span class="line"><span class="attr">Annotations:    kubectl.kubernetes.io/last-applied-configuration:</span></span><br><span class="line">                  <span class="string">&#123;"apiVersion":"batch/v1","kind":"Job","metadata":&#123;"annotations":&#123;&#125;,"name":"pi","namespace":"default"&#125;,"spec":&#123;"backoffLimit":4,"template":...</span></span><br><span class="line"><span class="attr">Parallelism:</span>    <span class="number">1</span></span><br><span class="line"><span class="attr">Completions:</span>    <span class="number">1</span></span><br><span class="line"><span class="attr">Start Time:</span>     <span class="string">Tue,</span> <span class="number">07</span> <span class="string">Apr</span> <span class="number">2020</span> <span class="number">16</span><span class="string">:57:38</span> <span class="string">+0800</span></span><br><span class="line"><span class="attr">Pods Statuses:</span>  <span class="number">1</span> <span class="string">Running</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Succeeded</span> <span class="string">/</span> <span class="number">0</span> <span class="string">Failed</span></span><br><span class="line"><span class="attr">Pod Template:</span></span><br><span class="line">  <span class="attr">Labels:</span>  <span class="string">controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60</span></span><br><span class="line">           <span class="string">job-name=pi</span></span><br><span class="line">  <span class="attr">Containers:</span></span><br><span class="line">   <span class="attr">pi:</span></span><br><span class="line">    <span class="attr">Image:</span>      <span class="string">perl</span></span><br><span class="line">    <span class="attr">Port:</span>       <span class="string">&lt;none&gt;</span></span><br><span class="line">    <span class="attr">Host Port:</span>  <span class="string">&lt;none&gt;</span></span><br><span class="line">    <span class="attr">Command:</span></span><br><span class="line">      <span class="string">perl</span></span><br><span class="line">      <span class="string">-Mbignum=bpi</span></span><br><span class="line">      <span class="string">-wle</span></span><br><span class="line">      <span class="string">print</span> <span class="string">bpi(2000)</span></span><br><span class="line">    <span class="attr">Environment:</span>  <span class="string">&lt;none&gt;</span></span><br><span class="line">    <span class="attr">Mounts:</span>       <span class="string">&lt;none&gt;</span></span><br><span class="line">  <span class="attr">Volumes:</span>        <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="attr">Events:</span></span><br><span class="line">  <span class="string">Type</span>    <span class="string">Reason</span>            <span class="string">Age</span>   <span class="string">From</span>            <span class="string">Message</span></span><br><span class="line"></span><br><span class="line"><span class="string">----</span>    <span class="string">------</span>            <span class="string">----</span>  <span class="string">----</span>            <span class="string">-------</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">Normal  SuccessfulCreate  48s   job-controller  Created pod:</span> <span class="string">pi-nn8pf</span></span><br><span class="line">  </span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="number">0407</span><span class="string">]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pods</span></span><br><span class="line"><span class="string">NAME</span>                                <span class="string">READY</span>   <span class="string">STATUS</span>             <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">pi-nn8pf</span>                            <span class="number">0</span><span class="string">/1</span>     <span class="string">Completed</span>          <span class="number">0</span>          <span class="string">79s</span></span><br></pre></td></tr></table></figure><p> 执行以下命令可获得该 Job 所有 Pod 的名字： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]<span class="comment"># pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='&#123;.items[*].metadata.name&#125;')</span></span><br><span class="line">[root@k8s-master 0407]<span class="comment"># echo $pods</span></span><br><span class="line">pi-nn8pf</span><br><span class="line"></span><br><span class="line">在这个命令中：</span><br><span class="line">selector 与 Job 定义中的 selector 相同</span><br><span class="line">--output=jsonpath 选项指定了一个表达式，该表达式从返回结果列表中的每一个 Pod 的信息中定位出 name 字段的取值</span><br></pre></td></tr></table></figure><p> 执行以下命令可查看 Pod 的日志： </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0407]<span class="comment"># kubectl logs $pods</span></span><br><span class="line">3.14159265358979............................4780275901</span><br></pre></td></tr></table></figure><h1 id="编写Job的定义"><a href="#编写Job的定义" class="headerlink" title="编写Job的定义"></a>编写Job的定义</h1><p>与所有的 Kubernetes 对象一样，Job 对象的 YAML 文件中，都需要包括如下三个字段：</p><ul><li><code>.apiVersion</code></li><li><code>.kind</code></li><li><code>.metadata</code></li></ul><p>Job 对象的 YAML 文件，还需要一个 <code>.spec</code> 字段。</p><h2 id="Pod-Template"><a href="#Pod-Template" class="headerlink" title="Pod Template"></a>Pod Template</h2><p>除了 Pod 所需的必填字段之外，Job 中的 pod template 必须指定</p><ul><li>合适的标签 <code>.spec.template.spec.labels</code></li><li>指定合适的[重启策略 restartPolicy <code>.spec.template.spec.restartPolicy</code>，此处只允许使用 <code>Never</code> 和 <code>OnFailure</code> 两个取值</li></ul><h1 id="处理Pod和容器的失败"><a href="#处理Pod和容器的失败" class="headerlink" title="处理Pod和容器的失败"></a>处理Pod和容器的失败</h1><p>Pod 中的容器可能会因为多种原因执行失败，例如：</p><ul><li>容器中的进程退出了，且退出码（exit code）不为 0</li><li>容器因为超出内存限制而被 Kill</li><li>其他原因</li></ul><p>如果 Pod 中的容器执行失败，且 <code>.spec.template.spec.restartPolicy = &quot;OnFailure&quot;</code>，则 Pod 将停留在该节点上，但是容器将被重新执行。此时，应用程序需要处理在原节点（失败之前的节点）上重启的情况。或者，设置为 <code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code>。</p><p>整个 Pod 也可能因为多种原因执行失败，例如：</p><ul><li>Pod 从节点上被驱逐（节点升级、重启、被删除等）</li><li>Pod 的容器执行失败，且 <code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code></li></ul><p>当 Pod 执行失败时，Job 控制器将创建一个新的 Pod </p><h2 id="Pod失败重试"><a href="#Pod失败重试" class="headerlink" title="Pod失败重试"></a>Pod失败重试</h2><p>Pod backoff failure policy</p><p>某些情况下（例如，配置错误），可以在 Job 多次重试仍然失败的情况下停止该 Job。此时，可通过 <code>.spec.backoffLimit</code> 来设定 Job 最大的重试次数。该字段的默认值为 6.</p><p>Job 中的 Pod 执行失败之后，Job 控制器将按照一个指数增大的时间延迟（10s,20s,40s … 最大为 6 分钟）来多次重新创建 Pod。如果没有新的 Pod 执行失败，则重试次数的计数将被重置。</p><h1 id="Job的终止和清理"><a href="#Job的终止和清理" class="headerlink" title="Job的终止和清理"></a>Job的终止和清理</h1><p>当 Job 完成后：</p><ul><li>将不会创建新的 Pod</li><li>已经创建的 Pod 也不会被清理掉。此时，您仍然可以继续查看已结束 Pod 的日志，以检查 errors/warnings 或者其他诊断用的日志输出</li><li>Job 对象也仍然保留着，以便您可以查看该 Job 的状态</li><li>由用户决定是否删除已完成的 Job 及其 Pod<ul><li>可通过 <code>kubectl</code> 命令删除 Job，例如： <code>kubectl delete jobs/pi</code> 或者 <code>kubectl delete -f job.yaml</code></li><li>删除 Job 对象时，由该 Job 创建的 Pod 也将一并被删除</li></ul></li></ul><p>Job 通常会顺利的执行下去，但是在如下情况可能会非正常终止：</p><ul><li>某一个 Pod 执行失败（且 <code>restartPolicy=Never</code>）</li><li>或者某个容器执行出错（且 <code>restartPolicy=OnFailure</code>） <ul><li>此时，Job 按照处理Pod和容器的失败中 <code>.spec.bakcoffLimit</code> 描述的方式进行处理</li><li>一旦重试次数达到了 <code>.spec.backoffLimit</code> 中的值，Job 将被标记为失败，且尤其创建的所有 Pod 将被终止</li></ul></li><li>Job 中设置了 <code>.spec.activeDeadlineSeconds</code>。该字段限定了 Job 对象在集群中的存活时长，一旦达到 <code>.spec.activeDeadlineSeconds</code> 指定的时长，该 Job 创建的所有的 Pod 都将被终止，Job 的 Status 将变为 <code>type:Failed</code> 、 <code>reason: DeadlineExceeded</code></li></ul><h1 id="Job的自动清理"><a href="#Job的自动清理" class="headerlink" title="Job的自动清理"></a>Job的自动清理</h1><p>系统中已经完成的 Job 通常是不在需要里的，长期在系统中保留这些对象，将给 apiserver 带来很大的压力。如果通过更高级别的控制器（例如 CronJobs）来管理 Job，则 CronJob 可以根据其中定义的基于容量的清理策略自动清理Job。 </p><h2 id="TTL-机制"><a href="#TTL-机制" class="headerlink" title="TTL 机制"></a>TTL 机制</h2><p>除了 CronJob 之外，TTL 机制是另外一种自动清理已结束Job（<code>Completed</code> 或 <code>Finished</code>）的方式：</p><ul><li>TTL 机制由 TTL 控制器 提供</li><li>在 Job 对象中指定 <code>.spec.ttlSecondsAfterFinished</code> 字段可激活该特性</li></ul><p>当 TTL 控制器清理 Job 时，TTL 控制器将删除 Job 对象，以及由该 Job 创建的所有 Pod 对象。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: pi-with-ttl</span><br><span class="line">spec:</span><br><span class="line">  ttlSecondsAfterFinished: 100          <span class="comment">####</span></span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: pi</span><br><span class="line">        image: perl</span><br><span class="line">        <span class="built_in">command</span>: [<span class="string">"perl"</span>,  <span class="string">"-Mbignum=bpi"</span>, <span class="string">"-wle"</span>, <span class="string">"print bpi(2000)"</span>]</span><br><span class="line">      restartPolicy: Never</span><br></pre></td></tr></table></figure><p><strong>字段解释 <code>ttlSecondsAfterFinished</code>：</strong></p><ul><li>Job <code>pi-with-ttl</code> 的 <code>ttlSecondsAfterFinished</code> 值为 100，则，在其结束 <code>100</code> 秒之后，将可以被自动删除</li><li>如果 <code>ttlSecondsAfterFinished</code> 被设置为 <code>0</code>，则 TTL 控制器在 Job 执行结束后，立刻就可以清理该 Job 及其 Pod</li><li>如果 <code>ttlSecondsAfterFinished</code> 值未设置，则 TTL 控制器不会清理该 Job</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;strong&gt;控制器-Job：&lt;/strong&gt; 只要完成就立即退出，不需要重启或重建。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>控制器-Deployment</title>
    <link href="http://yoursite.com/2020/04/02/%E6%8E%A7%E5%88%B6%E5%99%A8-Deployment/"/>
    <id>http://yoursite.com/2020/04/02/%E6%8E%A7%E5%88%B6%E5%99%A8-Deployment/</id>
    <published>2020-04-02T02:57:28.000Z</published>
    <updated>2020-04-07T07:49:10.517Z</updated>
    
    <content type="html"><![CDATA[<p>控制器-Deployment</p> <a id="more"></a><h1 id="一、Deployment-概述"><a href="#一、Deployment-概述" class="headerlink" title="一、Deployment 概述"></a>一、Deployment 概述</h1><table><thead><tr><th>英文</th><th>英文简称</th><th>中文</th></tr></thead><tbody><tr><td>Pod</td><td>Pod</td><td>容器组</td></tr><tr><td>Controller</td><td>Controller</td><td>控制器</td></tr><tr><td>ReplicaSet</td><td>ReplicaSet</td><td>副本集</td></tr><tr><td>Deployment</td><td>Deployment</td><td>部署</td></tr></tbody></table><p>Deployment 是最常用的用于部署无状态服务的方式。Deployment 控制器使得您能够以声明的方式更新 Pod（容器组）和 ReplicaSet（副本集）。 </p><blockquote><p>声明式配置</p><p>声明的方式是相对于非声明方式而言的。例如，以滚动更新为例，假设有 3 个容器组，现需要将他们的容器镜像更新为新的版本。</p><ul><li>非声明的方式，需要手动逐步执行以下过程：<ul><li>使用 kubectl 创建一个新版本镜像的容器组</li><li>使用 kubectl 观察新建容器组的状态</li><li>新建容器组的状态就绪以后，使用 kubectl 删除一个旧的容器组</li><li>重复执行上述过程，直到所有容器组都已经替换为新版本镜像的容器组</li></ul></li><li>声明的方式，只需要执行：<ul><li>使用 kubectl 更新 Deployment 定义中 spec.template.spec.containers[].image 字段</li></ul></li></ul></blockquote><h1 id="二、创建-Deployment"><a href="#二、创建-Deployment" class="headerlink" title="二、创建 Deployment"></a>二、创建 Deployment</h1><p>下面的 yaml 文件定义了一个 Deployment，该 Deployment 将创建一个有 3 个 nginx Pod 副本的 ReplicaSet（副本集）： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p><strong>在这个例子中：</strong></p><ul><li>将创建一个名为 nginx-deployment 的 Deployment（部署），名称由 <code>.metadata.name</code> 字段指定</li><li>该 Deployment 将创建 3 个 Pod 副本，副本数量由 <code>.spec.replicas</code> 字段指定</li><li><code>.spec.selector</code> 字段指定了 Deployment 如何找到由它管理的 Pod。此案例中，我们使用了 Pod template 中定义的一个标签（app: nginx）。对于极少数的情况，这个字段也可以定义更加复杂的规则</li><li>.template 字段包含了如下字段：<ul><li><code>.template.metadata.labels</code> 字段，指定了 Pod 的标签（app: nginx）</li><li><code>.template.spec.containers[].image</code> 字段，表明该 Pod 运行一个容器 <code>nginx:1.7.9</code></li><li><code>.template.spec.containers[].name</code> 字段，表明该容器的名字是 <code>nginx</code></li></ul></li></ul><p><strong>注意：</strong></p><p>必须为 Deployment 中的 <code>.spec.selector</code> 和 <code>.template.metadata.labels</code> 定义一个合适的标签（这个例子中的标签是 app: nginx）。请不要使用与任何其他控制器（其他 Deployment / StatefulSet 等）相同的 <code>.spec.selector</code> 和 <code>.template.metadata.labels</code>。否则可能发生冲突。</p><p>1.执行命令以创建 Deployment， 可以为该命令增加 –record 选项， 这样可以回顾某一个 Deployment 版本变化的原因 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl apply -f deploy.yaml --record </span><br><span class="line">deployment.apps&#x2F;nginx-deployment configured</span><br></pre></td></tr></table></figure><p>2.查看 Deployment 的创建情况 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl get deployments</span><br><span class="line">NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">myapp              2&#x2F;2     2            2           2d1h</span><br><span class="line">nginx-deployment   2&#x2F;3     3            2           11m</span><br></pre></td></tr></table></figure><p><strong>字段含义</strong></p><table><thead><tr><th>字段名称</th><th>说明</th></tr></thead><tbody><tr><td><strong>NAME</strong></td><td>Deployment name</td></tr><tr><td><strong>DESIRED</strong></td><td>Deployment 期望的 Pod 副本数，即 Deployment 中 <code>.spec.replicas</code> 字段指定的数值。该数值是“期望”值</td></tr><tr><td><strong>CURRENT</strong></td><td>当前有多少个 Pod 副本数在运行</td></tr><tr><td><strong>UP-TO-DATE</strong></td><td>Deployment 中，符合当前 Pod Template 定义的 Pod 数量</td></tr><tr><td><strong>AVAILABLE</strong></td><td>当前对用户可用的 Pod 副本数</td></tr><tr><td><strong>AGE</strong></td><td>Deployment 部署以来到现在的时长</td></tr></tbody></table><p>3.查看该 Deployment 创建的 ReplicaSet（rs），执行命令 <code>kubectl get rs</code>，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">myapp-57c9b8fc4               2         2         2       2d1h</span><br><span class="line">nginx-deployment-54f57cf6bf   3         3         2       26m</span><br></pre></td></tr></table></figure><p> 4.查看 Pod 的标签，执行命令 <code>kubectl get pods --show-labels</code>，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl get pods --show-labels </span><br><span class="line">NAME                                READY   STATUS             RESTARTS   AGE     LABELS</span><br><span class="line">init-demo                           1&#x2F;1     Running            0          42h     &lt;none&gt;</span><br><span class="line">lifecycle-demo                      1&#x2F;1     Running            0          4d20h   &lt;none&gt;</span><br><span class="line">myapp-57c9b8fc4-b4cqb               1&#x2F;1     Running            0          2d      app&#x3D;myapp,pod-template-hash&#x3D;57c9b8fc4</span><br><span class="line">myapp-57c9b8fc4-xtcwv               1&#x2F;1     Running            0          2d1h    app&#x3D;myapp,pod-template-hash&#x3D;57c9b8fc4</span><br><span class="line">nginx-deployment-54f57cf6bf-4kl6r   1&#x2F;1     Running            0          26m     app&#x3D;nginx,pod-template-hash&#x3D;54f57cf6bf</span><br><span class="line">nginx-deployment-54f57cf6bf-7zc2j   1&#x2F;1     Running            0          26m     app&#x3D;nginx,pod-template-hash&#x3D;54f57cf6bf</span><br><span class="line">nginx-deployment-54f57cf6bf-jnzkj   0&#x2F;1     ImagePullBackOff   0          26m     app&#x3D;nginx,pod-template-hash&#x3D;54f57cf6bf</span><br></pre></td></tr></table></figure><h1 id="三、更新-Deployment"><a href="#三、更新-Deployment" class="headerlink" title="三、更新 Deployment"></a>三、更新 Deployment</h1><h3 id="使用下述步骤更新您的-Deployment"><a href="#使用下述步骤更新您的-Deployment" class="headerlink" title="使用下述步骤更新您的 Deployment"></a><strong>使用下述步骤更新您的 Deployment</strong></h3><h3 id="方式一："><a href="#方式一：" class="headerlink" title="方式一："></a>方式一：</h3><ol><li>执行以下命令，将容器镜像从 nginx:1.7.9 更新到 nginx:1.9.1</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl --record deployment.apps&#x2F;nginx-deployment set image deployment.v1.apps&#x2F;nginx-deployment nginx&#x3D;nginx:1.9.1</span><br><span class="line">deployment.apps&#x2F;nginx-deployment image updated</span><br><span class="line">deployment.apps&#x2F;nginx-deployment image updated</span><br></pre></td></tr></table></figure><h3 id="方式二："><a href="#方式二：" class="headerlink" title="方式二："></a>方式二：</h3><p>使用 <code>edit</code> 该 Deployment，并将 <code>.spec.template.spec.containers[0].image</code> 从 <code>nginx:1.7.9</code> 修改为 <code>nginx:1.9.1</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl edit deployment nginx-deployment</span><br><span class="line">deployment.apps&#x2F;nginx-deployment edited</span><br></pre></td></tr></table></figure><ul><li>如果想要修改这些新的 Pod，只需要再次修改 Deployment 的 Pod template。</li><li>Deployment 将确保更新过程中，任意时刻只有一定数量的 Pod 被关闭。默认情况下，Deployment 确保至少 <code>.spec.replicas</code> 的 75% 的 Pod 保持可用（25% max unavailable）</li><li>Deployment 将确保更新过程中，任意时刻只有一定数量的 Pod 被创建。默认情况下，Deployment 确保最多 <code>.spec.replicas</code> 的 25% 的 Pod 被创建（25% max surge）</li></ul><p>Deployment Controller 先创建一个新 Pod，然后删除一个旧 Pod，然后再创建新的，如此循环，直到全部更新。Deployment Controller 不会 kill 旧的 Pod，除非足够数量的新 Pod 已经就绪，Deployment Controller 也不会创新新 Pod 直到足够数量的旧 Pod 已经被 kill。这个过程将确保更新过程中，任意时刻，最少 2 个 / 最多 4 个 Pod 可用。</p><h2 id="覆盖更新-Rollover-（更新过程中再更新）"><a href="#覆盖更新-Rollover-（更新过程中再更新）" class="headerlink" title="覆盖更新 Rollover （更新过程中再更新）"></a>覆盖更新 Rollover （更新过程中再更新）</h2><p>每创建一个 Deployment，Deployment Controller 都为其创建一个 ReplicaSet，并设定其副本数为期望的 Pod 数（ <code>.spec.replicas</code> 字段）。如果 Deployment 被更新，旧的 ReplicaSet 将被 Scale down，新建的 ReplicaSet 将被 Scale up；直到最后新旧两个 ReplicaSet，一个副本数为 <code>.spec.replias</code>，另一个副本数为 0。这个过程称为 rollout。</p><p>当 Deployment 的 rollout 正在进行中的时候，如果再次更新 Deployment 的信息，此时 Deployment 将再创建一个新的 ReplicaSet 并开始将其 scale up，将先前正在 scale up 的 ReplicaSet 也作为一个旧的 ReplicaSet，并开始将其 scale down。</p><p>例如：</p><ul><li>假设创建了一个 Deployment 有 5 个 nginx:1.7.9 的副本；</li><li>立刻更新该 Deployment 使得其 <code>.spec.replicas</code> 为 5，容器镜像为 <code>nginx:1.9.1</code>，而此时只有 3 个 nginx:1.7.9 的副本已创建；</li><li>此时，Deployment Controller 将立刻开始 kill 已经创建的 3 个 nginx:1.7.9 的 Pod，并开始创建 nginx:1.9.1 的 Pod。Deployment Controller 不会等到 5 个 nginx:1.7.9 的 Pod 都创建完之后在开始新的更新</li></ul><h1 id="四、回滚-Deployment"><a href="#四、回滚-Deployment" class="headerlink" title="四、回滚 Deployment"></a>四、回滚 Deployment</h1><h2 id="模拟更新错误"><a href="#模拟更新错误" class="headerlink" title="模拟更新错误"></a>模拟更新错误</h2><ul><li><p>假设您在更新 Deployment 的时候，犯了一个拼写错误，将 <code>nginx:1.9.1</code> 写成了 <code>nginx:1.91</code></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">set</span> image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=<span class="literal">true</span></span><br><span class="line">deployment.apps/nginx-deployment image updated</span><br></pre></td></tr></table></figure></li><li><p>该更新将卡住，执行命令 <code>kubectl rollout status deployment.v1.apps/nginx-deployment</code> 检查其状态，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Waiting for rollout to finish: 1 out of 3 new replicas have been updated...</span><br></pre></td></tr></table></figure></li></ul><h2 id="检查Deployment的更新历史"><a href="#检查Deployment的更新历史" class="headerlink" title="检查Deployment的更新历史"></a>检查Deployment的更新历史</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl rollout history deployment nginx-deployment</span><br><span class="line">deployment.apps&#x2F;nginx-deployment </span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">2         kubectl deployment.apps&#x2F;nginx-deployment set image deployment.v1.apps&#x2F;nginx-deployment nginx&#x3D;nginx:1.9.1 --record&#x3D;true</span><br><span class="line">3         kubectl deployment.apps&#x2F;nginx-deployment set image deployment.v1.apps&#x2F;nginx-deployment nginx&#x3D;nginx:1.9.1 --record&#x3D;true</span><br></pre></td></tr></table></figure><p>执行命令 <code>kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2</code>，查看 revision（版本）的详细信息，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl rollout history deployment.v1.apps&#x2F;nginx-deployment --revision&#x3D;2</span><br><span class="line">deployment.apps&#x2F;nginx-deployment with revision #2</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:app&#x3D;nginx</span><br><span class="line">pod-template-hash&#x3D;56f8998dbc</span><br><span class="line">  Annotations:kubernetes.io&#x2F;change-cause:</span><br><span class="line">  kubectl deployment.apps&#x2F;nginx-deployment set image deployment.v1.apps&#x2F;nginx-deployment nginx&#x3D;nginx:1.9.1 --record&#x3D;true</span><br><span class="line">  Containers:</span><br><span class="line">   nginx:</span><br><span class="line">    Image:nginx:1.9.1</span><br><span class="line">    Port:80&#x2F;TCP</span><br><span class="line">    Host Port:0&#x2F;TCP</span><br><span class="line">    Environment:&lt;none&gt;</span><br><span class="line">    Mounts:&lt;none&gt;</span><br><span class="line">  Volumes:&lt;none&gt;</span><br></pre></td></tr></table></figure><h2 id="回滚到前一个-revision（版本）"><a href="#回滚到前一个-revision（版本）" class="headerlink" title="回滚到前一个 revision（版本）"></a>回滚到前一个 revision（版本）</h2><p>下面的步骤可将 Deployment 从当前版本回滚到前一个版本（version 2）</p><p>执行命令 <code>kubectl rollout undo deployment.v1.apps/nginx-deployment</code> 将当前版本回滚到前一个版本，输出结果如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deployment.apps&#x2F;nginx-deployment</span><br></pre></td></tr></table></figure><p> 或者，也可以使用 <code>--to-revision</code> 选项回滚到前面的某一个指定版本，执行如下命令： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2</span><br></pre></td></tr></table></figure><ul><li>此时，Deployment 已经被回滚到前一个稳定版本。可以看到 Deployment Controller 为该 Deployment 产生了 DeploymentRollback event。</li><li>执行命令 <code>kubectl get deployment nginx-deployment</code>，检查该回滚是否成功，Deployment 是否按预期的运行。</li></ul><h1 id="五、伸缩-Deployment"><a href="#五、伸缩-Deployment" class="headerlink" title="五、伸缩 Deployment"></a>五、伸缩 Deployment</h1><h2 id="执行伸缩"><a href="#执行伸缩" class="headerlink" title="执行伸缩"></a>执行伸缩</h2><ul><li>执行命令 <code>kubectl scale deployment  nginx-deployment --replicas=5</code>，可以伸缩 Deployment，输出结果如下所示：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]<span class="comment"># kubectl scale deployment nginx-deployment --replicas=5</span></span><br><span class="line">deployment.apps/nginx-deployment scaled</span><br><span class="line"></span><br><span class="line">[root@k8s-master 0402]<span class="comment"># kubectl get deployment</span></span><br><span class="line">NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   4/5     5            4           5h59m</span><br><span class="line"></span><br><span class="line">[root@k8s-master 0402]<span class="comment"># kubectl get pod</span></span><br><span class="line">NAME                                READY   STATUS             RESTARTS   AGE</span><br><span class="line">nginx-deployment-54f57cf6bf-4kl6r   1/1     Running            0          5h59m</span><br><span class="line">nginx-deployment-54f57cf6bf-7zc2j   1/1     Running            0          5h59m</span><br><span class="line">nginx-deployment-54f57cf6bf-lch2d   1/1     Running            0          35s</span><br><span class="line">nginx-deployment-54f57cf6bf-mzrzq   1/1     Running            0          35s</span><br><span class="line">nginx-deployment-54f57cf6bf-pmgj8   0/1     ImagePullBackOff   0          3h22m</span><br></pre></td></tr></table></figure><p> 如果集群启用了自动伸缩，执行以下命令，就可以基于 CPU 的利用率在一个最大和最小的区间自动伸缩您的 Deployment： </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80</span><br></pre></td></tr></table></figure><h2 id="按比例伸缩"><a href="#按比例伸缩" class="headerlink" title="按比例伸缩"></a>按比例伸缩</h2><p>例如，假设已经运行了一个 10 副本数的 Deployment，其 maxSurge=3, maxUnavailable=2。</p><ul><li>执行命令 <code>kubectl scale deployment.v1.apps/nginx-deployment --replicas=15</code>，将 Deployment 的 replicas 调整到 15。此时，Deployment Controller 需要决定如何分配新增的 5 个 Pod 副本。根据“按比例伸缩”的原则：<ul><li>更大比例的新 Pod 数被分配到副本数最多的 ReplicaSet</li><li>更小比例的新 Pod 数被分配到副本数最少的 ReplicaSet</li><li>如果还有剩余的新 Pod 数未分配，则将被增加到副本数最多的 ReplicaSet</li><li>副本数为 0 的 ReplicaSet，scale up 之后，副本数仍然为 0</li></ul></li><li>在本例中，3 个新副本被添加到旧的 ReplicaSet，2个新副本被添加到新的 ReplicaSet。如果新的副本都达到就绪状态，滚动更新过程最终会将所有的副本数添加放到新 ReplicaSet。</li></ul><h1 id="六、暂停和继续-Deployment"><a href="#六、暂停和继续-Deployment" class="headerlink" title="六、暂停和继续 Deployment"></a>六、暂停和继续 Deployment</h1><p>可以先暂停 Deployment，然后再触发一个或多个更新，最后再继续（resume）该 Deployment。这种做法使得在暂停和继续中间对 Deployment 做多次更新，而无需触发不必要的滚动更新。 </p><p>执行命令 <code>kubectl rollout pause deployment.v1.apps/nginx-deployment</code> 暂停 Deployment，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl rollout pause deployment nginx-deployment </span><br><span class="line">deployment.apps&#x2F;nginx-deployment paused</span><br></pre></td></tr></table></figure><p>执行命令 <code>kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.17.9</code>，更新 Deployment 的容器镜像，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl set image deployment&#x2F;nginx-deployment nginx&#x3D;nginx:1.17.9</span><br><span class="line">deployment.apps&#x2F;nginx-deployment image updated</span><br></pre></td></tr></table></figure><p>执行命令 <code>kubectl rollout resume deployment/nginx-deployment</code>，继续（resume）该 Deployment，可使前面所有的变更一次性生效，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl rollout resume deployment nginx-deployment </span><br><span class="line">deployment.apps&#x2F;nginx-deployment resumed</span><br></pre></td></tr></table></figure><p>执行命令 <code>kubectl get rs -o wide</code> 查看 ResultSet 的最终状态，输出结果如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master 0402]# kubectl get rs -o wide</span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         SELECTOR</span><br><span class="line">myapp-57c9b8fc4               2         2         2       3d      nginx        nginx          app&#x3D;myapp,pod-template-hash&#x3D;57c9b8fc4</span><br><span class="line">nginx-deployment-54f57cf6bf   1         1         1       23h     nginx        nginx:1.7.9    app&#x3D;nginx,pod-template-hash&#x3D;54f57cf6bf</span><br><span class="line">nginx-deployment-56f8998dbc   0         0         0       23h     nginx        nginx:1.9.1    app&#x3D;nginx,pod-template-hash&#x3D;56f8998dbc</span><br><span class="line">nginx-deployment-6654964744   5         5         3       5m8s    nginx        nginx:1.17.9   app&#x3D;nginx,pod-template-hash&#x3D;6654964744</span><br></pre></td></tr></table></figure><p>不能回滚（rollback）一个已暂停的 Deployment，除非继续（resume）该 Deployment。 </p><h1 id="七、查看Deployment的状态"><a href="#七、查看Deployment的状态" class="headerlink" title="七、查看Deployment的状态"></a>七、查看Deployment的状态</h1><h2 id="Progressing-状态"><a href="#Progressing-状态" class="headerlink" title="Progressing 状态"></a>Progressing 状态</h2><p>当如下任何一个任务正在执行时，Kubernete 将 Deployment 的状态标记为 <strong><em>progressing</em></strong>：</p><ul><li>Deployment 创建了一个新的 ReplicaSet</li><li>Deployment 正在 scale up 其最新的 ReplicaSet</li><li>Deployment 正在 scale down 其旧的 ReplicaSet</li><li>新的 Pod 变为 <strong><em>就绪（ready）</em></strong> 或 <strong><em>可用（available）</em></strong></li></ul><p>可以使用命令 <code>kubectl rollout status</code> 监控 Deployment 滚动更新的过程</p><h2 id="Complete-状态"><a href="#Complete-状态" class="headerlink" title="Complete 状态"></a>Complete 状态</h2><p>如果 Deployment 符合以下条件，Kubernetes 将其状态标记为 <strong><em>complete</em></strong>：</p><ul><li>该 Deployment 中的所有 Pod 副本都已经被更新到指定的最新版本</li><li>该 Deployment 中的所有 Pod 副本都处于 <strong><em>可用（available）</em></strong> 状态</li><li>该 Deployment 中没有旧的 ReplicaSet 正在运行</li></ul><p>可以执行命令 <code>kubectl rollout status</code> 检查 Deployment 是否已经处于 <strong><em>complete</em></strong> 状态。如果是，则该命令的退出码为 0。 </p><p>例如，执行命令 <code>kubectl rollout status deployment.v1.apps/nginx-deployment</code>，输出结果如下所示：</p><h2 id="Failed-状态"><a href="#Failed-状态" class="headerlink" title="Failed 状态"></a>Failed 状态</h2><p>Deployment 在更新其最新的 ReplicaSet 时，可能卡住而不能达到 <strong><em>complete</em></strong> 状态。如下原因都可能导致此现象发生：</p><ul><li>集群资源不够</li><li>就绪检查（readiness probe）失败</li><li>镜像抓取失败</li><li>权限不够</li><li>资源限制</li><li>应用程序的配置错误导致启动失败</li></ul><p>指定 Deployment 定义中的 <code>.spec.progressDeadlineSeconds</code> 字段，Deployment Controller 在等待指定的时长后，将 Deployment 的标记为处理失败。</p><h2 id="操作处于-Failed-状态的-Deployment"><a href="#操作处于-Failed-状态的-Deployment" class="headerlink" title="操作处于 Failed 状态的 Deployment"></a>操作处于 Failed 状态的 Deployment</h2><p>可以针对 <strong><em>Failed</em></strong> 状态下的 Deployment 执行任何适用于 Deployment 的指令，例如：</p><ul><li>scale up / scale down</li><li>回滚到前一个版本</li><li>暂停（pause）Deployment，以对 Deployment 的 Pod template 执行多处更新</li></ul><h1 id="八、清除策略"><a href="#八、清除策略" class="headerlink" title="八、清除策略"></a>八、清除策略</h1><p>通过 Deployment 中 <code>.spec.revisionHistoryLimit</code> 字段，可指定为该 Deployment 保留多少个旧的 ReplicaSet。超出该数字的将被在后台进行垃圾回收。该字段的默认值是 10。</p><p>如果该字段被设为 0，Kubernetes 将清理掉该 Deployment 的所有历史版本（revision），将无法对该 Deployment 执行回滚操作 <code>kubectl rollout undo</code>。</p><h1 id="九、部署策略"><a href="#九、部署策略" class="headerlink" title="九、部署策略"></a>九、部署策略</h1><p> 通过 Deployment 中 <code>.spec.strategy</code> 字段，可以指定使用 <code>滚动更新 RollingUpdate</code> 的部署策略还是使用 <code>重新创建 Recreate</code> 的部署策略 </p><p>其中字段的含义如下：</p><table><thead><tr><th>字段名称</th><th>可选值</th><th>字段描述</th></tr></thead><tbody><tr><td>类型</td><td>滚动更新 重新创建</td><td>如果选择重新创建，Deployment将先删除原有副本集中的所有 Pod，然后再创建新的副本集和新的 Pod。如此，更新过程中将出现一段应用程序不可用的情况；</td></tr><tr><td>最大超出副本数</td><td>数字或百分比</td><td>滚动更新过程中，可以超出期望副本数的最大值。 该取值可以是一个绝对值（例如：5），也可以是一个相对于期望副本数的百分比（例如：10%）； 如果填写百分比，则以期望副本数乘以该百分比后向上取整的方式计算对应的绝对值； 当最大超出副本数 maxUnavailable 为 0 时，此数值不能为 0；默认值为 25%。 例如：假设此值被设定为 30%，当滚动更新开始时，新的副本集（ReplicaSet）可以立刻扩容， 但是旧 Pod 和新 Pod 的总数不超过 Deployment 期待副本数（spec.repilcas）的 130%。 一旦旧 Pod 被终止后，新的副本集可以进一步扩容，但是整个滚动更新过程中，新旧 Pod 的总 数不超过 Deployment 期待副本数（spec.repilcas）的 130%。</td></tr><tr><td>最大不可用副本数</td><td>数字或百分比</td><td>滚动更新过程中，不可用副本数的最大值。 该取值可以是一个绝对值（例如：5），也可以是一个相对于期望副本数的百分比（例如：10%）； 如果填写百分比，则以期望副本数乘以该百分比后向下取整的方式计算对应的绝对值； 当最大超出副本数 maxSurge 为 0 时，此数值不能为 0；默认值为 25%； 例如：假设此值被设定为 30%，当滚动更新开始时，旧的副本集（ReplicaSet）可以缩容到期望 副本数的 70%；在新副本集扩容的过程中，一旦新的 Pod 已就绪，旧的副本集可以进一步缩容， 整个滚动更新过程中，确保新旧就绪副本数之和不低于期望副本数的 70%。</td></tr></tbody></table><h1 id="十、金丝雀发布（灰度发布）"><a href="#十、金丝雀发布（灰度发布）" class="headerlink" title="十、金丝雀发布（灰度发布）"></a>十、金丝雀发布（灰度发布）</h1><p>可以使用 Deployment 将最新的应用程序版本发布给一部分用户（或服务器），为每个版本创建一个 Deployment，此时，应用程序的新旧两个版本都可以同时获得生产上的流量。 </p><p>部署第一个版本</p><p>第一个版本的 Deployment 包含了 3 个Pod副本，Service 通过 label selector <code>app: nginx</code> 选择对应的 Pod，nginx 的标签为 <code>1.7.9</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-port</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">32600</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br></pre></td></tr></table></figure><p> 假设此时想要发布新的版本 nginx <code>1.8.0</code>，可以创建第二个 Deployment： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment-canary</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">track:</span> <span class="string">canary</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">track:</span> <span class="string">canary</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">track:</span> <span class="string">canary</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.8.0</span></span><br></pre></td></tr></table></figure><ul><li>因为 Service 的LabelSelector 是 <code>app: nginx</code>，由 <code>nginx-deployment</code> 和 <code>nginx-deployment-canary</code> 创建的 Pod 都带有标签 <code>app: nginx</code>，所以，Service 的流量将会在两个 release 之间分配</li><li>在新旧版本之间，流量分配的比例为两个版本副本数的比例，此处为 1:3</li></ul><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>按照 Kubernetes 默认支持的这种方式进行金丝雀发布，有一定的局限性：</p><ul><li>不能根据用户注册时间、地区等请求中的内容属性进行流量分配</li><li>同一个用户如果多次调用该 Service，有可能第一次请求到了旧版本的 Pod，第二次请求到了新版本的 Pod</li></ul><p>在 Kubernetes 中不能解决上述局限性的原因是：Kubernetes Service 只在 TCP 层面解决负载均衡的问题，并不对请求响应的消息内容做任何解析和识别。 </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;控制器-Deployment&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>控制器 - ReplicaSet</title>
    <link href="http://yoursite.com/2020/04/01/%E6%8E%A7%E5%88%B6%E5%99%A8-ReplicaSet/"/>
    <id>http://yoursite.com/2020/04/01/%E6%8E%A7%E5%88%B6%E5%99%A8-ReplicaSet/</id>
    <published>2020-04-01T06:57:39.000Z</published>
    <updated>2020-04-07T08:18:57.740Z</updated>
    
    <content type="html"><![CDATA[<p> 控制器概述及控制器-ReplicaSet</p><a id="more"></a> <h1 id="控制器-概述"><a href="#控制器-概述" class="headerlink" title="控制器_概述"></a>控制器_概述</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Pod（容器组）是 Kubernetes 中最小的调度单元，可以通过 kubectl 直接创建一个 Pod。Pod 本身并不能自愈（self-healing）。如果一个 Pod 所在的 Node （节点）出现故障，或者调度程序自身出现故障，Pod 将被删除；同理，当因为节点资源不够或节点维护而驱逐 Pod 时，Pod 也将被删除。</p><p>Kubernetes 通过引入 Controller（控制器）的概念来管理 Pod 实例。在 Kubernetes 中，创建 Pod 始终应该用 Controller 来创建 Pod，而不是直接创建 Pod。</p><p>Pod控制器是用于实现管理 Pod 的中间层，确保 Pod 资源符合预期的状态，Pod 的资源出现故障时，会尝试进行重启，当根据重启策略无效，则会重新新建 Pod 的资源.。</p><p><strong>控制器可以提供如下特性：</strong></p><ul><li>水平扩展（运行 Pod 的多个副本）</li><li>rollout（版本更新）</li><li>self-healing（故障恢复） 例如：当一个节点出现故障，控制器可以自动地在另一个节点调度一个配置完全一样的 Pod，以替换故障节点上的 Pod。</li></ul><h2 id="控制器类型"><a href="#控制器类型" class="headerlink" title="控制器类型"></a>控制器类型</h2><ul><li><strong>控制器-ReplicaSet：</strong> 代用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能。<br>ReplicaSet主要三个组件组成：<br>　　（1）用户期望的pod副本数量<br>　　（2）标签选择器，判断哪个pod归自己管理<br>　　（3）当现存的pod数量不足，会根据pod资源模板进行新建<br>帮助用户管理无状态的pod资源，精确反应用户定义的目标数量，但是RelicaSet不是直接使用的控制器，而是使用Deployment。 </li><li><strong>控制器-Deployment：</strong> 工作在ReplicaSet之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置。 </li><li><strong>控制器-StatefulSet：</strong> 周期性任务控制，不需要持续后台运行。</li><li><strong>控制器-DaemonSet：</strong> 用于确保集群中的每一个节点只运行特定的pod副本，通常用于实现系统级后台任务。比如ELK服务<br>特性：服务是无状态的，服务必须是守护进程。 </li><li><strong>控制器-Job：</strong> 只要完成就立即退出，不需要重启或重建。 </li><li><strong>控制器CronJob：</strong> 周期性任务控制，不需要持续后台运行。</li></ul><h1 id="控制器-ReplicaSet"><a href="#控制器-ReplicaSet" class="headerlink" title="控制器 - ReplicaSet"></a>控制器 - ReplicaSet</h1><p>Kubernetes 中，ReplicaSet 用来维护一个数量稳定的 Pod 副本集合，可以保证某种定义一样的 Pod 始终有指定数量的副本数在运行。 </p><h2 id="ReplicaSet的工作方式"><a href="#ReplicaSet的工作方式" class="headerlink" title="ReplicaSet的工作方式"></a>ReplicaSet的工作方式</h2><p>ReplicaSet的定义中，包含：</p><ul><li><code>selector</code>： 用于指定哪些 Pod 属于该 ReplicaSet 的管辖范围</li><li><code>replicas</code>： 副本数，用于指定该 ReplicaSet 应该维持多少个 Pod 副本</li><li><code>template</code>： Pod模板，在 ReplicaSet 使用 Pod 模板的定义创建新的 Pod</li></ul><p>ReplicaSet 控制器将通过创建或删除 Pod，以使得当前 Pod 数量达到 <code>replicas</code> 指定的期望值。ReplicaSet 创建的 Pod 中，都有一个字段 metadata.ownerReferences 用于标识该 Pod 从属于哪一个 ReplicaSet。</p><p>ReplicaSet 通过 <code>selector</code> 字段的定义，识别哪些 Pod 应该由其管理。如果 Pod 没有 ownerReference 字段，或者 ownerReference 字段指向的对象不是一个控制器，且该 Pod 匹配了 ReplicaSet 的 <code>selector</code>，则该 Pod 的 ownerReference 将被修改为 该 ReplicaSet 的引用。</p><h2 id="何时使用-ReplicaSet"><a href="#何时使用-ReplicaSet" class="headerlink" title="何时使用 ReplicaSet"></a>何时使用 ReplicaSet</h2><p>ReplicaSet 用来维护一个数量稳定的 Pod 副本集合。Deployment 可以管理 ReplicaSet，并提供声明式的更新等。因此，推荐用户总是使用 Deployment，而不是直接使用 ReplicaSet，除非需要一些自定义的更新应用程序的方式，或者完全不更新应用。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">guestbook</span></span><br><span class="line">    <span class="attr">tier:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># modify replicas according to your case</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">tier:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">tier:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure><p>ReplicaSet 副本集的主要几个字段有：</p><ul><li>selector 确定哪些 Pod 属于该副本集</li><li>replicas 副本集应该维护几个 Pod 副本（实例）</li><li>template Pod 的定义</li></ul><p>副本集将通过创建、删除 Pod 容器组来确保符合 selector 选择器的 Pod 数量等于 replicas 指定的数量。当符合 selector 选择器的 Pod 数量不够时，副本集通过使用 template 中的定义来创建 Pod。</p><p>执行命令以创建该 YAML 对应的 ReplicaSet </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-yamls]# kubectl apply -f rs.yaml </span><br><span class="line">replicaset.apps&#x2F;frontend created</span><br><span class="line">[root@k8s-master k8s-yamls]# kubectl get rs</span><br><span class="line">NAME                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">frontend              3         3         1       10s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master k8s-yamls]# kubectl describe rs&#x2F;frontend</span><br><span class="line">Name:         frontend</span><br><span class="line">Namespace:    default</span><br><span class="line">Selector:     tier&#x3D;frontend</span><br><span class="line">Labels:       app&#x3D;guestbook</span><br><span class="line">              tier&#x3D;frontend</span><br><span class="line">Annotations:  kubectl.kubernetes.io&#x2F;last-applied-configuration:</span><br><span class="line">                &#123;&quot;apiVersion&quot;:&quot;apps&#x2F;v1&quot;,&quot;kind&quot;:&quot;ReplicaSet&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;app&quot;:&quot;guestbook&quot;,&quot;tier&quot;:&quot;frontend&quot;&#125;,&quot;name&quot;:&quot;frontend&quot;,...</span><br><span class="line">Replicas:     3 current &#x2F; 3 desired</span><br><span class="line">Pods Status:  2 Running &#x2F; 1 Waiting &#x2F; 0 Succeeded &#x2F; 0 Failed</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  tier&#x3D;frontend</span><br><span class="line">  Containers:</span><br><span class="line">   nginx:</span><br><span class="line">    Image:        nginx</span><br><span class="line">    Port:         &lt;none&gt;</span><br><span class="line">    Host Port:    &lt;none&gt;</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:       &lt;none&gt;</span><br><span class="line">  Volumes:        &lt;none&gt;</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age    From                   Message</span><br><span class="line"></span><br><span class="line">----    ------            ----   ----                   -------</span><br><span class="line"></span><br><span class="line">  Normal  SuccessfulCreate  2m58s  replicaset-controller  Created pod: frontend-fsfrx</span><br><span class="line">  Normal  SuccessfulCreate  2m58s  replicaset-controller  Created pod: frontend-72vr4</span><br><span class="line">  Normal  SuccessfulCreate  2m58s  replicaset-controller  Created pod: frontend-zp2k8</span><br></pre></td></tr></table></figure><h2 id="ReplicaSet的定义"><a href="#ReplicaSet的定义" class="headerlink" title="ReplicaSet的定义"></a>ReplicaSet的定义</h2><p>与其他 Kubernetes 对象一样，ReplicaSet需要的字段有：</p><ul><li><code>apiVersion</code>：apps/v1</li><li><code>kind</code>：始终为 ReplicaSet</li><li><code>metadata</code></li><li><code>spec</code>： ReplicaSet 的详细定义</li></ul><h3 id="PodTemplate"><a href="#PodTemplate" class="headerlink" title="PodTemplate"></a>PodTemplate</h3><p><code>.spec.template</code> 字段是一个 Pod Template，为必填字段，且其中必须定义 <code>.spec.template.metadata.labels</code> 字段。在前面的ReplicaSet例子中，定义了 label 为 <code>tier: frontend</code>。请小心该字段不要与其他控制器的 selector 重合，以免这些控制器尝试接管该 Pod。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.spec.template.spec.restartPolicy&#96; 的默认值为 &#96;Always</span><br></pre></td></tr></table></figure><h3 id="Pod-Selector"><a href="#Pod-Selector" class="headerlink" title="Pod Selector"></a>Pod Selector</h3><p><code>.spec.selector</code> 字段为一个标签选择器，用于识别可以接管哪些 Pod。在前面的例子中，标签选择器为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">matchLabels:</span></span><br><span class="line"><span class="attr">tier:</span> <span class="string">frontend</span></span><br></pre></td></tr></table></figure><p>在 ReplicaSet 中， <code>.spec.template.metadata.labels</code> 必须与 <code>.spec.selector</code> 匹配，否则将不能成功创建 ReplicaSet。</p><p>如果两个 ReplicaSet 指定了相同的 <code>.spec.selector</code> 但是不同的 <code>.spec.template.metadata.labels</code> 和不同的 <code>.spec.tempalte.spec</code> 字段，两个 ReplicaSet 都将忽略另外一个 ReplicaSet 创建的 Pod</p><h3 id="Replicas"><a href="#Replicas" class="headerlink" title="Replicas"></a>Replicas</h3><p><code>.spec.replicas</code> 字段用于指定同时运行的 Pod 的副本数。ReplicaSet 将创建或者删除由其管理的 Pod，以便使副本数与该字段指定的值匹配。</p><p>如果不指定，默认值为 1</p><h2 id="使用-ReplicaSet"><a href="#使用-ReplicaSet" class="headerlink" title="使用 ReplicaSet"></a>使用 ReplicaSet</h2><h3 id="删除ReplicaSet及其Pod"><a href="#删除ReplicaSet及其Pod" class="headerlink" title="删除ReplicaSet及其Pod"></a>删除ReplicaSet及其Pod</h3><p>使用 <code>kubectl delete</code> 可删除 ReplicaSet， Garbage Collector将自动删除该 ReplicaSet 所有从属的 Pod。</p><h3 id="只删除ReplicaSet"><a href="#只删除ReplicaSet" class="headerlink" title="只删除ReplicaSet"></a>只删除ReplicaSet</h3><p>使用 <code>kubectl delete --cascade=false</code> 命令，可以删除 ReplicaSet，但是仍然保留其 Pod。</p><p>一旦原来的 ReplicaSet 被删除，可以创建新的 ReplicaSet 作为替代。只要新 ReplicaSet 的 <code>.spec.selector</code> 字段与旧 ReplicaSet 的 <code>.spec.selector</code> 字段相同，则新的 ReplicaSet 将接管旧 ReplicaSet 遗留下来的 Pod。但是，新的 ReplicaSet 中定义的 <code>.spec.template</code> 对遗留下来的 Pod 不会产生任何影响。 </p><h3 id="将Pod从ReplicaSet中隔离"><a href="#将Pod从ReplicaSet中隔离" class="headerlink" title="将Pod从ReplicaSet中隔离"></a>将Pod从ReplicaSet中隔离</h3><p>修改 Pod 的标签，可以使 Pod 脱离 ReplicaSet 的管理。这个小技巧在如下场景可能非常有用：</p><ul><li>将 Pod 从 Service 中移除，以便 Debug 或者做数据恢复</li></ul><p>通过这种方式从 ReplicaSet 移除了 Pod 之后，ReplicaSet 将立刻自动创建一个新的 Pod 以维持其指定的 <code>replicas</code> 副本数。</p><h3 id="ReplicaSet的自动伸缩"><a href="#ReplicaSet的自动伸缩" class="headerlink" title="ReplicaSet的自动伸缩"></a>ReplicaSet的自动伸缩</h3><p>可以使用 Horizontal Pod Autoscalers(HPA) 对 ReplicaSet 执行自动的水平伸缩。下面例子中的 HPA 可以用来对前面例子中的 ReplicaSet 执行自动的水平伸缩：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">autoscaling/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">HorizontalPodAutoscaler</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend-scaler</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">scaleTargetRef:</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="attr">minReplicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">maxReplicas:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">targetCPUUtilizationPercentage:</span> <span class="number">50</span></span><br></pre></td></tr></table></figure><p>此外，也可以使用 <code>kubectl autoscale</code> 命令达到相同的效果：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl autoscale rs frontend --max=10</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 控制器概述及控制器-ReplicaSet&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s应用健康状态</title>
    <link href="http://yoursite.com/2020/04/01/K8s%E5%BA%94%E7%94%A8%E5%81%A5%E5%BA%B7%E7%8A%B6%E6%80%81/"/>
    <id>http://yoursite.com/2020/04/01/K8s%E5%BA%94%E7%94%A8%E5%81%A5%E5%BA%B7%E7%8A%B6%E6%80%81/</id>
    <published>2020-04-01T06:10:31.000Z</published>
    <updated>2020-04-01T06:52:54.190Z</updated>
    
    <content type="html"><![CDATA[<p> 应用健康状态和常见应用异常；</p><a id="more"></a> <h1 id="应用健康状态"><a href="#应用健康状态" class="headerlink" title="应用健康状态"></a>应用健康状态</h1><h2 id="应用健康状态-使用方式"><a href="#应用健康状态-使用方式" class="headerlink" title="应用健康状态-使用方式"></a>应用健康状态-使用方式</h2><h4 id="探测方式"><a href="#探测方式" class="headerlink" title="探测方式"></a>探测方式</h4><p>Liveness 指针和 Readiness 指针支持三种不同的探测方式：</p><ol><li>第一种是 httpGet。它是通过发送 http Get 请求来进行判断的，当返回码是 200-399 之间的状态码时，标识这个应用是健康的；</li><li>第二种探测方式是 Exec。它是通过执行容器中的一个命令来判断当前的服务是否是正常的，当命令行的返回结果是 0，则标识容器是健康的；</li><li>第三种探测方式是 tcpSocket 。它是通过探测容器的 IP 和 Port 进行 TCP 健康检查，如果这个 TCP 的链接能够正常被建立，那么标识当前这个容器是健康的。</li></ol><h4 id="探测结果"><a href="#探测结果" class="headerlink" title="探测结果"></a>探测结果</h4><p>从探测结果来讲主要分为三种：</p><ul><li>第一种是 success，当状态是 success 的时候，表示 container 通过了健康检查，也就是 Liveness probe 或 Readiness probe 是正常的一个状态；</li><li>第二种是 Failure，Failure 表示的是这个 container 没有通过健康检查，如果没有通过健康检查的话，那么此时就会进行相应的一个处理，那在 Readiness 处理的一个方式就是通过 service。service 层将没有通过 Readiness 的 pod 进行摘除，而 Liveness 就是将这个 pod 进行重新拉起，或者是删除。</li><li>第三种状态是 Unknown，Unknown 是表示说当前的执行的机制没有进行完整的一个执行，可能是因为类似像超时或者像一些脚本没有及时返回，那么此时 Readiness-probe 或 Liveness-probe 会不做任何的一个操作，会等待下一次的机制来进行检验。</li></ul><p>那在 kubelet 里面有一个叫 ProbeManager 的组件，这个组件里面会包含 Liveness-probe 或 Readiness-probe，这两个 probe 会将相应的 Liveness 诊断和 Readiness 诊断作用在 pod 之上，来实现一个具体的判断。</p><h3 id="应用健康状态-Pod-Probe-Spec"><a href="#应用健康状态-Pod-Probe-Spec" class="headerlink" title="应用健康状态-Pod Probe Spec"></a>应用健康状态-Pod Probe Spec</h3><p><strong>exec：</strong>如下图所示，可以看到这是一个 Liveness probe，它里面配置了一个 exec 的一个诊断。接下来，它又配置了一个 command 的字段，这个 command 字段里面通过 cat 一个具体的文件来判断当前 Liveness probe 的状态，当这个文件里面返回的结果是 0 时，或者说这个命令返回是 0 时，它会认为此时这个 pod 是处在健康的一个状态。 </p><p> <img src="https://kuboard.cn/assets/img/1936763b70d81b10f9e14c2f612819ed19865cf6.c7aeb95b.png" alt="4"> </p><p><strong>httpGet：</strong>httpGet 里面有一个字段是路径，第二个字段是 port，第三个是 headers。这个地方有时需要通过类似像 header 头的一个机制做 health 的一个判断时，需要配置这个 header，通常情况下，可能只需要通过 health 和 port 的方式就可以了。 </p><p> <img src="https://kuboard.cn/assets/img/f1053686bdb48dc98171a06351be4d5f139218b1.7b257b18.png" alt="5"> </p><p><strong>tcpSocket：</strong>tcpSocket 的使用方式其实也比较简单，只需要设置一个检测的端口，像下图使用的是 8080 端口，当这个 8080 端口 tcp connect 审核正常被建立的时候，那 tecSocket，Probe 会认为是健康的一个状态。 </p><p> <img src="https://kuboard.cn/assets/img/7f2a4c83600974bf9e6c7007c848e3eaad3b239d.7b0422c1.png" alt="6"> </p><p>此外还有如下的五个参数，是 Global 的参数。</p><ol><li>initialDelaySeconds：它表示的是说这个 pod 启动延迟多久进行一次检查，比如说现在有一个 Java 的应用，它启动的时间可能会比较长，因为涉及到 jvm 的启动，包括 Java 自身 jar 的加载。所以前期，可能有一段时间是没有办法被检测的，而这个时间又是可预期的，那这时可能要设置一下 initialDelaySeconds；</li><li>periodSeconds：它表示的是检测的时间间隔，正常默认的这个值是 10 秒；</li><li>timeoutSeconds：它表示的是检测的超时时间，当超时时间之内没有检测成功，那它会认为是失败的一个状态；</li><li>successThreshold：它表示的是：当这个 pod 从探测失败到再一次判断探测成功，所需要的阈值次数，默认情况下是 1 次，表示原本是失败的，那接下来探测这一次成功了，就会认为这个 pod 是处在一个探针状态正常的一个状态；</li><li>failureThreshold：它表示的是探测失败的重试次数，默认值是 3，表示的是当从一个健康的状态连续探测 3 次失败，那此时会判断当前这个pod的状态处在一个失败的状态。</li></ol><h3 id="应用健康状态-Liveness-与-Readiness-总结"><a href="#应用健康状态-Liveness-与-Readiness-总结" class="headerlink" title="应用健康状态-Liveness 与 Readiness 总结"></a>应用健康状态-Liveness 与 Readiness 总结</h3><table><thead><tr><th></th><th>Liveness</th><th>Readness</th></tr></thead><tbody><tr><td><strong>介绍</strong></td><td>用于判断容器是否存活，即Pod状态是否为Running，如果Liveness指针判断容器不健康则会触发kubelet杀掉容器，并根据配置的策略判断是否重启容器，如果默认不配置Liveness探针，则任务返回值默认为成功</td><td>用于判断容器是否启动完成，即Pod的Condition是否为Ready，如果探测结果不成功，则会将Pod从Endpoint中移除，直至下次判断成功，再将Pod挂回到Endpoint上。</td></tr><tr><td><strong>检测失败</strong></td><td>杀掉Pod</td><td>切断上层流量到Pod</td></tr><tr><td><strong>适用场景</strong></td><td>支持重新拉起的应用</td><td>启动后无法立即对外服务的应用</td></tr><tr><td><strong>注意事项</strong></td><td>不论是Liveness还是Readness探针，选择适合的探测方式可以防止被误操作 1.调大判断的差事阙值，防止在容器压力较高的情况下出现偶发超时 2.调整判断的次数阙值，3次的默认值在短周期下不一定是最佳实践 3.exec的如果执行的是shell脚本判断，在容器中可能调用时间会非常长 4.使用tcpSocket的方式遇到TLS的场景，需要业务层判断是否有影响</td><td></td></tr></tbody></table><h3 id="应用故障排查-了解状态机制"><a href="#应用故障排查-了解状态机制" class="headerlink" title="应用故障排查-了解状态机制"></a>应用故障排查-了解状态机制</h3><p>因为 K8S 的设计是面向状态机的，它里面通过 yaml 的方式来定义的是一个期望到达的一个状态，而真正这个 yaml 在执行过程中会由各种各样的 controller来负责整体的状态之间的一个转换。</p><table><thead><tr><th>Phase</th><th>描述</th></tr></thead><tbody><tr><td>Pending</td><td>Kubernetes 已经创建并确认该 Pod。此时可能有两种情况：Pod 还未完成调度（例如没有合适的节点）正在从 docker registry 下载镜像</td></tr><tr><td>Running</td><td>该 Pod 已经被绑定到一个节点，并且该 Pod 所有的容器都已经成功创建。其中至少有一个容器正在运行，或者正在启动/重启</td></tr><tr><td>Succeeded</td><td>Pod 中的所有容器都已经成功终止，并且不会再被重启</td></tr><tr><td>Failed</td><td>Pod 中的所有容器都已经终止，至少一个容器终止于失败状态：容器的进程退出码不是 0，或者被系统 kill</td></tr><tr><td>Unknown</td><td>因为某些未知原因，不能确定 Pod 的状态，通常的原因是 master 与 Pod 所在节点之间的通信故障</td></tr></tbody></table><h3 id="用故障排查-常见应用异常"><a href="#用故障排查-常见应用异常" class="headerlink" title="用故障排查-常见应用异常"></a>用故障排查-常见应用异常</h3><p>pod 上面可能会停留几个常见的状态。</p><h4 id="Pod-停留在-Pending"><a href="#Pod-停留在-Pending" class="headerlink" title="Pod 停留在 Pending"></a>Pod 停留在 Pending</h4><p>第一个就是 pending 状态，pending 表示调度器没有进行介入。此时可以通过 kubectl describe pod 来查看相应的事件，如果由于资源或者说端口占用，或者是由于 node selector 造成 pod 无法调度的时候，可以在相应的事件里面看到相应的结果，这个结果里面会表示说有多少个不满足的 node，有多少是因为 CPU 不满足，有多少是由于 node 不满足，有多少是由于 tag 打标造成的不满足。</p><h4 id="Pod-停留在-waiting"><a href="#Pod-停留在-waiting" class="headerlink" title="Pod 停留在 waiting"></a>Pod 停留在 waiting</h4><p>那第二个状态就是 pod 可能会停留在 waiting 的状态，pod 的 states 处在 waiting 的时候，通常表示说这个 pod 的镜像没有正常拉取，原因可能是由于这个镜像是私有镜像，但是没有配置 Pod secret；那第二种是说可能由于这个镜像地址是不存在的，造成这个镜像拉取不下来；还有一个是说这个镜像可能是一个公网的镜像，造成镜像的拉取失败。</p><h4 id="Pod-不断被拉取并且可以看到-crashing"><a href="#Pod-不断被拉取并且可以看到-crashing" class="headerlink" title="Pod 不断被拉取并且可以看到 crashing"></a>Pod 不断被拉取并且可以看到 crashing</h4><p>第三种是 pod 不断被拉起，而且可以看到类似像 backoff 。这个通常表示说 pod 已经被调度完成了，但是启动失败，那这个时候通常要关注的应该是这个应用自身的一个状态，并不是说配置是否正确、权限是否正确，此时需要查看的应该是 pod 的具体日志。</p><h4 id="Pod-处在-Runing-但是没有正常工作"><a href="#Pod-处在-Runing-但是没有正常工作" class="headerlink" title="Pod 处在 Runing 但是没有正常工作"></a>Pod 处在 Runing 但是没有正常工作</h4><p>第四种 pod 处在 running 状态，但是没有正常对外服务。那此时比较常见的一个点就可能是由于一些非常细碎的配置，类似像有一些字段可能拼写错误，造成了 yaml 下发下去了，但是有一段没有正常地生效，从而使得这个 pod 处在 running 的状态没有对外服务，那此时可以通过 apply-validate-f pod.yaml 的方式来进行判断当前 yaml 是否是正常的，如果 yaml 没有问题，那么接下来可能要诊断配置的端口是否是正常的，以及 Liveness 或 Readiness 是否已经配置正确。</p><h4 id="Service-无法正常的工作"><a href="#Service-无法正常的工作" class="headerlink" title="Service 无法正常的工作"></a>Service 无法正常的工作</h4><p>最后一种就是 service 无法正常工作的时候，该怎么去判断呢？那比较常见的 service 出现问题的时候，是自己的使用上面出现了问题。因为 service 和底层的 pod 之间的关联关系是通过 selector 的方式来匹配的，也就是说 pod 上面配置了一些 label，然后 service 通过 match label 的方式和这个 pod 进行相互关联。如果这个 label 配置的有问题，可能会造成这个 service 无法找到后面的 endpoint，从而造成相应的 service 没有办法对外提供服务，那如果 service 出现异常的时候，第一个要看的是这个 service 后面是不是有一个真正的 endpoint，其次来看这个 endpoint 是否可以对外提供正常的服务。</p><h3 id="开源的调试工具-kubectl-debug"><a href="#开源的调试工具-kubectl-debug" class="headerlink" title="开源的调试工具 - kubectl-debug"></a>开源的调试工具 - kubectl-debug</h3><p> kubectl-debug 这个工具是依赖于 Linux namespace 的方式来去做的，它可以 datash 一个 Linux namespace 到一个额外的 container，然后在这个 container 里面执行任何的 debug 动作，其实和直接去 debug 这个 Linux namespace 是一致的。 </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>关于 Liveness 和 Readiness 的指针。Liveness probe 就是保活指针，它是用来看 pod 是否存活的，而 Readiness probe 是就绪指针，它是判断这个 pod 是否就绪的，如果就绪了，就可以对外提供服务。这个就是 Liveness 和 Readiness 需要记住的部分；</li><li>应用诊断的三个步骤：首先 describe 相应的一个状态；然后提供状态来排查具体的一个诊断方向；最后来查看相应对象的一个 event 获取更详细的一个信息；</li><li>提供 pod 一个日志来定位应用的自身的一个状态；</li><li>远程调试的一个策略，如果想把本地的应用代理到远程集群，此时可以通过 Telepresence 这样的工具来实现，如果想把远程的应用代理到本地，然后在本地进行调用或者是调试，可以用类似像 port-forward 这种机制来实现</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; 应用健康状态和常见应用异常；&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
  <entry>
    <title>K8s容器组 Pod</title>
    <link href="http://yoursite.com/2020/03/31/K8s%E5%AE%B9%E5%99%A8%E7%BB%84/"/>
    <id>http://yoursite.com/2020/03/31/K8s%E5%AE%B9%E5%99%A8%E7%BB%84/</id>
    <published>2020-03-31T07:06:33.000Z</published>
    <updated>2020-04-01T06:07:02.600Z</updated>
    
    <content type="html"><![CDATA[<p>Pod的相关理解</p> <a id="more"></a> <h1 id="一、Pod-容器组-概述"><a href="#一、Pod-容器组-概述" class="headerlink" title="一、Pod 容器组_概述"></a>一、Pod 容器组_概述</h1><h2 id="什么是-Pod-容器组"><a href="#什么是-Pod-容器组" class="headerlink" title="什么是 Pod 容器组"></a>什么是 Pod 容器组</h2><p>Pod（容器组）是 Kubernetes 中最小的可部署单元。</p><p>一个 Pod（容器组）包含了一个应用程序容器（某些情况下是多个容器）、存储资源、一个唯一的网络 IP 地址、以及一些确定容器该如何运行的选项。</p><p>Pod 容器组代表了 Kubernetes 中一个独立的应用程序运行实例，该实例可能由单个容器或者几个紧耦合在一起的容器组成。</p><p>Kubernetes 集群中的 Pod 存在如下两种使用途径：</p><ul><li>一个 Pod 中只运行一个容器。”one-container-per-pod” 是 Kubernetes 中最常见的使用方式。Kubernetes 通过 Pod 管理容器，而不是直接管理容器。</li><li>一个 Pod 中运行多个需要互相协作的容器。可以将多个紧密耦合、共享资源且始终在一起运行的容器编排在同一个 Pod 中。</li></ul><p>每一个 Pod 都是用来运行某一特定应用程序的一个实例。如果想要水平扩展应用程序（运行多个实例），运行多个 Pod 容器组，每一个代表应用程序的一个实例。</p><ul><li>Kubernetes 中，称其为 replication（复制副本）。</li><li>Kubernetes 中 Controller（控制器）负责为应用程序创建和管理这些复制的副本。 </li></ul><h2 id="Pod-如何管理多个容器"><a href="#Pod-如何管理多个容器" class="headerlink" title="Pod 如何管理多个容器"></a>Pod 如何管理多个容器</h2><p>Pod 的设计目的是用来支持多个互相协同的容器，使得形成一个有意义的服务单元。一个 Pod 中的多个容器很自然就可以随 Pod 被一起调度到集群中的同一个物理机或虚拟机上。Pod 中的容器可以：</p><ul><li>共享资源、依赖</li><li>互相通信</li><li>相互协商何时以何种方式结束运行</li></ul><p><strong>Pod 为其成员容器提供了两种类型的共享资源：网络和存储</strong></p><ul><li><p>网络 Networking</p><p>每一个 Pod 被分配一个独立的 IP 地址。Pod 中的所有容器共享一个网络名称空间：</p></li></ul><ol><li>​    同一个 Pod 中的所有容器 IP 地址都相同</li><li>​    同一个 Pod 中的不同容器不能使用相同的端口，否则会导致端口冲突</li><li>​    同一个 Pod 中的不同容器可以通过 localhost:port 进行通信</li><li>​    同一个 Pod 中的不同容器可以通过使用常规的进程间通信手段，例如 SystemV semaphores 或者 POSIX 共享内存</li></ol><ul><li>存储 Storage</li></ul><p>Pod 中可以定义一组共享的数据卷。Pod 中所有的容器都可以访问这些共享数据卷，以便共享数据。Pod 中数据卷的数据也可以存储持久化的数据，使得容器在重启后仍然可以访问到之前存入到数据卷中的数据。</p><p><code>不同 Pod 上的两个容器如果要通信，必须使用对方 Pod 的 IP 地址 + 对方容器的端口号进行网络通信</code> </p><h2 id="使用-Pod"><a href="#使用-Pod" class="headerlink" title="使用 Pod"></a>使用 Pod</h2><p> 在 Pod 被创建后（直接创建，或者间接通过 Controller 创建），将被调度到集群中的一个节点上运行。Pod 将一直保留在该节点上，直到 Pod 以下情况发生： </p><ul><li><p>Pod 中的容器全部结束运行</p></li><li><p>Pod 被删除</p></li><li><p>由于节点资源不够，Pod 被驱逐</p></li><li><p>节点出现故障（例如死机）</p></li><li><input disabled type="checkbox"> <p>Pod 本身并不会运行，Pod 仅仅是容器运行的一个环境 </p></li></ul><p>Pod 本身并不能自愈（self-healing）。如果一个 Pod 所在的 Node （节点）出现故障，或者调度程序自身出现故障，Pod 将被删除；同理，当因为节点资源不够或节点维护而驱逐 Pod 时，Pod 也将被删除。Kubernetes 通过引入 Controller（控制器）的概念来管理 Pod 实例。在 Kubernetes 中，更为推荐的做法是使用 Controller 来管理 Pod，而不是直接创建 Pod。 </p><h2 id="容器组和控制器"><a href="#容器组和控制器" class="headerlink" title="容器组和控制器"></a>容器组和控制器</h2><p>用户应该始终使用控制器来创建 Pod，而不是直接创建 Pod，控制器可以提供如下特性：</p><ul><li><p>水平扩展（运行 Pod 的多个副本）</p></li><li><p>rollout（版本更新）</p></li><li><p>self-healing（故障恢复）</p><p>例如：当一个节点出现故障，控制器可以自动地在另一个节点调度一个配置完全一样的 Pod，以替换故障节点上的 Pod。</p></li></ul><p>在 Kubernetes 中，广泛使用的控制器有：</p><ul><li>Deployment</li><li>StatefulSet</li><li>DaemonSet</li></ul><h2 id="Termination-of-Pods（终止Pod）"><a href="#Termination-of-Pods（终止Pod）" class="headerlink" title="Termination of Pods（终止Pod）"></a>Termination of Pods（终止Pod）</h2><p>Pod 代表了运行在集群节点上的进程，而进程的终止有两种方式：</p><ul><li>gracefully terminate （优雅地终止）</li><li>直接 kill，此时进程没有机会执行清理动作</li></ul><p><strong>当用户发起删除 Pod 的指令时，Kubernetes 需要：</strong></p><ul><li>让用户知道 Pod 何时被删除</li><li>确保删除 Pod 的指令最终能够完成</li></ul><p><strong>Kubernetes 收到用户删除 Pod 的指令后：</strong></p><ol><li>记录强制终止前的等待时长（grace period）</li><li>向 Pod 中所有容器的主进程发送 TERM 信号</li><li>一旦等待超时，向超时的容器主进程发送 KILL 信号</li><li>删除 Pod 在 API Server 中的记录</li></ol><p>默认情况下，删除 Pod 的 grace period（等待时长）是 30 秒。</p><p>可以通过 kubectl delete 命令的选项 <code>--grace-period=</code> 自己指定 grace period（等待时长）。</p><p>强制删除 Pod，必须为 kubectl delete 命令同时指定两个选项 <code>--grace-period=0</code> 和 <code>--force</code> </p><h1 id="二、Pod-容器组-声明周期"><a href="#二、Pod-容器组-声明周期" class="headerlink" title="二、Pod 容器组_声明周期"></a>二、Pod 容器组_声明周期</h1><h2 id="Pod-phase"><a href="#Pod-phase" class="headerlink" title="Pod phase"></a>Pod phase</h2><table><thead><tr><th>Phase</th><th>描述</th></tr></thead><tbody><tr><td>Pending</td><td>Kubernetes 已经创建并确认该 Pod。此时可能有两种情况：Pod 还未完成调度（例如没有合适的节点）正在从 docker registry 下载镜像</td></tr><tr><td>Running</td><td>该 Pod 已经被绑定到一个节点，并且该 Pod 所有的容器都已经成功创建。其中至少有一个容器正在运行，或者正在启动/重启</td></tr><tr><td>Succeeded</td><td>Pod 中的所有容器都已经成功终止，并且不会再被重启</td></tr><tr><td>Failed</td><td>Pod 中的所有容器都已经终止，至少一个容器终止于失败状态：容器的进程退出码不是 0，或者被系统 kill</td></tr><tr><td>Unknown</td><td>因为某些未知原因，不能确定 Pod 的状态，通常的原因是 master 与 Pod 所在节点之间的通信故障</td></tr></tbody></table><h2 id="容器的检查"><a href="#容器的检查" class="headerlink" title="容器的检查"></a>容器的检查</h2><p>Probe 是指 kubelet 周期性地检查容器的状况。</p><p>有三种类型的 Probe：</p><ul><li><strong>ExecAction：</strong> 在容器内执行一个指定的命令。如果该命令的退出状态码为 0，则成功</li><li><strong>TCPSocketAction：</strong> 探测容器的指定 TCP 端口，如果该端口处于 open 状态，则成功</li><li><strong>HTTPGetAction：</strong> 探测容器指定端口/路径上的 HTTP Get 请求，如果 HTTP 响应状态码在 200 到 400（不包含400）之间，则成功</li></ul><p>Probe 有三种可能的结果：</p><ul><li><strong>Success：</strong> 容器通过检测</li><li><strong>Failure：</strong> 容器未通过检测</li><li><strong>Unknown：</strong> 检测执行失败，此时 kubelet 不做任何处理</li></ul><p>Kubelet 可以在两种情况下对运行中的容器执行 Probe：</p><ul><li><strong>就绪检查 readinessProbe：</strong> 确定容器是否已经就绪并接收服务请求。如果就绪检查失败，kubernetes 将该 Pod 的 IP 地址从所有匹配的 Service 的资源池中移除掉。</li><li><strong>健康检查 livenessProbe：</strong> 确定容器是否正在运行。如果健康检查失败，kubelete 将结束该容器，并根据 restart policy（重启策略）确定是否重启该容器。</li></ul><h3 id="何时使用-健康检查-就绪检查？"><a href="#何时使用-健康检查-就绪检查？" class="headerlink" title="何时使用 健康检查/就绪检查？"></a>何时使用 健康检查/就绪检查？</h3><ul><li>如果容器中的进程在碰到问题时可以自己 crash，并不需要执行健康检查；kubelet 可以自动的根据 Pod 的 restart policy（重启策略）执行对应的动作</li><li>如果希望在容器的进程无响应后，将容器 kill 掉并重启，则指定一个健康检查 liveness probe，并同时指定 restart policy（重启策略）为 Always 或者 OnFailure</li><li>如果想在探测 Pod 确实就绪之后才向其分发服务请求，请指定一个就绪检查 readiness probe。此时，就绪检查的内容可能和健康检查相同。就绪检查适合如下几类容器：<ul><li>初始化时需要加载大量的数据、配置文件</li><li>启动时需要执行迁移任务</li></ul></li></ul><h2 id="容器的状态"><a href="#容器的状态" class="headerlink" title="容器的状态"></a>容器的状态</h2><p>一旦 Pod 被调度到节点上，kubelet 便开始使用容器引擎（通常是 docker）创建容器。容器有三种可能的状态：Waiting / Running / Terminated：</p><ul><li><strong>Waiting：</strong> 容器的初始状态。处于 Waiting 状态的容器，仍然有对应的操作在执行，例如：拉取镜像、应用 Secrets等。</li><li><strong>Running：</strong> 容器处于正常运行的状态。容器进入 Running 状态之后，如果指定了 postStart hook，该钩子将被执行。</li><li><strong>Terminated：</strong> 容器处于结束运行的状态。容器进入 Terminated 状态之前，如果指定了 preStop hook，该钩子将被执行。</li></ul><h2 id="重启策略"><a href="#重启策略" class="headerlink" title="重启策略"></a>重启策略</h2><p>定义 Pod 或工作负载时，可以指定 restartPolicy，可选的值有：</p><ul><li>Always （默认值）</li><li>OnFailure</li><li>Never</li></ul><p>restartPolicy 将作用于 Pod 中的所有容器。kubelete 将在五分钟内，按照递延的时间间隔（10s, 20s, 40s ……）尝试重启已退出的容器，并在十分钟后再次启动这个循环，直到容器成功启动，或者 Pod 被删除。</p><h2 id="容器组的存活期"><a href="#容器组的存活期" class="headerlink" title="容器组的存活期"></a>容器组的存活期</h2><p>通常，如果没有人或者控制器删除 Pod，Pod 不会自己消失。</p><p>只有一种例外，那就是 Pod 处于 Scucceeded 或 Failed 的 phase，并超过了垃圾回收的时长（在 kubernetes master 中通过 terminated-pod-gc-threshold 参数指定），kubelet 自动将其删除。</p><h1 id="三、Pod-容器组-初始化容器"><a href="#三、Pod-容器组-初始化容器" class="headerlink" title="三、Pod 容器组_初始化容器"></a>三、Pod 容器组_初始化容器</h1><h2 id="初始化容器的行为"><a href="#初始化容器的行为" class="headerlink" title="初始化容器的行为"></a>初始化容器的行为</h2><ul><li>Pod 的启动时，首先初始化网络和数据卷，然后按顺序执行每一个初始化容器。任何一个初始化容器都必须成功退出，才能开始下一个初始化容器。如果某一个容器启动失败或者执行失败，kubelet 将根据 Pod 的 restartPolicy 决定是否重新启动 Pod。</li><li>只有所有的初始化容器全都执行成功，Pod 才能进入 ready 状态。初始化容器的端口是不能够通过 kubernetes Service 访问的。Pod 在初始化过程中处于 Pending 状态，并且同时有一个 type 为 <code>initializing</code> status 为 <code>True</code> 的 Condition</li><li>如果 Pod 重启，所有的初始化容器也将被重新执行。</li><li>可以组合使用就绪检查和 activeDeadlineSeconds ，以防止初始化容器始终失败。</li><li>Pod 中不能包含两个同名的容器（初始化容器和工作容器也不能同名）。</li></ul><h3 id="Pod-重启的原因"><a href="#Pod-重启的原因" class="headerlink" title="Pod 重启的原因"></a>Pod 重启的原因</h3><p>Pod 重启时，所有的初始化容器都会重新执行，Pod 重启的原因可能有：</p><ul><li>用户更新了 Pod 的定义，并改变了初始化容器的镜像<ul><li>改变任何一个初始化容器的镜像，将导致整个 Pod 重启</li><li>改变工作容器的镜像，将只重启该工作容器，而不重启 Pod</li></ul></li><li>Pod 容器基础设施被重启（例如 docker engine），这种情况不常见，通常只有 node 节点的 root 用户才可以执行此操作</li><li>Pod 中所有容器都已经结束，restartPolicy 是 Always，且初始化容器执行的记录已经被垃圾回收，此时将重启整个 Pod</li></ul><h2 id="配置初始化容器"><a href="#配置初始化容器" class="headerlink" title="配置初始化容器"></a>配置初始化容器</h2><p>Pod的配置文件如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">init-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">  <span class="comment"># These containers are run during pod initialization</span></span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">install</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">wget</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"-O"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">"/work-dir/index.html"</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">https://kuboard.cn</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">"/work-dir"</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">Default</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir</span></span><br><span class="line">    <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><p> 从配置文件可以看出，Pod 中初始化容器和应用程序共享了同一个数据卷。初始化容器将该共享数据卷挂载到 <code>/work-dir</code> 路径，应用程序容器将共享数据卷挂载到 <code>/usr/share/nginx/html</code> 路径。初始化容器执行如下命令后，就退出执行： <code>wget -O /work-dir/index.html https://kuboard.cn</code></p><p>验证：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master k8s-yamls]# kubectl get pod init-demo</span><br><span class="line">NAME        READY   STATUS    RESTARTS   AGE</span><br><span class="line">init-demo   1&#x2F;1     Running   0          22s</span><br><span class="line">[root@k8s-master k8s-yamls]# kubectl exec -it init-demo -- &#x2F;bin&#x2F;bash</span><br><span class="line">root@init-demo:&#x2F;# apt-get update</span><br></pre></td></tr></table></figure><h1 id="四、Pod-容器组-Debug初始化容器"><a href="#四、Pod-容器组-Debug初始化容器" class="headerlink" title="四、Pod 容器组_Debug初始化容器"></a>四、Pod 容器组_Debug初始化容器</h1><p>查看 Pod 的状态：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod &lt;pod-name&gt;</span><br><span class="line">NAME         READY     STATUS     RESTARTS   AGE</span><br><span class="line">&lt;pod-name&gt;   0/1       Init:1/2   0          7s</span><br><span class="line"></span><br><span class="line">例如，状态如果是 Init:1/2，则表明了两个初始化容器当中的一个已经成功执行：</span><br></pre></td></tr></table></figure><p>查看初始化容器的详情</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod &lt;pod-name&gt;</span><br></pre></td></tr></table></figure><h2 id="理解-Pod-状态"><a href="#理解-Pod-状态" class="headerlink" title="理解 Pod 状态"></a>理解 Pod 状态</h2><p>如果 Pod 的状态以 <code>Init:</code> 开头，表示该 Pod 正在执行初始化容器。下表描述了 Debug 初始化容器的过程中，一些可能出现的 Pod 状态：</p><table><thead><tr><th>状态</th><th>描述</th></tr></thead><tbody><tr><td><code>Init:N/M</code></td><td>Pod 中包含 M 个初始化容器，其中 N 个初始化容器已经成功执行</td></tr><tr><td><code>Init:Error</code></td><td>Pod 中有一个初始化容器执行失败</td></tr><tr><td><code>Init:CrashLoopBackOff</code></td><td>Pod 中有一个初始化容器反复执行失败</td></tr><tr><td><code>Pending</code></td><td>Pod 还未开始执行初始化容器</td></tr><tr><td><code>PodInitializing</code> or <code>Running</code></td><td>Pod 已经完成初始化容器的执行</td></tr></tbody></table><h1 id="五、Pod-容器组-配置PodDisruptionBudget"><a href="#五、Pod-容器组-配置PodDisruptionBudget" class="headerlink" title="五、Pod 容器组_配置PodDisruptionBudget"></a>五、Pod 容器组_配置PodDisruptionBudget</h1><p>​    在Kubernetes中，为了保证业务不中断或业务SLA不降级，需要将应用进行集群化部署。通过PodDisruptionBudget控制器可以设置应用 Pod 集群处于运行状态最低个数，也可以设置应用Pod 集群处于运行状态的最低百分比，这样可以保证在主动销毁应用Pod的时候，不会一次性销毁太多的应用Pod，从而保证业务不中断或业务SLA不降级。</p><p>在Kubernetes 1.5中，kubectl drain命令已经支持了PodDisruptionBudget控制器，在进行kubectl drain操作时会根据PodDisruptionBudget控制器判断应用Pod集群数量，进而保证在业务不中断或业务SLA不降级的情况下进行应用Pod销毁。</p><h2 id="确定需要PDB保护的应用"><a href="#确定需要PDB保护的应用" class="headerlink" title="确定需要PDB保护的应用"></a>确定需要PDB保护的应用</h2><p>通常如下几种 Kubernetes 控制器创建的应用程序可以使用 PDB：</p><ul><li>Deployment</li><li>ReplicationController</li><li>ReplicaSet</li><li>StatefulSet</li></ul><p>PDB 中 <code>.spec.selector</code> 字段的内容必须与控制器中 <code>.spec.selector</code> 字段的内容相同。</p><h3 id="当毁坏发生时，在短时间内，应用程序最多可以容许多少个实例被终止？"><a href="#当毁坏发生时，在短时间内，应用程序最多可以容许多少个实例被终止？" class="headerlink" title="当毁坏发生时，在短时间内，应用程序最多可以容许多少个实例被终止？"></a>当毁坏发生时，在短时间内，应用程序最多可以容许多少个实例被终止？</h3><ul><li>无状态的前端：<ul><li>关注点：不能让服务能力（serving capacity）降低超过 10%</li><li>解决方案：在 PDB 中配置 minAvailable 90%</li></ul></li><li>单实例有状态应用：<ul><li>关注点：未经同意不能关闭此应用程序</li><li>解决方案1： 不使用 PDB，并且容忍偶尔的停机</li><li>解决方案2： 在 PDB 中设置 maxUnavailable=0。与集群管理员达成一致（不是通过Kubernetes，而是邮件、电话或面对面），请集群管理员在终止应用之前与你沟通。当集群管理员联系你时，准备好停机时间，删除 PDB 以表示已准备好应对毁坏。并做后续处理</li></ul></li><li>多实例有状态应用，例如 consul、zookeeper、etcd：<ul><li>关注点：不能将实例数降低到某个数值，否则写入会失败</li><li>解决方案1： 在 PDB 中设置 maxUnavailable 为 1 （如果副本数会发生变化，可以使用此设置）</li><li>解决方案2： 在 PDB 中设置 minAvailable 为最低数量（例如，当总副本数为 5 时，设置为3）（可以同时容忍更多的毁坏数）</li></ul></li><li>可以重新开始的批处理任务：<ul><li>关注点：当发生自愿毁坏时，Job仍然需要完成其执行任务</li><li>解决方案： 不创建 PDB。Job 控制器将会创建一个 Pod 用于替换被毁坏的 Pod</li></ul></li></ul><h3 id="指定百分比时的舍入逻辑"><a href="#指定百分比时的舍入逻辑" class="headerlink" title="指定百分比时的舍入逻辑"></a>指定百分比时的舍入逻辑</h3><p><code>minAvailable</code> 或 <code>maxUnavailable</code> 可以指定为整数或者百分比。</p><ul><li>当指定一个整数时，代表 Pod 的数量。例如，设置 <code>minAvailable</code> 为 10，则至少 10 个 Pod 必须始终可用，即便是在毁坏发生时</li><li>当指定一个百分比时（例如，<code>50%</code>），代表总 Pod 数量的一个百分比。例如，设置 <code>maxUnavailable</code> 为 <code>50%</code>，则最多可以有 50% 的 Pod 可以被毁坏</li></ul><p>如果指定这些值为一个百分数，其计算结果可能不会正好是一个整数。例如，假设有 7 个 Pod，<code>minAvailable</code> 设置为 <code>50%</code>，你将很难判断，到底是 3 个还是 4 个 Pod 必须始终保持可用。Kubernetes 将向上舍入（round up to the nearest integer），因此，此处必须有 4 个 Pod 始终可用。</p><h3 id="定义PodDisruptionBudget"><a href="#定义PodDisruptionBudget" class="headerlink" title="定义PodDisruptionBudget"></a>定义PodDisruptionBudget</h3><p><code>PodDisruptionBudget</code> 包含三个字段：</p><ul><li>标签选择器 <code>.spec.selector</code> 用于指定 PDB 适用的 Pod。此字段为必填</li><li><code>.spec.minAvailable</code>：当完成驱逐时，最少仍然要保留多少个 Pod 可用。该字段可以是一个整数，也可以是一个百分比</li><li><code>.spec.maxUnavailable</code>： 当完成驱逐时，最多可以有多少个 Pod 被终止。该字段可以是一个整数，也可以是一个百分比</li></ul><p>在一个 <code>PodDisruptionBudget</code> 中，只能指定 <code>maxUnavailable</code> 和 <code>minAvailable</code> 中的一个。 <code>maxUnavailable</code> 只能应用到那些有控制器的 Pod 上。下面的例子中，“期望的副本数” 是 PodDisruptionBudget 对应 Pod 的控制器的 <code>.spec.replicas</code> 字段：</p><p>例子1： <code>minAvailable</code> 为 5 时，只要 PodDisruptionBudget 的 <code>selector</code> 匹配的 Pod 中有超过 5 个仍然可用，就可以继续驱逐 Pod </p><p>例子2： <code>minAvailable</code> 为 30% 时，至少保证期望副本数的 30% 可用 </p><p>例子3： <code>maxUnavailable</code> 为 5 时，最多可以有 5 个副本不可用（unthealthy）</p><p>例子4： <code>maxUnavailable</code> 为 30% 时，最多可以有期望副本数的 30% 不可用</p><p>通常，一个 PDB 对应一个控制器创建的 Pod，例如，Deployment、ReplicaSet或StatefulSet。</p><h3 id="使用-minAvailable"><a href="#使用-minAvailable" class="headerlink" title="使用 minAvailable"></a>使用 minAvailable</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodDisruptionBudget</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">zk-pdb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">minAvailable:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">zookeeper</span></span><br><span class="line">      </span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="string">k8s-yamls]#</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span>  <span class="string">zk-pdb.yaml</span> </span><br><span class="line"><span class="string">poddisruptionbudget.policy/zk-pdb</span> <span class="string">created</span></span><br><span class="line"></span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="string">k8s-yamls]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">poddisruptionbudgets</span></span><br><span class="line"><span class="string">NAME</span>     <span class="string">MIN</span> <span class="string">AVAILABLE</span>   <span class="string">MAX</span> <span class="string">UNAVAILABLE</span>   <span class="string">ALLOWED</span> <span class="string">DISRUPTIONS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">zk-pdb</span>   <span class="number">2</span>               <span class="string">N/A</span>               <span class="number">0</span>                     <span class="string">26s</span></span><br></pre></td></tr></table></figure><h3 id="使用-maxUnavailable"><a href="#使用-maxUnavailable" class="headerlink" title="使用 maxUnavailable"></a>使用 maxUnavailable</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodDisruptionBudget</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">zk-pdb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">maxUnavailable:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">zookeeper</span></span><br><span class="line">      </span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="string">k8s-yamls]#</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">max_zk-pdb.yaml</span> </span><br><span class="line"><span class="string">poddisruptionbudget.policy/zk-pdb</span> <span class="string">configured</span></span><br><span class="line"></span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="string">k8s-yamls]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">poddisruptionbudgets</span></span><br><span class="line"><span class="string">NAME</span>     <span class="string">MIN</span> <span class="string">AVAILABLE</span>   <span class="string">MAX</span> <span class="string">UNAVAILABLE</span>   <span class="string">ALLOWED</span> <span class="string">DISRUPTIONS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">zk-pdb</span>   <span class="string">N/A</span>             <span class="number">1</span>                 <span class="number">0</span>                     <span class="string">98s</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Pod的相关理解&lt;/p&gt;
    
    </summary>
    
    
      <category term="kubernetes" scheme="http://yoursite.com/categories/kubernetes/"/>
    
    
  </entry>
  
</feed>
