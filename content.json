{"meta":{"title":"拒绝再玩","subtitle":"","description":"","author":"duoyu","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"K8s-存储卷PersistentVolume","slug":"K8s-存储卷PersistentVolume","date":"2020-04-17T02:58:16.000Z","updated":"2020-04-22T07:38:27.320Z","comments":true,"path":"2020/04/17/K8s-存储卷PersistentVolume/","link":"","permalink":"http://yoursite.com/2020/04/17/K8s-%E5%AD%98%E5%82%A8%E5%8D%B7PersistentVolume/","excerpt":"PV &amp; PVC","text":"PV &amp; PVC 存储卷PersistentVolumePersistentVolume（PV 存储卷）是集群中的一块存储空间，由集群管理员管理、或者由 Storage Class（存储类）自动管理。PV（存储卷）和 node（节点）一样，是集群中的资源（kubernetes 集群由存储资源和计算资源组成）。 PersistentVolumeClaim（存储卷声明）是一种类型的 Volume（数据卷），PersistentVolumeClaim（存储卷声明）引用的 PersistentVolume（存储卷）有自己的生命周期，该生命周期独立于任何使用它的容器组。PersistentVolume（存储卷）描述了如何提供存储的细节信息（NFS、cephfs等存储的具体参数）。 PersistentVolumeClaim（PVC 存储卷声明）代表用户使用存储的请求。Pod 容器组消耗 node 计算资源，PVC 存储卷声明消耗 PersistentVolume 存储资源。Pod 容器组可以请求特定数量的计算资源（CPU / 内存）；PersistentVolumeClaim 可以请求特定大小/特定访问模式（只能被单节点读写/可被多节点只读/可被多节点读写）的存储资源。 根据应用程序的特点不同，其所需要的存储资源也存在不同的要求，例如读写性能等。集群管理员必须能够提供关于 PersistentVolume（存储卷）的更多选择，无需用户关心存储卷背后的实现细节。为了解决这个问题，Kubernetes 引入了 StorageClass（存储类）的概念 存储卷和存储卷声明的关系存储卷和存储卷声明的关系如下图所示： PersistentVolume 是集群中的存储资源，通常由集群管理员创建和管理 StorageClass 用于对 PersistentVolume 进行分类，如果正确配置，StorageClass 也可以根据 PersistentVolumeClaim 的请求动态创建 Persistent Volume PersistentVolumeClaim 是使用该资源的请求，通常由应用程序提出请求，并指定对应的 StorageClass 和需求的空间大小 PersistentVolumeClaim 可以做为数据卷的一种，被挂载到容器组/容器中使用 存储卷声明的管理过程PersistantVolume 和 PersistantVolumeClaim 的管理过程描述如下： 下图主要描述的是 PV 和 PVC 的管理过程，因为绘制空间的问题，将挂载点与Pod关联了，实际结构应该如上图所示： Pod 中添加数据卷，数据卷关联PVC Pod 中包含容器，容器挂载数据卷 1.提供 Provisioning有两种方式为 PersistentVolumeClaim 提供 PersistentVolume : 静态、动态 静态提供 Static 集群管理员实现创建好一系列 PersistentVolume，它们包含了可供集群中应用程序使用的关于实际存储的具体信息。 动态提供 Dynamic 在配置有合适的 StorageClass（存储类）且 PersistentVolumeClaim 关联了该 StorageClass 的情况下，kubernetes 集群可以为应用程序动态创建 PersistentVolume。 2.绑定 Binding假设用户创建了一个 PersistentVolumeClaim 存储卷声明，并指定了需求的存储空间大小以及访问模式。Kubernets master 将立刻为其匹配一个 PersistentVolume 存储卷，并将存储卷声明和存储卷绑定到一起。如果一个 PersistentVolume 是动态提供给一个新的 PersistentVolumeClaim，Kubernetes master 会始终将其绑定到该 PersistentVolumeClaim。除此之外，应用程序将被绑定一个不小于（可能大于）其 PersistentVolumeClaim 中请求的存储空间大小的 PersistentVolume。一旦绑定，PersistentVolumeClaim 将拒绝其他 PersistentVolume 的绑定关系。PVC 与 PV 之间的绑定关系是一对一的映射。 PersistentVolumeClaim 将始终停留在 未绑定 unbound 状态，直到有合适的 PersistentVolume 可用。举个例子：集群中已经存在一个 50Gi 的 PersistentVolume，同时有一个 100Gi 的 PersistentVolumeClaim，在这种情况下，该 PVC 将一直处于 未绑定 unbound 状态，直到管理员向集群中添加了一个 100Gi 的 PersistentVolume。 3.使用 Using对于 Pod 容器组来说，PersistentVolumeClaim 存储卷声明是一种类型的 Volume 数据卷。Kubernetes 集群将 PersistentVolumeClaim 所绑定的 PersistentVolume 挂载到容器组供其使用。 4.使用中保护 Storage Object in Use Protection 使用中保护的目的是确保正在被容器组使用的 PersistentVolumeClaim 以及其绑定的 PersistentVolume 不能被系统删除，以避免可能的数据丢失。 如果用户删除一个正在使用中的 PersistentVolumeClaim，则该 PVC 不会立即被移除掉，而是推迟到该 PVC 不在被任何容器组使用时才移除；同样的如果管理员删除了一个已经绑定到 PVC 的 PersistentVolume，则该 PV 也不会立刻被移除掉，而是推迟到其绑定的 PVC 被删除后才移除掉 5.回收 Reclaiming当用户不在需要其数据卷时，可以删除掉其 PersistentVolumeClaim，此时其对应的 PersistentVolume 将被集群回收并再利用。Kubernetes 集群根据 PersistentVolume 中的 reclaim policy（回收策略）决定在其被回收时做对应的处理。当前支持的回收策略有：Retained（保留）、Recycled（重复利用）、Deleted（删除） 保留 Retain 保留策略需要集群管理员手工回收该资源。当绑定的 PersistentVolumeClaim 被删除后，PersistentVolume 仍然存在，并被认为是”已释放“。但是此时该存储卷仍然不能被其他 PersistentVolumeClaim 绑定，因为前一个绑定的 PersistentVolumeClaim 对应容器组的数据还在其中。集群管理员可以通过如下步骤回收该 PersistentVolume： 删除该 PersistentVolume。PV 删除后，其数据仍然存在于对应的外部存储介质中（nfs、cefpfs、glusterfs 等） 手工删除对应存储介质上的数据 手工删除对应的存储介质，您也可以创建一个新的 PersistentVolume 并再次使用该存储介质 删除 Delete 删除策略将从 kubernete 集群移除 PersistentVolume 以及其关联的外部存储介质（云环境中的 AWA EBS、GCE PD、Azure Disk 或 Cinder volume）。 再利用 Recycle 再利用策略将在 PersistentVolume 回收时，执行一个基本的清除操作（rm -rf /thevolume/*），并使其可以再次被新的 PersistentVolumeClaim 绑定。 集群管理员也可以自定义一个 recycler pod template，用于执行清除操作。 存储卷类型Kubernetes 支持 20 种存储卷类型，如下所示： 非持久性存储 emptyDir HostPath (只在单节点集群上用做测试目的) 网络连接性存储 SAN：iSCSI、ScaleIO Volumes、FC (Fibre Channel) NFS：nfs，cfs 分布式存储 Glusterfs RBD (Ceph Block Device) CephFS Portworx Volumes Quobyte Volumes 云端存储 GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk Cinder (OpenStack block storage) VsphereVolume StorageOS 自定义存储 FlexVolume 不推荐 Flocker","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"K8s-数据卷Volume","slug":"K8s-数据卷Volume","date":"2020-04-16T08:09:30.000Z","updated":"2020-04-17T02:55:30.993Z","comments":true,"path":"2020/04/16/K8s-数据卷Volume/","link":"","permalink":"http://yoursite.com/2020/04/16/K8s-%E6%95%B0%E6%8D%AE%E5%8D%B7Volume/","excerpt":"数据卷Volume及挂载","text":"数据卷Volume及挂载 一、数据卷Volume数据卷概述Kubernetes Volume（数据卷）主要解决了如下两方面问题： 数据持久性：通常情况下，容器运行起来之后，写入到其文件系统的文件暂时性的。当容器崩溃后，kubelet 将会重启该容器，此时原容器运行后写入的文件将丢失，因为容器将重新从镜像创建。 数据共享：同一个 Pod（容器组）中运行的容器之间，经常会存在共享文件/文件夹的需求 Docker 里同样也存在一个 volume（数据卷）的概念，但是 docker 对数据卷的管理相对 kubernetes 而言要更少一些。在 Docker 里，一个 Volume（数据卷）仅仅是宿主机（或另一个容器）文件系统上的一个文件夹。Docker 并不管理 Volume（数据卷）的生命周期。 在 Kubernetes 里，Volume（数据卷）存在明确的生命周期（与包含该数据卷的容器组相同）。因此，Volume（数据卷）的生命周期比同一容器组中任意容器的生命周期要更长，不管容器重启了多少次，数据都能被保留下来。当然，如果容器组退出了，数据卷也就自然退出了。此时，根据容器组所使用的 Volume（数据卷）类型不同，数据可能随数据卷的退出而删除，也可能被真正持久化，并在下次容器组重启时仍然可以使用。 从根本上来说，一个 Volume（数据卷）仅仅是一个可被容器组中的容器访问的文件目录（也许其中包含一些数据文件）。这个目录是怎么来的，取决于该数据卷的类型（不同类型的数据卷使用不同的存储介质）。 使用 Volume（数据卷）时，我们需要先在容器组中定义一个数据卷，并将其挂载到容器的挂载点上。容器中的一个进程所看到（可访问）的文件系统是由容器的 docker 镜像和容器所挂载的数据卷共同组成的。Docker 镜像将被首先加载到该容器的文件系统，任何数据卷都被在此之后挂载到指定的路径上。Volume（数据卷）不能被挂载到其他数据卷上，或者通过引用其他数据卷。同一个容器组中的不同容器各自独立地挂载数据卷，即同一个容器组中的两个容器可以将同一个数据卷挂载到各自不同的路径上。 我们现在通过下图来理解 容器组、容器、挂载点、数据卷、存储介质（nfs、PVC、ConfigMap）等几个概念之间的关系： 一个容器组可以包含多个数据卷、多个容器 一个容器通过挂载点决定某一个数据卷被挂载到容器中的什么路径 不同类型的数据卷对应不同的存储介质 数据卷的常用类型emptyDir 描述 emptyDir类型的数据卷在容器组被创建时分配给该容器组，并且直到容器组被移除，该数据卷才被释放。该数据卷初始分配时，始终是一个空目录。同一容器组中的不同容器都可以对该目录执行读写操作，并且共享其中的数据，（尽管不同的容器可能将该数据卷挂载到容器中的不同路径）。当容器组被移除时，emptyDir数据卷中的数据将被永久删除 容器崩溃时，kubelet 并不会删除容器组，而仅仅是将容器重启，因此 emptyDir 中的数据在容器崩溃并重启后，仍然是存在的。 适用场景 空白的初始空间，例如合并/排序算法中，临时将数据存在磁盘上 长时间计算中存储检查点（中间结果），以便容器崩溃时，可以从上一次存储的检查点（中间结果）继续进行，而不是从头开始 作为两个容器的共享存储，使得第一个内容管理的容器可以将生成的页面存入其中，同时由一个 webserver 容器对外提供这些页面 默认情况下，emptyDir 数据卷被存储在 node（节点）的存储介质（机械硬盘、SSD、或者网络存储）上。此外，您可以设置 emptyDir.medium 字段为 “Memory”，此时 Kubernetes 将挂载一个 tmpfs（基于 RAM 的文件系统）。tmpfs 的读写速度非常快，但是与磁盘不一样，tmpfs 在节点重启后将被清空，且您向该 emptyDir 写入文件时，将消耗对应容器的内存限制。 nfs 描述 nfs 类型的数据卷可以加载 NFS（Network File System）到容器组/容器。容器组被移除时，将仅仅 umount（卸载）NFS 数据卷，NFS 中的数据仍将被保留。 可以在加载 NFS 数据卷前就在其中准备好数据； 可以在不同容器组之间共享数据； 可以被多个容器组加载并同时读写； 适用场景 存储日志文件 MySQL的data目录（建议只在测试环境中） 用户上传的临时文件 cephfs 描述 cephfs 数据卷可以挂载一个外部 CephFS 卷到容器组中。对于 kubernetes 而言，cephfs 与 nfs 的管理方式和行为完全相似，适用场景也相同。不同的仅仅是背后的存储介质。 适用场景 同 nfs 数据卷 configMap 描述 ConfigMap 提供了一种向容器组注入配置信息的途径。ConfigMap 中的数据可以被 Pod（容器组）中的容器作为一个数据卷挂载。 在数据卷中引用 ConfigMap 时： 可以直接引用整个 ConfigMap 到数据卷，此时 ConfigMap 中的每一个 key 对应一个文件名，value 对应该文件的内容 也可以只引用 ConfigMap 中的某一个名值对，此时可以将 key 映射成一个新的文件名 将 ConfigMap 数据卷挂载到容器时，如果该挂载点指定了 数据卷内子路径 （subPath），则该 ConfigMap 被改变后，该容器挂载的内容仍然不变。 适用场景 使用 ConfigMap 中的某一 key 作为文件名，对应 value 作为文件内容，替换 nginx 容器中的 /etc/nginx/conf.d/default.conf 配置文件。 secret 描述 secret 数据卷可以用来注入敏感信息（例如密码）到容器组。可以将敏感信息存入 kubernetes secret 对象，并通过 Volume（数据卷）以文件的形式挂载到容器组（或容器）。secret 数据卷使用 tmpfs（基于 RAM 的文件系统）挂载。 将 Secret 数据卷挂载到容器时，如果该挂载点指定了 数据卷内子路径 （subPath），则该 Secret 被改变后，该容器挂载的内容仍然不变。 适用场景 将 HTTPS 证书存入 kubernets secret，并挂载到 /etc/nginx/conf.d/myhost.crt、/etc/nginx/conf.d/myhost.pem 路径，用来配置 nginx 的 HTTPS 证书 PersistentVolumeClaim 描述 persistentVolumeClaim 数据卷用来挂载 PersistentVolume 存储卷。 PersistentVolume 存储卷为用户提供了一种在无需关心具体所在云环境的情况下”声明“ 所需持久化存储的方式。 二、数据卷-挂载挂载是指将定义在 Pod 中的数据卷关联到容器，同一个 Pod 中的同一个数据卷可以被挂载到该 Pod 中的多个容器上。 数据卷内子路径同一个 Pod 的不同容器间共享数据卷。使用 volumeMounts.subPath 属性，可以使容器在挂载数据卷时指向数据卷内部的一个子路径，而不是直接指向数据卷的根路径。 下面的例子中，一个 LAMP（Linux Apache Mysql PHP）应用的 Pod 使用了一个共享数据卷，HTML 内容映射到数据卷的 html 目录，数据库的内容映射到了 mysql 目录： 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: name: my-lamp-sitespec: containers: - name: mysql image: mysql env: - name: MYSQL_ROOT_PASSWORD value: \"rootpasswd\" volumeMounts: - mountPath: /var/lib/mysql name: site-data subPath: mysql readOnly: false - name: php image: php:7.0-apache volumeMounts: - mountPath: /var/www/html name: site-data subPath: html readOnly: false volumes: - name: site-data persistentVolumeClaim: claimName: my-lamp-site-data 通过环境变量指定数据卷内子路径使用 volumeMounts.subPathExpr 字段，可以通过容器的环境变量指定容器内路径。使用此特性时，必须启用 VolumeSubpathEnvExpansion（自 Kubernetes v1.15 开始，是默认启用的。） 如下面的例子，该 Pod 使用 subPathExpr 在 hostPath 数据卷 /var/log/pods 中创建了一个目录 pod1（该参数来自于Pod的名字）。此时，宿主机目录 /var/log/pods/pod1 挂载到了容器的 /logs 路径： 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: pod1spec: containers: - name: container1 env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name image: busybox command: [ \"sh\", \"-c\", \"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt\" ] volumeMounts: - name: workdir1 mountPath: /logs subPathExpr: $(POD_NAME) readOnly: false restartPolicy: Never volumes: - name: workdir1 hostPath: path: /var/log/pods 容器内路径mountPath 数据卷被挂载到容器的路径，不能包含 : 权限容器对挂载的数据卷是否具备读写权限，如果 readOnly 为 true，则只读，否则可以读写（为 false 或者不指定）。默认为 false","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"Kubernetes网络模型","slug":"Kubernetes网络模型","date":"2020-04-14T06:55:20.000Z","updated":"2020-04-16T07:54:50.876Z","comments":true,"path":"2020/04/14/Kubernetes网络模型/","link":"","permalink":"http://yoursite.com/2020/04/14/Kubernetes%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","excerpt":"Container-to-Container 的网络 Pod-to-Pod 的网络 Pod-to-Service 的网络 Internet-to-Service 的网络","text":"Container-to-Container 的网络 Pod-to-Pod 的网络 Pod-to-Service 的网络 Internet-to-Service 的网络 Kubernetes网络模型一、Kubernetes基本概念Kubernetes 基于少数几个核心概念，不断完善，提供了非常丰富和实用的功能。本章节罗列了这些核心概念，并简要的做了概述，以便更好地支持后面的讨论。熟悉 Kubernetes 的读者可跳过这个章节。 Kubernetes API Server操作 Kubernetes 的方式，是调用 Kubernetes API Server（kube-apiserver）的 API 接口。kubectl、kubernetes dashboard、kuboard 都是通过调用 kube-apiserver 的接口实现对 kubernetes 的管理。API server 最终将集群状态的数据存储在 etcd 中。 控制器Controller控制器（Controller）是 Kubernetes 中最核心的抽象概念。在用户通过 kube-apiserver 声明了期望的状态以后，控制器通过不断监控 apiserver 中的当前状态，并对当前状态与期望状态之间的差异做出反应，以确保集群的当前状态不断地接近用户声明的期望状态。这个过程实现在一个循环中，参考如下伪代码： 12345678while true: X = currentState() Y = desiredState() if X == Y: return # Do nothing else: do(tasks to get to Y) 例如，当你通过 API Server 创建一个新的 Pod 对象时，Kubernetes调度器（是一个控制器）注意到此变化，并做出将该 Pod 运行在集群中哪个节点的决定。然后，通过 API Server 修改 Pod 对象的状态。此时，对应节点上的kubelet（是一个控制器）注意到此变化，并将在其所在节点运行该 Pod，设置需要的网络，使 Pod 在集群内可以访问。此处，两个控制器针对不同的状态变化做出反应，以使集群的当前状态与用户指定的期望状态匹配。 容器组PodPod 是 Kubernetes 中的最小可部署单元。一个 Pod 代表了集群中运行的一个工作负载，可以包括一个或多个 docker 容器、挂载需要的存储，并拥有唯一的 IP 地址。Pod 中的多个容器将始终在同一个节点上运行。 节点Node节点是Kubernetes集群中的一台机器，可以是物理机，也可以是虚拟机。 二、Kubernetes网络模型关于 Pod 如何接入网络这件事情，Kubernetes 做出了明确的选择。具体来说，Kubernetes 要求所有的网络插件实现必须满足如下要求： 所有的 Pod 可以与任何其他 Pod 直接通信，无需使用 NAT 映射（network address translation） 所有节点可以与所有 Pod 直接通信，无需使用 NAT 映射 Pod 内部获取到的 IP 地址与其他 Pod 或节点与其通信时的 IP 地址是同一个 在这些限制条件下，需要解决如下四种完全不同的网络使用场景的问题： Container-to-Container 的网络 Pod-to-Pod 的网络 Pod-to-Service 的网络 Internet-to-Service 的网络 三、Container-to-Container的网络Linux系统中，每一个进程都在一个 network namespace 中进行通信，network namespace 提供了一个逻辑上的网络堆栈（包含自己的路由、防火墙规则、网络设备）。换句话说，network namespace 为其中的所有进程提供了一个全新的网络堆栈。 Linux 用户可以使用 ip 命令创建 network namespace。 例如，下面的命令创建了一个新的 network namespace 名称为 ns1： 1$ ip netns add ns1 当创建 network namespace 时，同时将在 /var/run/netns 下创建一个挂载点（mount point）用于存储该 namespace 的信息。 执行 ls /var/run/netns 命令，或执行 ip 命令，可以查看所有的 network namespace： 1234$ ls /var/run/netnsns1$ ip netnsns1 默认情况下，Linux 将所有的进程都分配到 root network namespace，以使得进程可以访问外部网络，如下图所示： 在 Kubernetes 中，Pod 是一组 docker 容器的集合，这一组 docker 容器将共享一个 network namespace。Pod 中所有的容器都： 使用该 network namespace 提供的同一个 IP 地址以及同一个端口空间 可以通过 localhost 直接与同一个 Pod 中的另一个容器通信 Kubernetes 为每一个 Pod 都创建了一个 network namespace。具体做法是，把一个 Docker 容器当做 “Pod Container” 用来获取 network namespace，在创建 Pod 中新的容器时，都使用 docker run 的 --network:container 功能来加入该 network namespace。 如下图所示，每一个 Pod 都包含了多个 docker 容器（ctr*），这些容器都在同一个共享的 network namespace 中： 此外，Pod 中可以定义数据卷，Pod 中的容器都可以共享这些数据卷，并通过挂载点挂载到容器内部不同的路径。 四、Pod-to-Pod的网络在 Kubernetes 中，每一个 Pod 都有一个真实的 IP 地址，并且每一个 Pod 都可以使用此 IP 地址与 其他 Pod 通信。Pod-to-Pod 通信中使用真实 IP ，不管两个 Pod 是在同一个节点上，还是集群中的不同节点上。 从 Pod 的视角来看，Pod 是在其自身所在的 network namespace 与同节点上另外一个 network namespace 进程通信。在Linux上，不同的 network namespace 可以通过 Virtual Ethernet Device 或 veth pair (两块跨多个名称空间的虚拟网卡)进行通信。为连接 pod 的 network namespace，可以将 veth pair 的一段指定到 root network namespace，另一端指定到 Pod 的 network namespace。每一组 veth pair 类似于一条网线，连接两端，并可以使流量通过。节点上有多少个 Pod，就会设置多少组 veth pair。下图展示了 veth pair 连接 Pod 到 root namespace 的情况： 此时，每个Pod 都有了自己的 network namespace，从 Pod 的角度来看，他们都有自己的以太网卡以及 IP 地址，并且都连接到了节点的 root network namespace。为了让 Pod 可以互相通过 root network namespace 通信，通过使用 network bridge（网桥）。 Linux Ethernet bridge 是一个虚拟的 Layer 2 网络设备，可用来连接两个或多个网段（network segment）。网桥的工作原理是，在源于目标之间维护一个转发表（forwarding table），通过检查通过网桥的数据包的目标地址（destination）和该转发表来决定是否将数据包转发到与网桥相连的另一个网段。桥接代码通过网络中具备唯一性的网卡MAC地址来判断是否桥接或丢弃数据。 网桥实现了 ARP 协议，以发现链路层与 IP 地址绑定的 MAC 地址。当网桥收到数据帧时，网桥将该数据帧广播到所有连接的设备上（除了发送者以外），对该数据帧做出相应的设备被记录到一个查找表中（lookup table）。后续网桥再收到发向同一个 IP 地址的流量时，将使用查找表（lookup table）来找到对应的 MAC 地址，并转发数据包。 数据包的传递：Pod-to-Pod，同节点在 network namespace 将每一个 Pod 隔离到各自的网络堆栈的情况下，虚拟以太网设备（virtual Ethernet device）将每一个 namespace 连接到 root namespace，网桥将 namespace 又连接到一起，此时，Pod 可以向同一节点上的另一个 Pod 发送网络报文了。下图演示了同节点上，网络报文从一个Pod传递到另一个Pod的情况。 Pod1 发送一个数据包到其自己的默认以太网设备 eth0。 对 Pod1 来说，eth0 通过虚拟以太网设备（veth0）连接到 root namespace 网桥 cbr0 中为 veth0 配置了一个网段。一旦数据包到达网桥，网桥使用 ARP 协议解析出其正确的目标网段 veth1 网桥 cbr0 将数据包发送到 veth1 数据包到达 veth1 时，被直接转发到 Pod2 的 network namespace 中的 eth0 网络设备。 在整个数据包传递过程中，每一个 Pod 都只和 localhost 上的 eth0 通信，且数包被路由到正确的 Pod 上。 Kubernetes 的网络模型规定，在跨节点的情况下 Pod 也必须可以通过 IP 地址访问。也就是说，Pod 的 IP 地址必须始终对集群中其他 Pod 可见；且从 Pod 内部和从 Pod 外部来看，Pod 的IP地址都是相同的。 数据包的传递：Pod-to-Pod，跨节点Kubernetes 网络模型要求 Pod 的 IP 在整个网络中都可访问，但是并不指定如何实现这一点。实际上，这是所使用网络插件相关的，但是，仍然有一些模式已经被确立了。 通常，集群中每个节点都被分配了一个 CIDR 网段，指定了该节点上的 Pod 可用的 IP 地址段。一旦发送到该 CIDR 网段的流量到达节点，就由节点负责将流量继续转发给对应的 Pod。下图展示了两个节点之间的数据报文传递过程。 图中，目标 Pod（以绿色高亮）与源 Pod（以蓝色高亮）在不同的节点上，数据包传递过程如下： 数据包从 Pod1 的网络设备 eth0，该设备通过 veth0 连接到 root namespace 数据包到达 root namespace 中的网桥 cbr0 网桥上执行 ARP 将会失败，因为与网桥连接的所有设备中，没有与该数据包匹配的 MAC 地址。一旦 ARP 失败，网桥会将数据包发送到默认路由（root namespace 中的 eth0 设备）。此时，数据包离开节点进入网络 假设网络可以根据各节点的CIDR网段，将数据包路由到正确的节点 数据包进入目标节点的 root namespace（VM2 上的 eth0）后，通过网桥路由到正确的虚拟网络设备（veth1） 最终，数据包通过 veth1 发送到对应 Pod 的 eth0，完成了数据包传递的过程 通常来说，每个节点知道如何将数据包分发到运行在该节点上的 Pod。一旦一个数据包到达目标节点，数据包的传递方式与同节点上不同Pod之间数据包传递的方式就是一样的了。 此处，我们直接跳过了如何配置网络，以使得数据包可以从一个节点路由到匹配的节点。这些是与具体的网络插件实现相关的。 Container Network Interface(CNI) plugin 提供了一组通用 API 用来连接容器与外部网络。具体到容器化应用开发者来说，只需要了解在整个集群中，可以通过 Pod 的 IP 地址直接访问 Pod；网络插件是如何做到跨节点的数据包传递这件事情对容器化应用来说是透明的。 五、Pod-to-Service的网络Pod 可以通过 IP 地址之间传递数据包，但是，Pod 的 IP 地址并非是固定不变的，随着 Pod 的重新调度（例如水平伸缩、应用程序崩溃、节点重启等），Pod 的 IP 地址将会出现又消失。此时，Pod 的客户端无法得知该访问哪一个 IP 地址。Kubernetes 中，Service 的概念用于解决此问题。 一个 Kubernetes Service 管理了一组 Pod 的状态，可以追踪一组 Pod 的 IP 地址的动态变化过程。一个 Service 拥有一个 IP 地址，并且充当了一组 Pod 的 IP 地址的“虚拟 IP 地址”。任何发送到 Service 的 IP 地址的数据包将被负载均衡到该 Service 对应的 Pod 上。在此情况下，Service 关联的 Pod 可以随时间动态变化，客户端只需要知道 Service 的 IP 地址即可（该地址不会发生变化）。 从效果上来说，Kubernetes 自动为 Service 创建和维护了集群内部的分布式负载均衡，可以将发送到 Service IP 地址的数据包分发到 Service 对应的健康的 Pod 上。 netfilter and iptablesKubernetes 利用 Linux 内建的网络框架 - netfilter 来实现负载均衡。Netfilter 是由 Linux 提供的一个框架，可以通过自定义 handler 的方式来实现多种网络相关的操作。Netfilter 提供了许多用于数据包过滤、网络地址转换、端口转换的功能，通过这些功能，自定义的 handler 可以在网络上转发数据包、禁止数据包发送到敏感的地址等。 iptables 是一个 user-space 应用程序，可以提供基于决策表的规则系统，以使用 netfilter 操作或转换数据包。在 Kubernetes 中，kube-proxy 控制器监听 apiserver 中的变化，并配置 iptables 规则。当 Service 或 Pod 发生变化时（例如 Service 被分配了 IP 地址，或者新的 Pod 被关联到 Service），kube-proxy 控制器将更新 iptables 规则，以便将发送到 Service 的数据包正确地路由到其后端 Pod 上。iptables 规则将监听所有发向 Service 的虚拟 IP 的数据包，并将这些数据包转发到该Service 对应的一个随机的可用 Pod 的 IP 地址，同时 iptables 规则将修改数据包的目标 IP 地址（从 Service 的 IP 地址修改为选中的 Pod 的 IP 地址）。当 Pod 被创建或者被终止时，iptables 的规则也被对应的修改。换句话说，iptables 承担了从 Service IP 地址到实际 Pod IP 地址的负载均衡的工作。 在返回数据包的路径上，数据包从目标 Pod 发出，此时，iptables 规则又将数据包的 IP 头从 Pod 的 IP 地址替换为 Service 的 IP 地址。从请求的发起方来看，就好像始终只是在和 Service 的 IP 地址通信一样。 IPVSKubernetes v1.11 开始，提供了另一个选择用来实现集群内部的负载均衡：IPVS。 IPVS（IP Virtual Server）也是基于 netfilter 构建的，在 Linux 内核中实现了传输层的负载均衡。 IPVS 被合并到 LVS（Linux Virtual Server）当中，充当一组服务器的负载均衡器。 IPVS 可以转发 TCP / UDP 请求到实际的服务器上，使得一组实际的服务器看起来像是只通过一个单一 IP 地址访问的服务一样。IPVS 的这个特点天然适合与用在 Kubernetes Service 的这个场景下。 当声明一个 Kubernetes Service 时，可以指定是使用 iptables 还是 IPVS 来提供集群内的负载均衡工作。 IPVS 是转为负载均衡设计的，并且使用更加有效率的数据结构（hash tables），相较于 iptables，可以支持更大数量的网络规模。当创建使用 IPVS 形式的 Service 时，Kubernetes 执行了如下三个操作： 在节点上创建一个 dummy IPVS interface 将 Service 的 IP 地址绑定到该 dummy IPVS interface 为每一个 Service IP 地址创建 IPVS 服务器 将来，IPVS 有可能成为 kubernetes 中默认的集群内负载均衡方式。这个改变将只影响到集群内的负载均衡，以 iptables 为例子，所有讨论对 IPVS 是同样适用。 数据包的传递：Pod-to-Service 在 Pod 和 Service 之间路由数据包时，数据包的发起和以前一样： 数据包首先通过 Pod 的 eth0 网卡发出 数据包经过虚拟网卡 veth0 到达网桥 cbr0 网桥上的 APR 协议查找不到该 Service，所以数据包被发送到 root namespace 中的默认路由 - eth0 此时，在数据包被 eth0 接受之前，数据包将通过 iptables 过滤。iptables 使用其规则（由 kube-proxy 根据 Service、Pod 的变化在节点上创建的 iptables 规则）重写数据包的目标地址（从 Service 的 IP 地址修改为某一个具体 Pod 的 IP 地址） 数据包现在的目标地址是 Pod 4，而不是 Service 的虚拟 IP 地址。iptables 使用 Linux 内核的 conntrack 工具包来记录具体选择了哪一个 Pod，以便可以将未来的数据包路由到同一个 Pod。简而言之，iptables 直接在节点上完成了集群内负载均衡的功能。数据包后续如何发送到 Pod 上，其路由方式与Pod-to-Pod的网络中相同。 数据包的传递：Service-to-Pod 接收到此请求的 Pod 将会发送返回数据包，其中标记源 IP 为接收请求 Pod 自己的 IP，目标 IP 为最初发送对应请求的 Pod 的 IP 当数据包进入节点后，数据包将经过 iptables 的过滤，此时记录在 conntrack 中的信息将被用来修改数据包的源地址（从接收请求的 Pod 的 IP 地址修改为 Service 的 IP 地址） 然后，数据包将通过网桥、以及虚拟网卡 veth0 最终到达 Pod 的网卡 eth0 使用DNSKubernetes 也可以使用 DNS，以避免将 Service 的 cluster IP 地址硬编码到应用程序当中。Kubernetes DNS 是 Kubernetes 上运行的一个普通的 Service。每一个节点上的 kubelet 都使用该 DNS Service 来执行 DNS 名称的解析。集群中每一个 Service（包括 DNS Service 自己）都被分配了一个 DNS 名称。DNS 记录将 DNS 名称解析到 Service 的 ClusterIP 或者 Pod 的 IP 地址。SRV 记录用来指定 Service 的已命名端口。 DNS Pod 由三个不同的容器组成： kubedns：观察 Kubernetes master 上 Service 和 Endpoints 的变化，并维护内存中的 DNS 查找表 dnsmasq：添加 DNS 缓存，以提高性能 sidecar：提供一个健康检查端点，可以检查 dnsmasq 和 kubedns 的健康状态 DNS Pod 被暴露为 Kubernetes 中的一个 Service，该 Service 及其 ClusterIP 在每一个容器启动时都被传递到容器中（环境变量及 /etc/resolves），因此，每一个容器都可以正确的解析 DNS。DNS 条目最终由 kubedns 解析，kubedns 将 DNS 的所有信息都维护在内存中。etcd 中存储了集群的所有状态，kubedns 在必要的时候将 etcd 中的 key-value 信息转化为 DNS 条目信息，以重建内存中的 DNS 查找表。 CoreDNS 的工作方式与 kubedns 类似，但是通过插件化的架构构建，因而灵活性更强。自 Kubernetes v1.11 开始，CoreDNS 是 Kubernetes 中默认的 DNS 实现。 六、Internet-to-Service的网络 从集群内部访问互联网 从互联网访问集群内部 出方向 - 从集群内部访问互联网将网络流量从集群内的一个节点路由到公共网络是与具体网络以及实际网络配置紧密相关的。为了更加具体地讨论此问题，本文将使用 AWS VPC 来讨论其中的具体问题。 在 AWS，Kubernetes 集群在 VPC 内运行，在此处，每一个节点都被分配了一个内网地址（private IP address）可以从 Kubernetes 集群内部访问。为了使访问外部网络，通常会在 VPC 中添加互联网网关（Internet Gateway），以实现如下两个目的： 作为 VPC 路由表中访问外网的目标地址 提供网络地址转换（NAT Network Address Translation），将节点的内网地址映射到一个外网地址，以使外网可以访问内网上的节点 在有互联网网关（Internet Gateway）的情况下，虚拟机可以任意访问互联网。但是，存在一个小问题：Pod 有自己的 IP 地址，且该 IP 地址与其所在节点的 IP 地址不一样，并且，互联网网关上的 NAT 地址映射只能够转换节点（虚拟机）的 IP 地址，因为网关不知道每个节点（虚拟机）上运行了哪些 Pod （互联网网关不知道 Pod 的存在）。那么 Kubernetes 是如何使用 iptables 解决此问题的。 数据包的传递：Node-to-Internet下图中： 数据包从 Pod 的 network namespace 发出 通过 veth0 到达虚拟机的 root network namespace 由于网桥上找不到数据包目标地址对应的网段，数据包将被网桥转发到 root network namespace 的网卡 eth0。在数据包到达 eth0 之前，iptables 将过滤该数据包。 在此处，数据包的源地址是一个 Pod，如果仍然使用此源地址，互联网网关将拒绝此数据包，因为其 NAT 只能识别与节点（虚拟机）相连的 IP 地址。因此，需要 iptables 执行源地址转换（source NAT），这样子，对互联网网关来说，该数据包就是从节点（虚拟机）发出的，而不是从 Pod 发出的 数据包从节点（虚拟机）发送到互联网网关 互联网网关再次执行源地址转换（source NAT），将数据包的源地址从节点（虚拟机）的内网地址修改为网关的外网地址，最终数据包被发送到互联网 在回路径上，数据包沿着相同的路径反向传递，源地址转换（source NAT）在对应的层级上被逆向执行。 入方向 - 从互联网访问Kubernetes Service LoadBalancer Ingress Controller 4 层：LoadBalancer当创建 Kubernetes Service 时，可以指定其类型为 LoadBalancer。 LoadBalancer 的实现由 cloud controller 提供，cloud controller 可以调用云供应商 IaaS 层的接口，为 Kubernetes Service 创建负载均衡器（如果是自建 Kubernetes 集群，可以使用 NodePort 类型的 Service，并手动创建负载均衡器）。用户可以将请求发送到负载均衡器来访问 Kubernetes 中的 Service。 在 AWS，负载均衡器可以将网络流量分发到其目标服务器组（即 Kubernetes 集群中的所有节点）。一旦数据包到达节点，Service 的 iptables 规则将确保其被转发到 Service 的一个后端 Pod。 数据包的传递：LoadBalancer-to-Service接下来了解一下 Layer 4 的入方向访问具体是如何做到的： Loadbalancer 类型的 Service 创建后，cloud controller 将为其创建一个负载均衡器 负载均衡器只能直接和节点（虚拟机沟通），不知道 Pod 的存在，当数据包从请求方（互联网）到达 LoadBalancer 之后，将被分发到集群的节点上 节点上的 iptables 规则将数据包转发到合适的 Pod 上 从 Pod 到请求方的相应数据包将包含 Pod 的 IP 地址，但是请求方需要的是负载均衡器的 IP 地址。iptables 和 conntrack 被用来重写返回路径上的正确的 IP 地址。 下图描述了一个负载均衡器和三个集群节点： 请求数据包从互联网发送到负载均衡器 负载均衡器将数据包随机分发到其中的一个节点（虚拟机），此处，我们假设数据包被分发到了一个没有对应 Pod 的节点（VM2）上 在 VM2 节点上，kube-proxy 在节点上安装的 iptables 规则会将该数据包的目标地址判定到对应的 Pod 上（集群内负载均衡将生效） iptables 完成 NAT 映射，并将数据包转发到目标 Pod 7 层：Ingress控制器Layer 7 网络入方向访问在网络堆栈的 HTTP/HTTPS 协议层面工作，并且依赖于 KUbernetes Service。要实现 Layer 7 网络入方向访问，首先需要将 Service 指定为 NodtePort 类型，此时 Kubernetes master 将会为该 Service 分配一个节点端口，每一个节点上的 iptables 都会将此端口上的请求转发到 Service 的后端 Pod 上。此时，Service-to-Pod 的路由与数据包的传递：Service-to-Pod的描述相同。 接下来，创建一个 Kubernetes Ingress 对象可以将该 Service 发布到互联网。Ingress 是一个高度抽象的 HTTP 负载均衡器，可以将 HTTP 请求映射到 Kubernetes Service。在不同的 Kubernetes 集群中，Ingress 的具体实现可能是不一样的。与 Layer 4 的网络负载均衡器相似，HTTP 负载均衡器只理解节点的 IP 地址（而不是 Pod 的 IP 地址），因此，也同样利用了集群内部通过 iptables 实现的负载均衡特性。 在 AWS 中，ALB Ingress 控制器使用 Amazon 的 Layer 7 Application Load Balancer实现了 Kubernetes Ingress 的功能。下图展示了 AWS 上 Ingress 控制器的细节，也展示了网络请求是如何从 ALB 路由到 Kubernetes 集群的。 ALB Ingress Controller 创建后，将监听 Kubernetes API 上关于 Ingress 的事件。当发现匹配的 Ingress 对象时，Ingress Controller 开始创建 AWS 资源 AWS 使用 Application Load Balancer（ALB）来满足 Ingress 对象的要求，并使用 Target Group 将请求路由到目标节点 ALB Ingress Controller 为 Kubernetes Ingress 对象中用到的每一个 Kubernetes Service 创建一个 AWS Target Group Listener 是一个 ALB 进程，由 ALB Ingress Controller 根据 Ingress 的注解（annotations）创建，监听 ALB 上指定的协议和端口，并接收外部的请求 ALB Ingress Controller 还根据 Kubernetes Ingress 中的路径定义，创建了 Target Group Rule，确保指定路径上的请求被路由到合适的 Kubernetes Service 数据包的传递：Ingress-to-ServiceIngress-to-Service 的数据包传递与 LoadBalancer-to-Service 的数据包传递非常相似。核心差别是： Ingress 能够解析 URL 路径（可基于路径进行路由） Ingress 连接到 Service 的 NodePort 下图展示了 Ingress-to-Service 的数据包传递过程。 创建 Ingress 之后，cloud controller 将会为其创建一个新的 Ingress Load Balancer 由于 Load Balancer 并不知道 Pod 的 IP 地址，当路由到达 Ingress Load Balancer 之后，会被转发到集群中的节点上（Service的节点端口） 节点上的 iptables 规则将数据包转发到合适的 Pod Pod 接收到数据包 从 Pod 返回的响应数据包将包含 Pod 的 IP 地址，但是请求客户端需要的是 Ingress Load Balancer 的 IP 地址。iptables 和 conntrack 被用来重写返回路径上的 IP 地址。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"K8s-网络策略","slug":"K8s-网络策略","date":"2020-04-10T08:59:31.000Z","updated":"2020-04-14T06:50:29.028Z","comments":true,"path":"2020/04/10/K8s-网络策略/","link":"","permalink":"http://yoursite.com/2020/04/10/K8s-%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/","excerpt":"Network Policies 网络策略","text":"Network Policies 网络策略 Network PoliciesKubernetes 中，Network Policy（网络策略）定义了一组 Pod 是否允许相互通信，或者与网络中的其他端点 endpoint 通信。 NetworkPolicy 对象使用标签选择Pod，并定义规则指定选中的Pod可以执行什么样的网络通信 前提条件Network Policy 由网络插件实现，因此，使用的网络插件必须能够支持 NetworkPolicy 才可以使用此特性。如果仅仅是创建了一个 Network Policy 对象，但使用的网络插件并不支持此特性，所创建的 Network Policy 对象是不生效的。 solated/Non-isolated Pods默认情况下，Pod 都是非隔离的（non-isolated），可以接受来自任何请求方的网络请求。 如果一个 NetworkPolicy 的标签选择器选中了某个 Pod，则该 Pod 将变成隔离的（isolated），并将拒绝任何不被 NetworkPolicy 许可的网络连接。（名称空间中其他未被 NetworkPolicy 选中的 Pod 将认可接受来自任何请求方的网络请求。） Network Police 不会相互冲突，而是相互叠加的。如果多个 NetworkPolicy 选中了同一个 Pod，则该 Pod 可以接受这些 NetworkPolicy 当中任何一个 NetworkPolicy 定义的（入口/出口）规则，是所有NetworkPolicy规则的并集，因此，NetworkPolicy 的顺序并不重要，因为不会影响到最终的结果。 NetworkPolicy对象 一个 NetworkPolicy 的 Example 如下所示： 12345678910111213141516171819202122232425262728293031323334apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 基本信息： 同其他的 Kubernetes 对象一样，NetworkPolicy 需要 apiVersion、kind、metadata 字段 spec： NetworkPolicy 的 spec 字段包含了定义网络策略的主要信息： podSelector： 同名称空间中，符合此标签选择器 .spec.podSelector 的 Pod 都将应用这个 NetworkPolicy。上面的 Example中的 podSelector 选择了 role=db 的 Pod。如果该字段为空，则将对名称空间中所有的 Pod 应用这个 NetworkPolicy policyTypes： .spec.policyTypes 是一个数组类型的字段，该数组中可以包含 Ingress、Egress 中的一个，也可能两个都包含。该字段标识了此 NetworkPolicy 是否应用到 入方向的网络流量、出方向的网络流量、或者两者都有。如果不指定 policyTypes 字段，该字段默认将始终包含 Ingress，当 NetworkPolicy 中包含出方向的规则时，Egress 也将被添加到默认值。 ingress： ingress 是一个数组，代表入方向的白名单规则。每一条规则都将允许与 from 和 ports 匹配的入方向的网络流量发生。例子中的 ingress 包含了一条规则，允许的入方向网络流量必须符合如下条件： Pod 的监听端口为 6379 请求方可以是如下三种来源当中的任意一种： ipBlock 为 172.17.0.0/16 网段，但是不包括 172.17.1.0/24 网段 namespaceSelector 标签选择器，匹配标签为 project=myproject podSelector 标签选择器，匹配标签为 role=frontend egress： egress 是一个数组，代表出方向的白名单规则。每一条规则都将允许与 to 和 ports 匹配的出方向的网络流量发生。例子中的 egress 允许的出方向网络流量必须符合如下条件： 目标端口为 5978 目标 ipBlock 10.0.0.0/24网段 因此，例子中的 NetworkPolicy 对网络流量做了如下限制： 隔离了 default 名称空间中带有 role=db 标签的所有 Pod 的入方向网络流量和出方向网络流量 Ingress规则（入方向白名单规则）： 当请求方是如下三种来源当中的任意一种时，允许访问 default 名称空间中所有带 role=db 标签的 Pod 的 6379 端口： ipBlock 为 172.17.0.0/16 网段，但是不包括 172.17.1.0/24 网段 namespaceSelector 标签选择器，匹配标签为 project=myproject podSelector 标签选择器，匹配标签为 role=frontend Egress rules（出方向白名单规则）： 当如下条件满足时，允许出方向的网络流量： 目标端口为 5978 目标 ipBlock 为 10.0.0.0/24 网段","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"通过Ingress访问应用程序","slug":"通过Ingress访问应用程序","date":"2020-04-10T07:05:26.000Z","updated":"2020-04-10T08:32:27.837Z","comments":true,"path":"2020/04/10/通过Ingress访问应用程序/","link":"","permalink":"http://yoursite.com/2020/04/10/%E9%80%9A%E8%BF%87Ingress%E8%AE%BF%E9%97%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"用 Ingress 访问应用程序","text":"用 Ingress 访问应用程序 通过 Ingress 访问应用程序什么是 Ingress？通常情况下，Service 和 Pod 的 IP 仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到 Service 在 Node 上暴露的 NodePort 上，然后再由 kube-proxy 通过边缘路由器 (edge router) 将其转发给相关的 Pod 或者丢弃。而 Ingress 就是为进入集群的请求提供路由规则的集合。 Ingress 可以给 Service 提供集群外部访问的 URL、负载均衡、SSL 终止、HTTP 路由等。为了配置这些 Ingress 规则，集群管理员需要部署一个 Ingress Controller，它监听 Ingress 和 Service 的变化，并根据规则配置负载均衡并提供访问入口。 Pod 与 Ingress 的关系 通过label-selector相关联 通过Ingress Controller实现Pod的负载均衡 -支持TCP/UDP 4层和HTTP 7层 Ingress 组成： ingress controller：将新加入的Ingress转化成Nginx的配置文件并使之生效； ingress服务：将Nginx的配置抽象成一个Ingress对象，每添加一个新的服务只需写一个新的Ingress的yaml文件即可； Ingress 工作原理：ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，再写到nginx-ingress-control的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，然后reload一下使配置生效。以此达到域名分配置和动态更新的问题。 实战：通过 Ingress 使的应用程序在互联网可用 创建文件 nginx-deployment.yaml 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 创建文件 nginx-service.yaml 12345678910111213141516apiVersion: v1kind: Servicemetadata: name: nginx-service labels: app: nginxspec: selector: app: nginx ports: - name: nginx-port protocol: TCP port: 80 nodePort: 32600 targetPort: 80 type: NodePort 创建文件 nginx-ingress.yaml 12345678910111213apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: my-ingress-for-nginx # Ingress 的名字，仅用于标识spec: rules: # Ingress 中定义 L7 路由规则 - host: www.test.com # 根据 virtual hostname 进行路由（请使用您自己的域名） http: paths: # 按路径进行路由 - path: / backend: serviceName: nginx-service # 指定后端的 Service 为之前创建的 nginx-service servicePort: 80 执行命令 123kubectl apply -f nginx-deployment.yamlkubectl apply -f nginx-service.yamlkubectl apply -f nginx-ingress.yaml 检查执行结果 123456[root@k8s-master ingress]# kubectl get ingress -o wideNAME HOSTS ADDRESS PORTS AGEmy-ingress-for-nginx www.test.com 80 39m#从互联网访问[root@k8s-master ingress]# curl www.test.com","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"K8s-Service连接应用程序","slug":"K8s-Service连接应用程序","date":"2020-04-08T08:00:56.000Z","updated":"2020-04-10T03:36:38.870Z","comments":true,"path":"2020/04/08/K8s-Service连接应用程序/","link":"","permalink":"http://yoursite.com/2020/04/08/K8s-Service%E8%BF%9E%E6%8E%A5%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"创建/访问/Service，保护 Service 安全，暴露 Service；","text":"创建/访问/Service，保护 Service 安全，暴露 Service； Service连接应用程序Kubernetes 的网络模型通常，Docker 使用一种 host-private 的联网方式，在此情况下，只有两个容器都在同一个节点（主机）上时，一个容器才可以通过网络连接另一个容器。为了使 Docker 容器可以跨节点通信，必须在宿主节点（主机）的 IP 地址上分配端口，并将该端口接收到的网络请求转发（或代理）到容器中。这意味着，用户必须非常小心地为容器分配宿主节点（主机）的端口号，或者端口号可以自动分配。 在一个集群中，多个开发者之间协调分配端口号是非常困难的。Kubernetes 认为集群中的两个 Pod 应该能够互相通信，无论他们各自在哪个节点上。每一个 Pod 都被分配自己的 “cluster-private-IP”，因此，无需在 Pod 间建立连接，或者将容器的端口映射到宿主机的端口。因此： Pod 中的任意容器可以使用 localhost 直连同 Pod 中另一个容器的端口 集群中的任意 Pod 可以使用另一的 Pod 的 cluster-private-IP 直连对方的端口，（无需 NAT 映射） 在集群中部署 Pod 创建文件 run-my-nginx.yaml，文件内容如下 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: my-nginxspec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx ports: - containerPort: 80 执行以下命令，部署 Pod 并检查运行情况： 1234567[root@k8s-master 0408]# kubectl apply -f run-my-nginx.yaml deployment.apps&#x2F;my-nginx created[root@k8s-master 0408]# kubectl get pods -l run&#x3D;my-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmy-nginx-75897978cd-6vfrl 0&#x2F;1 ErrImagePull 0 39s 10.244.235.206 k8s-master &lt;none&gt; &lt;none&gt;my-nginx-75897978cd-p2tcd 1&#x2F;1 Running 0 39s 10.244.44.235 k8s-node-02 &lt;none&gt; &lt;none&gt; 执行命令 kubectl get pods -l run=my-nginx -o yaml | grep podIP， 检查 Pod 的 IP 地址，输出结果如下： 1234567[root@k8s-master 0408]# kubectl get pods -l run&#x3D;my-nginx -o yaml | grep podIP cni.projectcalico.org&#x2F;podIP: 10.244.235.206&#x2F;32 podIP: 10.244.235.206 podIPs: cni.projectcalico.org&#x2F;podIP: 10.244.44.235&#x2F;32 podIP: 10.244.44.235 podIPs: 在集群中的任意节点上，您可以执行 curl 10.244.235.206 或curl 10.244.44.235` 获得 nginx 的响应。此时： 容器并没有使用节点上的 80 端口 没有使用 NAT 规则对容器端口进行映射 这意味着，您可以 在同一节点上使用 80 端口运行多个 nginx Pod 在集群的任意节点/Pod 上使用 nginx Pod 的 clusterIP 访问 nginx 的 80 端口 同 Docker 一样，Kubernets 中，仍然可以将 Pod 的端口映射到宿主节点的网络地址上（使用 nodePort），但是使用 Kubernetes 的网络模型时，这类需求已经大大减少了。 创建 ServicePod 因为故障或其他原因终止后，Deployment Controller 将创建一个新的 Pod 以替代该 Pod，但是 IP 地址将发生变化。Kubernetes Service 解决了这样的问题。 Kubernetes Service： 定义了集群中一组 Pod 的逻辑集合，该集合中的 Pod 提供了相同的功能 被创建后，获得一个唯一的 IP 地址（ClusterIP）。直到该 Service 被删除，此地址不会发生改变 Pod 可以直接连接 Service IP 地址上的端口，且发送到该 IP 地址的网络请求被自动负载均衡分发到 Service 所选取的 Pod 集合中 执行命令 kubectl expose deployment/my-nginx 可以为上面的两个 nginx Pod 创建 Service，输出结果如下所示： 12[root@k8s-master 0408]# kubectl expose deployment&#x2F;my-nginxservice&#x2F;my-nginx exposed 该命令等价于 kubectl apply -f nginx-svc.yaml，其中 nginx-svc.yaml 文件的内容如下所示： 12345678910111213apiVersion: v1kind: Servicemetadata: name: my-nginx labels: run: my-nginxspec: ports: - port: 80 targetPort: 80 protocol: TCP selector: run: my-nginx 该 yaml 文件将创建一个 Service： 该 Service 通过 label selector 选取包含 run: my-nginx 标签的 Pod 作为后端 Pod 该 Service 暴露一个端口 80（spec.ports[*].port） 该 Service 将 80 端口上接收到的网络请求转发到后端 Pod 的 80 （spec.ports[*].targetPort）端口上，支持负载均衡 123[root@k8s-master 0408]# kubectl get svc my-nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmy-nginx ClusterIP 10.97.72.97 &lt;none&gt; 80&#x2F;TCP 28m Service 的后端 Pod 实际上通过 Endpoints 来暴露。Kubernetes 会持续检查 Service 的 label selector spec.selector，并将符合条件的 Pod 更新到与 Service 同名（my-nginx）的 Endpoints 对象。如果 Pod 终止了，该 Pod 将被自动从 Endpoints 中移除，新建的 Pod 将自动被添加到该 Endpoint。 执行命令 kubectl describe svc my-nginx，输出结果如下，请注意 Endpoints 中的 IP 地址与上面获得的 Pod 地址相同： 12345678910111213[root@k8s-master 0408]# kubectl describe svc my-nginxName: my-nginxNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: run&#x3D;my-nginxType: ClusterIPIP: 10.97.72.97Port: &lt;unset&gt; 80&#x2F;TCPTargetPort: 80&#x2F;TCPEndpoints: 10.244.154.209:80,10.244.44.236:80Session Affinity: NoneEvents: &lt;none 执行命令 kubectl get ep my-nginx，输出结果如下： 123[root@k8s-master 0408]# kubectl get ep my-nginxNAME ENDPOINTS AGEmy-nginx 10.244.154.209:80,10.244.44.236:80 38m 此时，可以在集群的任意节点上执行 curl 10.0.162.149:80，通过 Service 的 ClusterIP:Port 访问 nginx。 Service 的 IP 地址是虚拟地址。 访问 ServiceKubernetes 支持两种方式发现服务： 环境变量 DNS 环境变量针对每一个有效的 Service，kubelet 在创建 Pod 时，向 Pod 添加一组环境变量。这种做法引发了一个 Pod 和 Service 的顺序问题。例如： 1234567891011121314151617[root@k8s-master 0408]# kubectl exec my-nginx-75897978cd-4t7zb -- printenv | grep SERVICEMY_SERVICE_PORT_80_TCP_PORT=80MY_SERVICE_PORT=tcp://10.100.240.247:80MY_SERVICE_SERVICE_HOST=10.100.240.247MY_SERVICE_PORT_80_TCP=tcp://10.100.240.247:80KUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_SERVICE_PORT=443NGINX_DEP_SERVICE_PORT_80_80=80MY_SERVICE_PORT_80_TCP_ADDR=10.100.240.247MYAPP_SERVICE_PORT_80_80=80MY_SERVICE_SERVICE_PORT=80MYAPP_SERVICE_HOST=10.102.48.254MYAPP_SERVICE_PORT=80NGINX_DEP_SERVICE_HOST=10.98.115.52NGINX_DEP_SERVICE_PORT=80MY_SERVICE_PORT_80_TCP_PROTO=tcpKUBERNETES_SERVICE_HOST=10.96.0.1 DNSKubernetes 提供了一个 DNS cluster addon，可自动为 Service 分配 DNS name。该 addon 已经默认安装。 执行命令 kubectl get services kube-dns --namespace=kube-system 查看该 addon 在集群上是否可用，输出结果如下所示： 123[root@k8s-master 0408]# kubectl get services kube-dns --namespace&#x3D;kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53&#x2F;UDP,53&#x2F;TCP,9153&#x2F;TCP 13d 此时，可以从集群中任何 Pod 中按 Service 的名称访问该 Service。 执行命令 kubectl run curl --image=radial/busyboxplus:curl -i --tty 获得 busyboxplus 容器的命令行终端，该命令输出结果如下所示： 123[root@k8s-master 0408]# kubectl run curl --image=radial/busyboxplus:curl -i --ttykubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.If you don't see a command prompt, try pressing enter. 执行命令 nslookup my-nginx，输出结果如下所示： 123456[ root@curl-69c656fd45-848f6:/ ]$ nslookup my-nginxServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: my-nginxAddress 1: 10.97.72.97 my-nginx.default.svc.cluster.local 执行命令 curl my-nginx:80，可获得 Nginx 的响应。 执行命令 kubectl delete deployment curl 可删除刚才创建的 curl 测试容器 保护 Service 的安全在将该 Service 公布到互联网时，可能需要确保该通信渠道是安全的。为此： 准备 https 证书（购买，或者自签名） 将该 nginx 服务配置好，并使用该 https 证书 配置 Secret，以使得其他 Pod 可以使用该证书 配置 nginx 使用自签名证书： 创建密钥对 12345[root@k8s-master 0408]# openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/nginx.key -out /tmp/nginx.crt -subj \"/CN=my-nginx/O=my-nginx\"Generating a 2048 bit RSA private key.............................................+++.............................................+++writing new private key to '/tmp/nginx.key' 将密钥对转换为 base64 编码 12cat /tmp/nginx.crt | base64cat /tmp/nginx.key | base64 创建一个如下格式的 nginxsecrets.yaml 文件，使用前面命令输出的 base64 编码替换其中的内容（base64编码内容不能换行） 12345678apiVersion: \"v1\"kind: \"Secret\"metadata: name: \"nginxsecret\" namespace: \"default\"data: nginx.crt: \"\" nginx.key: \"\" 使用该文件创建 Secrets 123456789#创建 Secrets[root@k8s-master 0408]# kubectl apply -f 1.yaml secret/nginxsecret created#查看 Secrets[root@k8s-master 0408]# kubectl get secretsNAME TYPE DATA AGEdefault-token-xws5p kubernetes.io/service-account-token 3 13dnginxsecret Opaque 2 38s 修改 nginx 部署，使 nginx 使用 Secrets 中的 https 证书，修改 Service，使其暴露 80 端口和 443端口。nginx-secure-app.yaml 文件如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546apiVersion: v1kind: Servicemetadata: name: my-nginx labels: run: my-nginxspec: type: NodePort ports: - port: 80 targetPort: 80 protocol: TCP name: http - port: 443 protocol: TCP name: https selector: run: my-nginx---apiVersion: apps/v1kind: Deploymentmetadata: name: my-nginxspec: selector: matchLabels: run: my-nginx replicas: 1 template: metadata: labels: run: my-nginx spec: volumes: - name: secret-volume secret: secretName: nginxsecret ##### containers: - name: nginxhttps image: bprashanth/nginxhttps:1.0 ports: - containerPort: 443 - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/ssl ##### name: secret-volume 关于 nginx-secure-app.yaml 该文件同时包含了 Deployment 和 Service 的定义 nginx server 监听 HTTP 80 端口和 HTTPS 443 端口的请求， nginx Service 同时暴露了这两个端口 nginx 容器可以通过 /etc/nginx/ssl 访问到 https 证书，https 证书存放在 Secrets 中，且必须在 Pod 创建之前配置好。 执行命令使该文件生效： 12kubectl delete deployments,svc my-nginxkubectl create -f ./nginx-secure-app.yaml 此时，可以从任何节点访问该 nginx server 12345kubectl get pods -o yaml | grep -i podip podIP: 10.244.3.5node $ curl -k https://10.244.3.5...&lt;h1&gt;Welcome to nginx!&lt;/h1&gt; 创建 curlpod.yaml 文件，内容如下： 12345678910111213141516171819202122232425262728apiVersion: apps/v1kind: Deploymentmetadata: name: curl-deploymentspec: selector: matchLabels: app: curlpod replicas: 1 template: metadata: labels: app: curlpod spec: volumes: - name: secret-volume secret: secretName: nginxsecret containers: - name: curlpod command: - sh - -c - while true; do sleep 1; done image: radial/busyboxplus:curl volumeMounts: - mountPath: /etc/nginx/ssl name: secret-volume 执行命令，完成 curlpod 的部署 123456789101112[root@k8s-master 0408]# kubectl apply -f curlpod.yaml deployment.apps&#x2F;curl-deployment created[root@k8s-master 0408]# kubectl get pods -l app&#x3D;curlpodNAME READY STATUS RESTARTS AGEcurl-deployment-f8c5c685b-cwqrp 1&#x2F;1 Running 0 17s#执行 curl，访问 nginx 的 https 端口[root@k8s-master 0408]# kubectl exec curl-deployment-f8c5c685b-cwqrp -- curl https:&#x2F;&#x2F;my-nginx --cacert &#x2F;etc&#x2F;nginx&#x2F;ssl&#x2F;nginx.crt % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 暴露 Service在应用程序中，可能有一部分功能需要通过 Service 发布到一个外部的 IP 地址上。Kubernetes 支持如下两种方式： NodePort LoadBalancer 需要云环境支持 执行命令查看service暴露的外部端口 [root@k8s-master 0408]# kubectl get svc my-nginx -o yaml | grep nodePort -C 5 spec: clusterIP: 10.107.66.92 externalTrafficPolicy: Cluster ports: - name: http nodePort: 30407 ### port: 80 protocol: TCP targetPort: 80 - name: https nodePort: 30534 ### port: 443 protocol: TCP targetPort: 443 selector: run: my-nginx 123456假设某一节点的公网 IP 地址为 23.251.152.56，可以在任意一台可上网的机器执行命令 &#96;curl https:&#x2F;&#x2F;23.251.152.56:30407 -k&#96;。输出结果为：&#96;&#96;&#96;text...&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"K8s-Service","slug":"K8s-Service","date":"2020-04-08T05:30:32.000Z","updated":"2020-04-08T07:59:36.978Z","comments":true,"path":"2020/04/08/K8s-Service/","link":"","permalink":"http://yoursite.com/2020/04/08/K8s-Service/","excerpt":"Service的功能及作用","text":"Service的功能及作用 一、Service概述为何需要 ServiceKubernetes 中 Pod 是随时可以消亡的（节点故障、容器内应用程序错误等原因）。如果使用 Deployment运行应用程序，Deployment 将会在 Pod 消亡后再创建一个新的 Pod 以维持所需要的副本数。每一个 Pod 有自己的 IP 地址，然而，对于 Deployment 而言，对应 Pod 集合是动态变化的。 这个现象导致了如下问题： 如果某些 Pod（假设是 ‘backends’）为另外一些 Pod（假设是 ‘frontends’）提供接口，在 ‘backends’ 中的 Pod 集合不断变化（IP 地址也跟着变化）的情况下，’frontends’ 中的 Pod 如何才能知道应该将请求发送到哪个 IP 地址？ Service 存在的意义，就是为了解决这个问题 Kubernetes ServiceKubernetes 中 Service 是一个 API 对象，通过 kubectl + YAML，定义一个 Service，可以将符合 Service 指定条件的 Pod 作为可通过网络访问的服务提供给服务调用者。 Service 是 Kubernetes 中的一种服务发现机制： Pod 有自己的 IP 地址 Service 被赋予一个唯一的 dns name Service 通过 label selector 选定一组 Pod Service 实现负载均衡，可将请求均衡分发到选定这一组 Pod 中 例如，假设有一个无状态的图像处理后端程序运行了 3 个 Pod 副本。这些副本是相互可替代的（前端程序调用其中任何一个都可以）。在后端程序的副本集中的 Pod 经常变化（销毁、重建、扩容、缩容等）的情况下，前端程序不应该关注这些变化。 Kubernetes 通过引入 Service 的概念，将前端与后端解耦 二、Service详细描述创建 ServiceKubernetes Servies 是一个 RESTFul 接口对象，可通过 yaml 文件创建。 例如，假设您有一组 Pod： 每个 Pod 都监听 9376 TCP 端口 每个 Pod 都有标签 app=MyApp 1234567891011apiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: app: MyApp ports: - protocol: TCP port: 80 targetPort: 9376 上述 YAML 文件可用来创建一个 Service： 名字为 my-service 目标端口为 TCP 9376 选取所有包含标签 app=MyApp 的 Pod 关于 Service： Kubernetes 将为该 Service 分配一个 IP 地址（ClusterIP 或 集群内 IP），供 Service Proxy 使用。 Kubernetes 将不断扫描符合该 selector 的 Pod，并将最新的结果更新到与 Service 同名 my-service 的 Endpoint 对象中。 Service 从自己的 IP 地址和 port 端口接收请求，并将请求映射到符合条件的 Pod 的 targetPort。为了方便，默认 targetPort 的取值 与 port 字段相同 Pod 的定义中，Port 可能被赋予了一个名字，可以在 Service 的 targetPort 字段引用这些名字，而不是直接写端口号。这种做法可以使得将来修改后端程序监听的端口号，而无需影响到前端程序。 Service 的默认传输协议是 TCP，也可以使用其他支持的传输协议。 Kubernetes Service 中，可以定义多个端口，不同的端口可以使用相同或不同的传输协议。 创建 Service（无 label selector）Service 通常用于提供对 Kubernetes Pod 的访问，但是也可以将其用于任何其他形式的后端。例如： 在生产环境中使用一个 Kubernetes 外部的数据库集群，在测试环境中使用 Kubernetes 内部的 数据库 将 Service 指向另一个名称空间中的 Service，或者另一个 Kubernetes 集群中的 Service 将程序迁移到 Kubernetes，但是根据迁移路径，只将一部分后端程序运行在 Kubernetes 中 在上述这些情况下，可以定义一个没有 Pod Selector 的 Service。例如： 123456789apiVersion: v1kind: Servicemetadata: name: my-servicespec: ports: - protocol: TCP port: 80 targetPort: 9376 因为该 Service 没有 selector，相应的 Endpoint 对象就无法自动创建。可以手动创建一个 Endpoint 对象，以便将该 Service 映射到后端服务真实的 IP 地址和端口： 123456789apiVersion: v1kind: Endpointsmetadata: name: my-servicesubsets: - addresses: - ip: 192.0.2.42 ports: - port: 9376 注意： 对于 Service 的访问者来说，Service 是否有 label selector 都是一样的。在上述例子中，Service 将请求路由到 Endpoint 192.0.2.42:9376 (TCP)。 Endpoint 中的 IP 地址不可以是集群中其他 Service 的 ClusterIP。 Kubernetes 支持三种 proxy mode（代理模式）1.User space 代理模式在 user space proxy mode 下： kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件 kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 打开一个随机端口 kube-proxy 安装 iptables 规则，将发送到该 Service 的 ClusterIP（虚拟 IP）/ Port 的请求重定向到该随机端口 任何发送到该随机端口的请求将被代理转发到该 Service 的后端 Pod 上（kube-proxy 从 Endpoint 信息中获得可用 Pod） kube-proxy 在决定将请求转发到后端哪一个 Pod 时，默认使用 round-robin（轮询）算法，并会考虑到 Service 中的 SessionAffinity 的设定 如下图所示： 2.Iptables 代理模式 默认模式在 iptables proxy mode 下： kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件 kube-proxy 在其所在的节点（每个节点都有 kube-proxy）上为每一个 Service 安装 iptable 规则 iptables 将发送到 Service 的 ClusterIP / Port 的请求重定向到 Service 的后端 Pod 上 对于 Service 中的每一个 Endpoint，kube-proxy 安装一个 iptable 规则 默认情况下，kube-proxy 随机选择一个 Service 的后端 Pod iptables proxy mode 的优点： 更低的系统开销：在 linux netfilter 处理请求，无需在 userspace 和 kernel space 之间切换 更稳定 与 user space mode 的差异： 使用 iptables mode 时，如果第一个 Pod 没有响应，则创建连接失败 使用 user space mode 时，如果第一个 Pod 没有响应，kube-proxy 会自动尝试连接另外一个后端 Pod 可以配置 Pod 就绪检查（readiness probe）确保后端 Pod 正常工作，此时，在 iptables 模式下 kube-proxy 将只使用健康的后端 Pod，从而避免了 kube-proxy 将请求转发到已经存在问题的 Pod 上。 3.IPVS 代理模式在 IPVS proxy mode 下： kube-proxy 监听 kubernetes master 以获得添加和移除 Service / Endpoint 的事件 kube-proxy 根据监听到的事件，调用 netlink 接口，创建 IPVS 规则；并且将 Service/Endpoint 的变化同步到 IPVS 规则中 当访问一个 Service 时，IPVS 将请求重定向到后端 Pod IPVS 模式的优点 IPVS proxy mode 基于 netfilter 的 hook 功能，与 iptables 代理模式相似，但是 IPVS 代理模式使用 hash table 作为底层的数据结构，并在 kernel space 运作。这就意味着 IPVS 代理模式可以比 iptables 代理模式有更低的网络延迟，在同步代理规则时，也有更高的效率 与 user space 代理模式 / iptables 代理模式相比，IPVS 模式可以支持更大的网络流量 注意： 如果要使用 IPVS 模式，必须在启动 kube-proxy 前为节点的 linux 启用 IPVS kube-proxy 以 IPVS 模式启动时，如果发现节点的 linux 未启用 IPVS，则退回到 iptables 模式 代理模式总结在所有的代理模式中，发送到 Service 的 IP:Port 的请求将被转发到一个合适的后端 Pod，而无需调用者知道任何关于 Kubernetes/Service/Pods 的细节。 Service 中额外字段的作用： service.spec.sessionAffinity 12345 - 默认值为 &quot;None&quot; - 如果设定为 &quot;ClientIP&quot;，则同一个客户端的连接将始终被转发到同一个 Pod- service.spec.sessionAffinityConfig.clientIP.timeoutSeconds 12345678910111213141516171819202122232425 - 默认值为 10800 （3 小时） - 设定会话保持的持续时间## 多端口的ServiceKubernetes 中，可以在一个 Service 对象中定义多个端口，但必须为每个端口定义一个名字。如下所示：&#96;&#96;&#96;yamlapiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 - name: https protocol: TCP port: 443 targetPort: 9377 使用自定义的 IP 地址创建 Service 时，如果指定 .spec.clusterIP 字段，可以使用自定义的 Cluster IP 地址。该 IP 地址必须是 APIServer 中配置字段 service-cluster-ip-range CIDR 范围内的合法 IPv4 或 IPv6 地址，否则不能创建成功。 服务发现Kubernetes 支持两种主要的服务发现模式： 环境变量 DNS 环境变量kubelet 查找有效的 Service，并针对每一个 Service，向其所在节点上的 Pod 注入一组环境变量。支持的环境变量有： {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT Service name 被转换为大写 小数点 . 被转换为下划线 _ 如果要在 Pod 中使用基于环境变量的服务发现方式，必须先创建 Service，再创建调用 Service 的 Pod。否则，Pod 中不会有该 Service 对应的环境变量。 DNSCoreDNS 监听 Kubernetes API 上创建和删除 Service 的事件，并为每一个 Service 创建一条 DNS 记录。集群中所有的 Pod 都可以使用 DNS Name 解析到 Service 的 IP 地址。 虚拟 IP 的实现避免冲突Kubernetes 的一个设计哲学是：尽量避免非人为错误产生的可能性。就设计 Service 而言，Kubernetes 应该将选择的端口号与其他人选择的端口号隔离开。为此，Kubernetes 为每一个 Service 分配一个该 Service 专属的 IP 地址。 为了确保每个 Service 都有一个唯一的 IP 地址，kubernetes 在创建 Service 之前，先更新 etcd 中的一个全局分配表，如果更新失败（例如 IP 地址已被其他 Service 占用），则 Service 不能成功创建。 Kubernetes 使用一个后台控制器检查该全局分配表中的 IP 地址的分配是否仍然有效，并且自动清理不再被 Service 使用的 IP 地址。 Service 的 IP 地址Pod 的 IP 地址路由到一个确定的目标，然而 Service 的 IP 地址则不同，通常背后并不对应一个唯一的目标。 kube-proxy 使用 iptables （Linux 中的报文处理逻辑）来定义虚拟 IP 地址。当客户端连接到该虚拟 IP 地址时，它们的网络请求将自动发送到一个合适的 Endpoint。Service 对应的环境变量和 DNS 实际上反应的是 Service 的虚拟 IP 地址（和端口）。 Userspace以上面提到的图像处理程序为例。当后端 Service 被创建时，Kubernetes master 为其分配一个虚拟 IP 地址（假设是 10.0.0.1），并假设 Service 的端口是 1234。集群中所有的 kube-proxy 都实时监听者 Service 的创建和删除。Service 创建后，kube-proxy 将打开一个新的随机端口，并设定 iptables 的转发规则（以便将该 Service 虚拟 IP 的网络请求全都转发到这个新的随机端口上），并且 kube-proxy 将开始接受该端口上的连接。 当一个客户端连接到该 Service 的虚拟 IP 地址时，iptables 的规则被触发，并且将网络报文重定向到 kube-proxy 自己的随机端口上。kube-proxy 接收到请求后，选择一个后端 Pod，再将请求以代理的形式转发到该后端 Pod。 这意味着 Service 可以选择任意端口号，而无需担心端口冲突。客户端可以直接连接到一个 IP:port，无需关心最终在使用哪个 Pod 提供服务。 iptables仍然以上面提到的图像处理程序为例。当后端 Service 被创建时，Kubernetes master 为其分配一个虚拟 IP 地址（假设是 10.0.0.1），并假设 Service 的端口是 1234。集群中所有的 kube-proxy 都实时监听者 Service 的创建和删除。Service 创建后，kube-proxy 设定了一系列的 iptables 规则（这些规则可将虚拟 IP 地址映射到 per-Service 的规则）。per-Service 规则进一步链接到 per-Endpoint 规则，并最终将网络请求重定向（使用 destination-NAT）到后端 Pod。 当一个客户端连接到该 Service 的虚拟 IP 地址时，iptables 的规则被触发。一个后端 Pod 将被选中（基于 session affinity 或者随机选择），且网络报文被重定向到该后端 Pod。与 userspace proxy 不同，网络报文不再被复制到 userspace，kube-proxy 也无需处理这些报文，且报文被直接转发到后端 Pod。 在使用 node-port 或 load-balancer 类型的 Service 时，以上的代理处理过程是相同的。 IPVS在一个大型集群中（例如，存在 10000 个 Service）iptables 的操作将显著变慢。IPVS 的设计是基于 in-kernel hash table 执行负载均衡。因此，使用 IPVS 的 kube-proxy 在 Service 数量较多的情况下仍然能够保持好的性能。同时，基于 IPVS 的 kube-proxy 可以使用更复杂的负载均衡算法（最少连接数、基于地址的、基于权重的等） 支持的传输协议 TCP UDP HTTP Proxy Protocol SCTP 三、发布ServiceService 类型Kubernetes 中可以通过不同方式发布 Service，通过 ServiceType 字段指定，该字段的默认值是 ClusterIP，可选值有： ClusterIP: 默认值。通过集群内部的一个 IP 地址暴露 Service，只在集群内部可以访问 NodePort: 通过每一个节点上的的静态端口（NodePort）暴露 Service，同时自动创建 ClusterIP 类型的访问方式 在集群内部通过 $(ClusterIP): $(Port) 访问 在集群外部通过 $(NodeIP): $(NodePort) 访问 LoadBalancer: 通过云服务供应商（AWS、Azure、GCE 等）的负载均衡器在集群外部暴露 Service，同时自动创建 NodePort 和 ClusterIP 类型的访问方式 在集群内部通过 $(ClusterIP): $(Port) 访问 在集群外部通过 $(NodeIP): $(NodePort) 访问 在集群外部通过 $(LoadBalancerIP): $(Port) 访问 ExternalName: 将 Service 映射到 externalName 指定的地址（例如：foo.bar.example.com），返回值是一个 CNAME 记录。不使用任何代理机制。 ClusterIPClusterIP 是 ServiceType 的默认值 NodePort对于 NodePort 类型的 Service，Kubernetes 为其分配一个节点端口（对于同一 Service，在每个节点上的节点端口都相同），该端口的范围在初始化 apiserver 时可通过参数 --service-node-port-range 指定（默认是：30000-32767）。节点将该端口上的网络请求转发到对应的 Service 上。可通过 Service 的 .spec.ports[*].nodePort 字段查看该 Service 分配到的节点端口号。 在启动 kube-proxy 时使用参数 --nodeport-address 可指定阶段端口可以绑定的 IP 地址段。该参数接收以逗号分隔的 CIDR 作为参数值（例如：10.0.0.0/8,192.0.2.0/25），kube-proxy 将查找本机符合该 CIDR 的 IP 地址，并将节点端口绑定到符合的 IP 地址上。 NodePort 类型的 Service 可通过如下方式访问： 在集群内部通过 $(ClusterIP): $(Port) 访问 在集群外部通过 $(NodeIP): $(NodePort) 访问 LoadBalancer在支持外部负载均衡器的云环境中（例如 GCE、AWS、Azure 等），将 .spec.type 字段设置为 LoadBalancer，Kubernetes 将为该Service 自动创建一个负载均衡器，负载均衡器的创建操作异步完成。 ExternalNameExternalName 类型的 Service 映射到一个外部的 DNS name，而不是一个 pod label selector。可通过 spec.externalName 字段指定外部 DNS name。 External IP如果有外部 IP 路由到 Kubernetes 集群的一个或多个节点，Kubernetes Service 可以通过这些 externalIPs 进行访问。externalIP 需要由集群管理员在 Kubernetes 之外配置。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"控制器-CronJob","slug":"控制器-CronJob","date":"2020-04-08T02:20:54.000Z","updated":"2020-04-08T05:31:25.505Z","comments":true,"path":"2020/04/08/控制器-CronJob/","link":"","permalink":"http://yoursite.com/2020/04/08/%E6%8E%A7%E5%88%B6%E5%99%A8-CronJob/","excerpt":"控制器CronJob： 周期性任务控制，不需要持续后台运行","text":"控制器CronJob： 周期性任务控制，不需要持续后台运行 CronJob CronJob 按照预定的时间计划（schedule）创建 Job。一个 CronJob 对象类似于 crontab (cron table) 文件中的一行记录。该对象根据 Cron 格式定义的时间计划，周期性地创建 Job 对象。 CronJob 只负责按照时间计划的规定创建 Job 对象，由 Job 来负责管理具体 Pod 的创建和执行。 使用CronJob执行自动任务CronJob可以用来执行基于时间计划的定时任务，类似于Linux/Unix系统中的crontable。 CronJob 执行周期性的重复任务时非常有用，例如备份数据、发送邮件等。CronJob 也可以用来指定将来某个时间点执行单个任务，例如将某项任务定时到系统负载比较低的时候执行。 CronJob 也存在某些限制，例如，在某些情况下，一个 CronJob 可能会创建多个 Job。因此，Job 必须是幂等的。 创建CronJob下面例子中的 CronJob 每分钟，打印一次当前时间并输出 hello world 信息。 123456789101112131415161718apiVersion: batch/v1beta1kind: CronJobmetadata: name: hellospec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 执行命令以创建 CronJob： 12[root@k8s-master 0407]# kubectl create -f cronjob.yaml cronjob.batch&#x2F;hello created 或者直接用kubectl run 命令来创建CronJob 1kubectl run hello --schedule=\"*/1 * * * *\" --restart=OnFailure --image=busybox -- /bin/sh -c \"date; echo Hello from the Kubernetes cluster\" 查看已创建的CronJob 123[root@k8s-master 0407]# kubectl get cronjob helloNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEhello *&#x2F;1 * * * * False 1 31s 16m 输出结果显示，该 CronJob 在 LAST SCHEDULE 这个时间点成功创建了一个 Job。当前 ACTIVE Job 数为 0，意味着，该 Job 已经成功结束，或者已经失败。 删除CronJob当您不再需要某个 CronJob 时，可以使用命令将其删除 kubectl delete cronjob，在本例中，可以执行命令： 12345kubectl delete cronjob hellocronjob.batch \"hello\" deleted或者kubectl delete -f cronjob.yaml 删除 CronJob 时，将移除该 CronJob 创建的所有 Job 和 Pod，并且 CronJob 控制器将不会为其在创建任何新的 Job。 写CronJob YAML与其他所有 Kubernetes 对象一样，CronJob 对象需要 apiVersion、kind、metadata 这几个字段。CronJob 还需要 .spec 字段。 所有对 CronJob 对象作出的修改，尤其是 .spec 的修改，都只对修改之后新建的 Job 有效，已经创建的 Job 不会受到影响 Schedule.spec.schedule 是一个必填字段。类型为 Cron 格式的字符串，例如 0 * * * * 或者 @hourly，该字段定义了 CronJob 应该何时创建和执行 Job。 该字段同样支持 vixie cron step 值，指定 CronJob 每隔两个小时执行一次，可以有如下三种写法： 0 0,2,4,5,6,8,12,14,16,17,20,22 * * *） 使用 范围值 + Step 值的写法：0 0-23/2 * * * Step 也可以跟在一个星号后面，如 0 */2 * * * 问号 ? 与 星号 * 的含义相同，代表着该字段不做限定 Job Template.spec.jobTemplate 字段是必填字段。该字段的结构与Job相同，只是不需要 apiVersion 和 kind。 tarting Deadline.spec.startingDeadlineSeconds 为可选字段，代表着从计划的时间点开始，最迟多少秒之内必须启动 Job。如果超过了这个时间点，CronJob 就不会为其创建 Job，并将其记录为一次错过的执行次数。如果该字段未指定，则 Job 必须在指定的时间点执行。 CronJob 控制器将为每一个 CronJob 记录错过了多少次执行次数，如果错过的执行次数超过 100，则控制器将不会再为该 CronJob 创建新的 Job。如果 .spec.startingDeadlineSeconds 未指定，CronJob 控制器计算从 .status.lastScheduleTime 开始到现在为止总共错过的执行次数。 例如，某一个 CronJob 应该每分钟执行一次，.status.lastScheduleTime 的值是 上午5:00，假设现在已经是上午7:00。这意味着已经有 120 次执行时间点被错过，因此该 CronJob 将不再执行了。 如果 .spec.startingDeadlineSeconds 字段被设置为一个非空的值，则 CronJob 控制器计算将从 .spec.startingDeadlineSeconds 秒以前到现在这个时间段内错过的执行次数。 例如，假设该字段被设置为 200，控制器将只计算过去 200 秒内错过的执行次数。如果在过去 200 秒之内，有超过 100 次错过的执行次数，则 CronJob 将不再执行。 Concurrency Policy.spec.concurrencyPolicy 是选填字段，指定了如何控制该 CronJob 创建的 Job 的并发性，可选的值有： Allow： 默认值，允许并发运行 Job Forbid： 不允许并发运行 Job；如果新的执行时间点到了，而上一个 Job 还未执行完，则 CronJob 将跳过新的执行时间点，保留仍在运行的 Job，且不会在此刻创建新的 Job Replace： 如果新的执行时间点到了，而上一个 Job 还未执行完，则 CronJob 将创建一个新的 Job 以替代正在执行的 Job TIP Concurrency policy 只对由同一个 CronJob 创建的 Job 生效。如果有多个 CronJob，则他们各自创建的 Job 之间不会相互影响。 Suspend.spec.suspend 是选填字段。如果该字段设置为 true，所有的后续执行都将挂起，该字段不会影响到已经创建的 Job。默认值为 false。 警告 挂起（suspend）的时间段内，如果恰好存在有计划的执行时间点，则这些执行时间计划都被记录下来。如果不指定 .spec.startingDeadlineSeconds，并将 .spec.suspend 字段从 true 修改为 false，则挂起这段时间内的执行计划都将被立刻执行。 Job History Limits.spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit 字段是可选的。这些字段指定了 CronJob 应该保留多少个 completed 和 failed 的 Job 记录。 .spec.successfulJobsHistoryLimit 的默认值为 3 .spec.failedJobsHistoryLimit 的默认值为 1 如果将其设置为 0，则 CronJob 不会保留已经结束的 Job 的记录。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"控制器-Job","slug":"控制器-Job","date":"2020-04-07T09:01:49.000Z","updated":"2020-04-08T02:21:57.991Z","comments":true,"path":"2020/04/07/控制器-Job/","link":"","permalink":"http://yoursite.com/2020/04/07/%E6%8E%A7%E5%88%B6%E5%99%A8-Job/","excerpt":"控制器-Job： 只要完成就立即退出，不需要重启或重建。","text":"控制器-Job： 只要完成就立即退出，不需要重启或重建。 控制器 - JobKubernetes中的 Job 对象将创建一个或多个 Pod，并确保指定数量的 Pod 可以成功执行到进程正常结束： 当 Job 创建的 Pod 执行成功并正常结束时，Job 将记录成功结束的 Pod 数量 当成功结束的 Pod 达到指定的数量时，Job 将完成执行 删除 Job 对象时，将清理掉由 Job 创建的 Pod 一个简单的例子是：创建一个 Job 对象用来确保一个 Pod 的成功执行并结束。在第一个 Pod 执行失败或者被删除（例如，节点硬件故障或机器重启）的情况下，该 Job 对象将创建一个新的 Pod 以重新执行。 当然，也可以使用 Job 对象并行执行多个 Pod。 运行一个Job的例子在下面这个 Job 的例子中，Pod 执行了一个跟 π 相关的计算，并打印出最终结果，该计算大约需要 10 秒钟执行结束。 12345678910111213apiVersion: batch/v1kind: Jobmetadata: name: pispec: template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never backoffLimit: 4 创建Job 12[root@k8s-master 0407]# kubectl apply -f job1.yaml job.batch&#x2F;pi created 查看已创建的Job 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master 0407]# kubectl get jobNAME COMPLETIONS DURATION AGEpi 0/1 38s 38s[root@k8s-master 0407]# kubectl describe jobName: piNamespace: defaultSelector: controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60Labels: controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60 job-name=piAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":&#123;\"annotations\":&#123;&#125;,\"name\":\"pi\",\"namespace\":\"default\"&#125;,\"spec\":&#123;\"backoffLimit\":4,\"template\":...Parallelism: 1Completions: 1Start Time: Tue, 07 Apr 2020 16:57:38 +0800Pods Statuses: 1 Running / 0 Succeeded / 0 FailedPod Template: Labels: controller-uid=e1b373ab-1c02-4d49-b386-ead11d012a60 job-name=pi Containers: pi: Image: perl Port: &lt;none&gt; Host Port: &lt;none&gt; Command: perl -Mbignum=bpi -wle print bpi(2000) Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Events: Type Reason Age From Message---- ------ ---- ---- ------- Normal SuccessfulCreate 48s job-controller Created pod: pi-nn8pf [root@k8s-master 0407]# kubectl get podsNAME READY STATUS RESTARTS AGEpi-nn8pf 0/1 Completed 0 79s 执行以下命令可获得该 Job 所有 Pod 的名字： 1234567[root@k8s-master 0407]# pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='&#123;.items[*].metadata.name&#125;')[root@k8s-master 0407]# echo $podspi-nn8pf在这个命令中：selector 与 Job 定义中的 selector 相同--output=jsonpath 选项指定了一个表达式，该表达式从返回结果列表中的每一个 Pod 的信息中定位出 name 字段的取值 执行以下命令可查看 Pod 的日志： 12[root@k8s-master 0407]# kubectl logs $pods3.14159265358979............................4780275901 编写Job的定义与所有的 Kubernetes 对象一样，Job 对象的 YAML 文件中，都需要包括如下三个字段： .apiVersion .kind .metadata Job 对象的 YAML 文件，还需要一个 .spec 字段。 Pod Template除了 Pod 所需的必填字段之外，Job 中的 pod template 必须指定 合适的标签 .spec.template.spec.labels 指定合适的[重启策略 restartPolicy .spec.template.spec.restartPolicy，此处只允许使用 Never 和 OnFailure 两个取值 处理Pod和容器的失败Pod 中的容器可能会因为多种原因执行失败，例如： 容器中的进程退出了，且退出码（exit code）不为 0 容器因为超出内存限制而被 Kill 其他原因 如果 Pod 中的容器执行失败，且 .spec.template.spec.restartPolicy = &quot;OnFailure&quot;，则 Pod 将停留在该节点上，但是容器将被重新执行。此时，应用程序需要处理在原节点（失败之前的节点）上重启的情况。或者，设置为 .spec.template.spec.restartPolicy = &quot;Never&quot;。 整个 Pod 也可能因为多种原因执行失败，例如： Pod 从节点上被驱逐（节点升级、重启、被删除等） Pod 的容器执行失败，且 .spec.template.spec.restartPolicy = &quot;Never&quot; 当 Pod 执行失败时，Job 控制器将创建一个新的 Pod Pod失败重试Pod backoff failure policy 某些情况下（例如，配置错误），可以在 Job 多次重试仍然失败的情况下停止该 Job。此时，可通过 .spec.backoffLimit 来设定 Job 最大的重试次数。该字段的默认值为 6. Job 中的 Pod 执行失败之后，Job 控制器将按照一个指数增大的时间延迟（10s,20s,40s … 最大为 6 分钟）来多次重新创建 Pod。如果没有新的 Pod 执行失败，则重试次数的计数将被重置。 Job的终止和清理当 Job 完成后： 将不会创建新的 Pod 已经创建的 Pod 也不会被清理掉。此时，您仍然可以继续查看已结束 Pod 的日志，以检查 errors/warnings 或者其他诊断用的日志输出 Job 对象也仍然保留着，以便您可以查看该 Job 的状态 由用户决定是否删除已完成的 Job 及其 Pod 可通过 kubectl 命令删除 Job，例如： kubectl delete jobs/pi 或者 kubectl delete -f job.yaml 删除 Job 对象时，由该 Job 创建的 Pod 也将一并被删除 Job 通常会顺利的执行下去，但是在如下情况可能会非正常终止： 某一个 Pod 执行失败（且 restartPolicy=Never） 或者某个容器执行出错（且 restartPolicy=OnFailure） 此时，Job 按照处理Pod和容器的失败中 .spec.bakcoffLimit 描述的方式进行处理 一旦重试次数达到了 .spec.backoffLimit 中的值，Job 将被标记为失败，且尤其创建的所有 Pod 将被终止 Job 中设置了 .spec.activeDeadlineSeconds。该字段限定了 Job 对象在集群中的存活时长，一旦达到 .spec.activeDeadlineSeconds 指定的时长，该 Job 创建的所有的 Pod 都将被终止，Job 的 Status 将变为 type:Failed 、 reason: DeadlineExceeded Job的自动清理系统中已经完成的 Job 通常是不在需要里的，长期在系统中保留这些对象，将给 apiserver 带来很大的压力。如果通过更高级别的控制器（例如 CronJobs）来管理 Job，则 CronJob 可以根据其中定义的基于容量的清理策略自动清理Job。 TTL 机制除了 CronJob 之外，TTL 机制是另外一种自动清理已结束Job（Completed 或 Finished）的方式： TTL 机制由 TTL 控制器 提供 在 Job 对象中指定 .spec.ttlSecondsAfterFinished 字段可激活该特性 当 TTL 控制器清理 Job 时，TTL 控制器将删除 Job 对象，以及由该 Job 创建的所有 Pod 对象。 12345678910111213apiVersion: batch/v1kind: Jobmetadata: name: pi-with-ttlspec: ttlSecondsAfterFinished: 100 #### template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never 字段解释 ttlSecondsAfterFinished： Job pi-with-ttl 的 ttlSecondsAfterFinished 值为 100，则，在其结束 100 秒之后，将可以被自动删除 如果 ttlSecondsAfterFinished 被设置为 0，则 TTL 控制器在 Job 执行结束后，立刻就可以清理该 Job 及其 Pod 如果 ttlSecondsAfterFinished 值未设置，则 TTL 控制器不会清理该 Job","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"控制器-Deployment","slug":"控制器-Deployment","date":"2020-04-02T02:57:28.000Z","updated":"2020-04-07T07:49:10.517Z","comments":true,"path":"2020/04/02/控制器-Deployment/","link":"","permalink":"http://yoursite.com/2020/04/02/%E6%8E%A7%E5%88%B6%E5%99%A8-Deployment/","excerpt":"控制器-Deployment","text":"控制器-Deployment 一、Deployment 概述 英文 英文简称 中文 Pod Pod 容器组 Controller Controller 控制器 ReplicaSet ReplicaSet 副本集 Deployment Deployment 部署 Deployment 是最常用的用于部署无状态服务的方式。Deployment 控制器使得您能够以声明的方式更新 Pod（容器组）和 ReplicaSet（副本集）。 声明式配置 声明的方式是相对于非声明方式而言的。例如，以滚动更新为例，假设有 3 个容器组，现需要将他们的容器镜像更新为新的版本。 非声明的方式，需要手动逐步执行以下过程： 使用 kubectl 创建一个新版本镜像的容器组 使用 kubectl 观察新建容器组的状态 新建容器组的状态就绪以后，使用 kubectl 删除一个旧的容器组 重复执行上述过程，直到所有容器组都已经替换为新版本镜像的容器组 声明的方式，只需要执行： 使用 kubectl 更新 Deployment 定义中 spec.template.spec.containers[].image 字段 二、创建 Deployment下面的 yaml 文件定义了一个 Deployment，该 Deployment 将创建一个有 3 个 nginx Pod 副本的 ReplicaSet（副本集）： 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 在这个例子中： 将创建一个名为 nginx-deployment 的 Deployment（部署），名称由 .metadata.name 字段指定 该 Deployment 将创建 3 个 Pod 副本，副本数量由 .spec.replicas 字段指定 .spec.selector 字段指定了 Deployment 如何找到由它管理的 Pod。此案例中，我们使用了 Pod template 中定义的一个标签（app: nginx）。对于极少数的情况，这个字段也可以定义更加复杂的规则 .template 字段包含了如下字段： .template.metadata.labels 字段，指定了 Pod 的标签（app: nginx） .template.spec.containers[].image 字段，表明该 Pod 运行一个容器 nginx:1.7.9 .template.spec.containers[].name 字段，表明该容器的名字是 nginx 注意： 必须为 Deployment 中的 .spec.selector 和 .template.metadata.labels 定义一个合适的标签（这个例子中的标签是 app: nginx）。请不要使用与任何其他控制器（其他 Deployment / StatefulSet 等）相同的 .spec.selector 和 .template.metadata.labels。否则可能发生冲突。 1.执行命令以创建 Deployment， 可以为该命令增加 –record 选项， 这样可以回顾某一个 Deployment 版本变化的原因 12[root@k8s-master 0402]# kubectl apply -f deploy.yaml --record deployment.apps&#x2F;nginx-deployment configured 2.查看 Deployment 的创建情况 1234[root@k8s-master 0402]# kubectl get deploymentsNAME READY UP-TO-DATE AVAILABLE AGEmyapp 2&#x2F;2 2 2 2d1hnginx-deployment 2&#x2F;3 3 2 11m 字段含义 字段名称 说明 NAME Deployment name DESIRED Deployment 期望的 Pod 副本数，即 Deployment 中 .spec.replicas 字段指定的数值。该数值是“期望”值 CURRENT 当前有多少个 Pod 副本数在运行 UP-TO-DATE Deployment 中，符合当前 Pod Template 定义的 Pod 数量 AVAILABLE 当前对用户可用的 Pod 副本数 AGE Deployment 部署以来到现在的时长 3.查看该 Deployment 创建的 ReplicaSet（rs），执行命令 kubectl get rs，输出结果如下所示： 1234[root@k8s-master 0402]# kubectl get rsNAME DESIRED CURRENT READY AGEmyapp-57c9b8fc4 2 2 2 2d1hnginx-deployment-54f57cf6bf 3 3 2 26m 4.查看 Pod 的标签，执行命令 kubectl get pods --show-labels，输出结果如下所示： 123456789[root@k8s-master 0402]# kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELSinit-demo 1&#x2F;1 Running 0 42h &lt;none&gt;lifecycle-demo 1&#x2F;1 Running 0 4d20h &lt;none&gt;myapp-57c9b8fc4-b4cqb 1&#x2F;1 Running 0 2d app&#x3D;myapp,pod-template-hash&#x3D;57c9b8fc4myapp-57c9b8fc4-xtcwv 1&#x2F;1 Running 0 2d1h app&#x3D;myapp,pod-template-hash&#x3D;57c9b8fc4nginx-deployment-54f57cf6bf-4kl6r 1&#x2F;1 Running 0 26m app&#x3D;nginx,pod-template-hash&#x3D;54f57cf6bfnginx-deployment-54f57cf6bf-7zc2j 1&#x2F;1 Running 0 26m app&#x3D;nginx,pod-template-hash&#x3D;54f57cf6bfnginx-deployment-54f57cf6bf-jnzkj 0&#x2F;1 ImagePullBackOff 0 26m app&#x3D;nginx,pod-template-hash&#x3D;54f57cf6bf 三、更新 Deployment使用下述步骤更新您的 Deployment方式一： 执行以下命令，将容器镜像从 nginx:1.7.9 更新到 nginx:1.9.1 123[root@k8s-master 0402]# kubectl --record deployment.apps&#x2F;nginx-deployment set image deployment.v1.apps&#x2F;nginx-deployment nginx&#x3D;nginx:1.9.1deployment.apps&#x2F;nginx-deployment image updateddeployment.apps&#x2F;nginx-deployment image updated 方式二：使用 edit 该 Deployment，并将 .spec.template.spec.containers[0].image 从 nginx:1.7.9 修改为 nginx:1.9.1 12[root@k8s-master 0402]# kubectl edit deployment nginx-deploymentdeployment.apps&#x2F;nginx-deployment edited 如果想要修改这些新的 Pod，只需要再次修改 Deployment 的 Pod template。 Deployment 将确保更新过程中，任意时刻只有一定数量的 Pod 被关闭。默认情况下，Deployment 确保至少 .spec.replicas 的 75% 的 Pod 保持可用（25% max unavailable） Deployment 将确保更新过程中，任意时刻只有一定数量的 Pod 被创建。默认情况下，Deployment 确保最多 .spec.replicas 的 25% 的 Pod 被创建（25% max surge） Deployment Controller 先创建一个新 Pod，然后删除一个旧 Pod，然后再创建新的，如此循环，直到全部更新。Deployment Controller 不会 kill 旧的 Pod，除非足够数量的新 Pod 已经就绪，Deployment Controller 也不会创新新 Pod 直到足够数量的旧 Pod 已经被 kill。这个过程将确保更新过程中，任意时刻，最少 2 个 / 最多 4 个 Pod 可用。 覆盖更新 Rollover （更新过程中再更新）每创建一个 Deployment，Deployment Controller 都为其创建一个 ReplicaSet，并设定其副本数为期望的 Pod 数（ .spec.replicas 字段）。如果 Deployment 被更新，旧的 ReplicaSet 将被 Scale down，新建的 ReplicaSet 将被 Scale up；直到最后新旧两个 ReplicaSet，一个副本数为 .spec.replias，另一个副本数为 0。这个过程称为 rollout。 当 Deployment 的 rollout 正在进行中的时候，如果再次更新 Deployment 的信息，此时 Deployment 将再创建一个新的 ReplicaSet 并开始将其 scale up，将先前正在 scale up 的 ReplicaSet 也作为一个旧的 ReplicaSet，并开始将其 scale down。 例如： 假设创建了一个 Deployment 有 5 个 nginx:1.7.9 的副本； 立刻更新该 Deployment 使得其 .spec.replicas 为 5，容器镜像为 nginx:1.9.1，而此时只有 3 个 nginx:1.7.9 的副本已创建； 此时，Deployment Controller 将立刻开始 kill 已经创建的 3 个 nginx:1.7.9 的 Pod，并开始创建 nginx:1.9.1 的 Pod。Deployment Controller 不会等到 5 个 nginx:1.7.9 的 Pod 都创建完之后在开始新的更新 四、回滚 Deployment模拟更新错误 假设您在更新 Deployment 的时候，犯了一个拼写错误，将 nginx:1.9.1 写成了 nginx:1.91 12kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=truedeployment.apps/nginx-deployment image updated 该更新将卡住，执行命令 kubectl rollout status deployment.v1.apps/nginx-deployment 检查其状态，输出结果如下所示： 1Waiting for rollout to finish: 1 out of 3 new replicas have been updated... 检查Deployment的更新历史12345[root@k8s-master 0402]# kubectl rollout history deployment nginx-deploymentdeployment.apps&#x2F;nginx-deployment REVISION CHANGE-CAUSE2 kubectl deployment.apps&#x2F;nginx-deployment set image deployment.v1.apps&#x2F;nginx-deployment nginx&#x3D;nginx:1.9.1 --record&#x3D;true3 kubectl deployment.apps&#x2F;nginx-deployment set image deployment.v1.apps&#x2F;nginx-deployment nginx&#x3D;nginx:1.9.1 --record&#x3D;true 执行命令 kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2，查看 revision（版本）的详细信息，输出结果如下所示： 123456789101112131415[root@k8s-master 0402]# kubectl rollout history deployment.v1.apps&#x2F;nginx-deployment --revision&#x3D;2deployment.apps&#x2F;nginx-deployment with revision #2Pod Template: Labels: app&#x3D;nginx pod-template-hash&#x3D;56f8998dbc Annotations: kubernetes.io&#x2F;change-cause: kubectl deployment.apps&#x2F;nginx-deployment set image deployment.v1.apps&#x2F;nginx-deployment nginx&#x3D;nginx:1.9.1 --record&#x3D;true Containers: nginx: Image: nginx:1.9.1 Port: 80&#x2F;TCP Host Port: 0&#x2F;TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; 回滚到前一个 revision（版本）下面的步骤可将 Deployment 从当前版本回滚到前一个版本（version 2） 执行命令 kubectl rollout undo deployment.v1.apps/nginx-deployment 将当前版本回滚到前一个版本，输出结果如下所示： 1deployment.apps&#x2F;nginx-deployment 或者，也可以使用 --to-revision 选项回滚到前面的某一个指定版本，执行如下命令： 1kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2 此时，Deployment 已经被回滚到前一个稳定版本。可以看到 Deployment Controller 为该 Deployment 产生了 DeploymentRollback event。 执行命令 kubectl get deployment nginx-deployment，检查该回滚是否成功，Deployment 是否按预期的运行。 五、伸缩 Deployment执行伸缩 执行命令 kubectl scale deployment nginx-deployment --replicas=5，可以伸缩 Deployment，输出结果如下所示： 1234567891011121314[root@k8s-master 0402]# kubectl scale deployment nginx-deployment --replicas=5deployment.apps/nginx-deployment scaled[root@k8s-master 0402]# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 4/5 5 4 5h59m[root@k8s-master 0402]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-deployment-54f57cf6bf-4kl6r 1/1 Running 0 5h59mnginx-deployment-54f57cf6bf-7zc2j 1/1 Running 0 5h59mnginx-deployment-54f57cf6bf-lch2d 1/1 Running 0 35snginx-deployment-54f57cf6bf-mzrzq 1/1 Running 0 35snginx-deployment-54f57cf6bf-pmgj8 0/1 ImagePullBackOff 0 3h22m 如果集群启用了自动伸缩，执行以下命令，就可以基于 CPU 的利用率在一个最大和最小的区间自动伸缩您的 Deployment： 1kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80 按比例伸缩例如，假设已经运行了一个 10 副本数的 Deployment，其 maxSurge=3, maxUnavailable=2。 执行命令 kubectl scale deployment.v1.apps/nginx-deployment --replicas=15，将 Deployment 的 replicas 调整到 15。此时，Deployment Controller 需要决定如何分配新增的 5 个 Pod 副本。根据“按比例伸缩”的原则： 更大比例的新 Pod 数被分配到副本数最多的 ReplicaSet 更小比例的新 Pod 数被分配到副本数最少的 ReplicaSet 如果还有剩余的新 Pod 数未分配，则将被增加到副本数最多的 ReplicaSet 副本数为 0 的 ReplicaSet，scale up 之后，副本数仍然为 0 在本例中，3 个新副本被添加到旧的 ReplicaSet，2个新副本被添加到新的 ReplicaSet。如果新的副本都达到就绪状态，滚动更新过程最终会将所有的副本数添加放到新 ReplicaSet。 六、暂停和继续 Deployment可以先暂停 Deployment，然后再触发一个或多个更新，最后再继续（resume）该 Deployment。这种做法使得在暂停和继续中间对 Deployment 做多次更新，而无需触发不必要的滚动更新。 执行命令 kubectl rollout pause deployment.v1.apps/nginx-deployment 暂停 Deployment，输出结果如下所示： 12[root@k8s-master 0402]# kubectl rollout pause deployment nginx-deployment deployment.apps&#x2F;nginx-deployment paused 执行命令 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.17.9，更新 Deployment 的容器镜像，输出结果如下所示： 12[root@k8s-master 0402]# kubectl set image deployment&#x2F;nginx-deployment nginx&#x3D;nginx:1.17.9deployment.apps&#x2F;nginx-deployment image updated 执行命令 kubectl rollout resume deployment/nginx-deployment，继续（resume）该 Deployment，可使前面所有的变更一次性生效，输出结果如下所示： 12[root@k8s-master 0402]# kubectl rollout resume deployment nginx-deployment deployment.apps&#x2F;nginx-deployment resumed 执行命令 kubectl get rs -o wide 查看 ResultSet 的最终状态，输出结果如下所示： 123456[root@k8s-master 0402]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORmyapp-57c9b8fc4 2 2 2 3d nginx nginx app&#x3D;myapp,pod-template-hash&#x3D;57c9b8fc4nginx-deployment-54f57cf6bf 1 1 1 23h nginx nginx:1.7.9 app&#x3D;nginx,pod-template-hash&#x3D;54f57cf6bfnginx-deployment-56f8998dbc 0 0 0 23h nginx nginx:1.9.1 app&#x3D;nginx,pod-template-hash&#x3D;56f8998dbcnginx-deployment-6654964744 5 5 3 5m8s nginx nginx:1.17.9 app&#x3D;nginx,pod-template-hash&#x3D;6654964744 不能回滚（rollback）一个已暂停的 Deployment，除非继续（resume）该 Deployment。 七、查看Deployment的状态Progressing 状态当如下任何一个任务正在执行时，Kubernete 将 Deployment 的状态标记为 progressing： Deployment 创建了一个新的 ReplicaSet Deployment 正在 scale up 其最新的 ReplicaSet Deployment 正在 scale down 其旧的 ReplicaSet 新的 Pod 变为 就绪（ready） 或 可用（available） 可以使用命令 kubectl rollout status 监控 Deployment 滚动更新的过程 Complete 状态如果 Deployment 符合以下条件，Kubernetes 将其状态标记为 complete： 该 Deployment 中的所有 Pod 副本都已经被更新到指定的最新版本 该 Deployment 中的所有 Pod 副本都处于 可用（available） 状态 该 Deployment 中没有旧的 ReplicaSet 正在运行 可以执行命令 kubectl rollout status 检查 Deployment 是否已经处于 complete 状态。如果是，则该命令的退出码为 0。 例如，执行命令 kubectl rollout status deployment.v1.apps/nginx-deployment，输出结果如下所示： Failed 状态Deployment 在更新其最新的 ReplicaSet 时，可能卡住而不能达到 complete 状态。如下原因都可能导致此现象发生： 集群资源不够 就绪检查（readiness probe）失败 镜像抓取失败 权限不够 资源限制 应用程序的配置错误导致启动失败 指定 Deployment 定义中的 .spec.progressDeadlineSeconds 字段，Deployment Controller 在等待指定的时长后，将 Deployment 的标记为处理失败。 操作处于 Failed 状态的 Deployment可以针对 Failed 状态下的 Deployment 执行任何适用于 Deployment 的指令，例如： scale up / scale down 回滚到前一个版本 暂停（pause）Deployment，以对 Deployment 的 Pod template 执行多处更新 八、清除策略通过 Deployment 中 .spec.revisionHistoryLimit 字段，可指定为该 Deployment 保留多少个旧的 ReplicaSet。超出该数字的将被在后台进行垃圾回收。该字段的默认值是 10。 如果该字段被设为 0，Kubernetes 将清理掉该 Deployment 的所有历史版本（revision），将无法对该 Deployment 执行回滚操作 kubectl rollout undo。 九、部署策略 通过 Deployment 中 .spec.strategy 字段，可以指定使用 滚动更新 RollingUpdate 的部署策略还是使用 重新创建 Recreate 的部署策略 其中字段的含义如下： 字段名称 可选值 字段描述 类型 滚动更新 重新创建 如果选择重新创建，Deployment将先删除原有副本集中的所有 Pod，然后再创建新的副本集和新的 Pod。如此，更新过程中将出现一段应用程序不可用的情况； 最大超出副本数 数字或百分比 滚动更新过程中，可以超出期望副本数的最大值。 该取值可以是一个绝对值（例如：5），也可以是一个相对于期望副本数的百分比（例如：10%）； 如果填写百分比，则以期望副本数乘以该百分比后向上取整的方式计算对应的绝对值； 当最大超出副本数 maxUnavailable 为 0 时，此数值不能为 0；默认值为 25%。 例如：假设此值被设定为 30%，当滚动更新开始时，新的副本集（ReplicaSet）可以立刻扩容， 但是旧 Pod 和新 Pod 的总数不超过 Deployment 期待副本数（spec.repilcas）的 130%。 一旦旧 Pod 被终止后，新的副本集可以进一步扩容，但是整个滚动更新过程中，新旧 Pod 的总 数不超过 Deployment 期待副本数（spec.repilcas）的 130%。 最大不可用副本数 数字或百分比 滚动更新过程中，不可用副本数的最大值。 该取值可以是一个绝对值（例如：5），也可以是一个相对于期望副本数的百分比（例如：10%）； 如果填写百分比，则以期望副本数乘以该百分比后向下取整的方式计算对应的绝对值； 当最大超出副本数 maxSurge 为 0 时，此数值不能为 0；默认值为 25%； 例如：假设此值被设定为 30%，当滚动更新开始时，旧的副本集（ReplicaSet）可以缩容到期望 副本数的 70%；在新副本集扩容的过程中，一旦新的 Pod 已就绪，旧的副本集可以进一步缩容， 整个滚动更新过程中，确保新旧就绪副本数之和不低于期望副本数的 70%。 十、金丝雀发布（灰度发布）可以使用 Deployment 将最新的应用程序版本发布给一部分用户（或服务器），为每个版本创建一个 Deployment，此时，应用程序的新旧两个版本都可以同时获得生产上的流量。 部署第一个版本 第一个版本的 Deployment 包含了 3 个Pod副本，Service 通过 label selector app: nginx 选择对应的 Pod，nginx 的标签为 1.7.9 123456789101112131415161718192021222324252627282930313233343536apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9---apiVersion: v1kind: Servicemetadata: name: nginx-service labels: app: nginxspec: selector: app: nginx ports: - name: nginx-port protocol: TCP port: 80 nodePort: 32600 targetPort: 80 type: NodePort 假设此时想要发布新的版本 nginx 1.8.0，可以创建第二个 Deployment： 12345678910111213141516171819202122apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment-canary labels: app: nginx track: canaryspec: replicas: 1 selector: matchLabels: app: nginx track: canary template: metadata: labels: app: nginx track: canary spec: containers: - name: nginx image: nginx:1.8.0 因为 Service 的LabelSelector 是 app: nginx，由 nginx-deployment 和 nginx-deployment-canary 创建的 Pod 都带有标签 app: nginx，所以，Service 的流量将会在两个 release 之间分配 在新旧版本之间，流量分配的比例为两个版本副本数的比例，此处为 1:3 局限性按照 Kubernetes 默认支持的这种方式进行金丝雀发布，有一定的局限性： 不能根据用户注册时间、地区等请求中的内容属性进行流量分配 同一个用户如果多次调用该 Service，有可能第一次请求到了旧版本的 Pod，第二次请求到了新版本的 Pod 在 Kubernetes 中不能解决上述局限性的原因是：Kubernetes Service 只在 TCP 层面解决负载均衡的问题，并不对请求响应的消息内容做任何解析和识别。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"控制器 - ReplicaSet","slug":"控制器-ReplicaSet","date":"2020-04-01T06:57:39.000Z","updated":"2020-04-07T08:18:57.740Z","comments":true,"path":"2020/04/01/控制器-ReplicaSet/","link":"","permalink":"http://yoursite.com/2020/04/01/%E6%8E%A7%E5%88%B6%E5%99%A8-ReplicaSet/","excerpt":"控制器概述及控制器-ReplicaSet","text":"控制器概述及控制器-ReplicaSet 控制器_概述概述Pod（容器组）是 Kubernetes 中最小的调度单元，可以通过 kubectl 直接创建一个 Pod。Pod 本身并不能自愈（self-healing）。如果一个 Pod 所在的 Node （节点）出现故障，或者调度程序自身出现故障，Pod 将被删除；同理，当因为节点资源不够或节点维护而驱逐 Pod 时，Pod 也将被删除。 Kubernetes 通过引入 Controller（控制器）的概念来管理 Pod 实例。在 Kubernetes 中，创建 Pod 始终应该用 Controller 来创建 Pod，而不是直接创建 Pod。 Pod控制器是用于实现管理 Pod 的中间层，确保 Pod 资源符合预期的状态，Pod 的资源出现故障时，会尝试进行重启，当根据重启策略无效，则会重新新建 Pod 的资源.。 控制器可以提供如下特性： 水平扩展（运行 Pod 的多个副本） rollout（版本更新） self-healing（故障恢复） 例如：当一个节点出现故障，控制器可以自动地在另一个节点调度一个配置完全一样的 Pod，以替换故障节点上的 Pod。 控制器类型 控制器-ReplicaSet： 代用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能。ReplicaSet主要三个组件组成： （1）用户期望的pod副本数量 （2）标签选择器，判断哪个pod归自己管理 （3）当现存的pod数量不足，会根据pod资源模板进行新建帮助用户管理无状态的pod资源，精确反应用户定义的目标数量，但是RelicaSet不是直接使用的控制器，而是使用Deployment。 控制器-Deployment： 工作在ReplicaSet之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置。 控制器-StatefulSet： 周期性任务控制，不需要持续后台运行。 控制器-DaemonSet： 用于确保集群中的每一个节点只运行特定的pod副本，通常用于实现系统级后台任务。比如ELK服务特性：服务是无状态的，服务必须是守护进程。 控制器-Job： 只要完成就立即退出，不需要重启或重建。 控制器CronJob： 周期性任务控制，不需要持续后台运行。 控制器 - ReplicaSetKubernetes 中，ReplicaSet 用来维护一个数量稳定的 Pod 副本集合，可以保证某种定义一样的 Pod 始终有指定数量的副本数在运行。 ReplicaSet的工作方式ReplicaSet的定义中，包含： selector： 用于指定哪些 Pod 属于该 ReplicaSet 的管辖范围 replicas： 副本数，用于指定该 ReplicaSet 应该维持多少个 Pod 副本 template： Pod模板，在 ReplicaSet 使用 Pod 模板的定义创建新的 Pod ReplicaSet 控制器将通过创建或删除 Pod，以使得当前 Pod 数量达到 replicas 指定的期望值。ReplicaSet 创建的 Pod 中，都有一个字段 metadata.ownerReferences 用于标识该 Pod 从属于哪一个 ReplicaSet。 ReplicaSet 通过 selector 字段的定义，识别哪些 Pod 应该由其管理。如果 Pod 没有 ownerReference 字段，或者 ownerReference 字段指向的对象不是一个控制器，且该 Pod 匹配了 ReplicaSet 的 selector，则该 Pod 的 ownerReference 将被修改为 该 ReplicaSet 的引用。 何时使用 ReplicaSetReplicaSet 用来维护一个数量稳定的 Pod 副本集合。Deployment 可以管理 ReplicaSet，并提供声明式的更新等。因此，推荐用户总是使用 Deployment，而不是直接使用 ReplicaSet，除非需要一些自定义的更新应用程序的方式，或者完全不更新应用。 123456789101112131415161718192021apiVersion: apps/v1kind: ReplicaSetmetadata: name: frontend labels: app: guestbook tier: frontendspec: # modify replicas according to your case replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: nginx image: nginx ReplicaSet 副本集的主要几个字段有： selector 确定哪些 Pod 属于该副本集 replicas 副本集应该维护几个 Pod 副本（实例） template Pod 的定义 副本集将通过创建、删除 Pod 容器组来确保符合 selector 选择器的 Pod 数量等于 replicas 指定的数量。当符合 selector 选择器的 Pod 数量不够时，副本集通过使用 template 中的定义来创建 Pod。 执行命令以创建该 YAML 对应的 ReplicaSet 1234567891011121314151617181920212223242526272829303132333435[root@k8s-master k8s-yamls]# kubectl apply -f rs.yaml replicaset.apps&#x2F;frontend created[root@k8s-master k8s-yamls]# kubectl get rsNAME DESIRED CURRENT READY AGEfrontend 3 3 1 10s[root@k8s-master k8s-yamls]# kubectl describe rs&#x2F;frontendName: frontendNamespace: defaultSelector: tier&#x3D;frontendLabels: app&#x3D;guestbook tier&#x3D;frontendAnnotations: kubectl.kubernetes.io&#x2F;last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;apps&#x2F;v1&quot;,&quot;kind&quot;:&quot;ReplicaSet&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;app&quot;:&quot;guestbook&quot;,&quot;tier&quot;:&quot;frontend&quot;&#125;,&quot;name&quot;:&quot;frontend&quot;,...Replicas: 3 current &#x2F; 3 desiredPods Status: 2 Running &#x2F; 1 Waiting &#x2F; 0 Succeeded &#x2F; 0 FailedPod Template: Labels: tier&#x3D;frontend Containers: nginx: Image: nginx Port: &lt;none&gt; Host Port: &lt;none&gt; Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Events: Type Reason Age From Message---- ------ ---- ---- ------- Normal SuccessfulCreate 2m58s replicaset-controller Created pod: frontend-fsfrx Normal SuccessfulCreate 2m58s replicaset-controller Created pod: frontend-72vr4 Normal SuccessfulCreate 2m58s replicaset-controller Created pod: frontend-zp2k8 ReplicaSet的定义与其他 Kubernetes 对象一样，ReplicaSet需要的字段有： apiVersion：apps/v1 kind：始终为 ReplicaSet metadata spec： ReplicaSet 的详细定义 PodTemplate.spec.template 字段是一个 Pod Template，为必填字段，且其中必须定义 .spec.template.metadata.labels 字段。在前面的ReplicaSet例子中，定义了 label 为 tier: frontend。请小心该字段不要与其他控制器的 selector 重合，以免这些控制器尝试接管该 Pod。 1.spec.template.spec.restartPolicy&#96; 的默认值为 &#96;Always Pod Selector.spec.selector 字段为一个标签选择器，用于识别可以接管哪些 Pod。在前面的例子中，标签选择器为： 12matchLabels: tier: frontend 在 ReplicaSet 中， .spec.template.metadata.labels 必须与 .spec.selector 匹配，否则将不能成功创建 ReplicaSet。 如果两个 ReplicaSet 指定了相同的 .spec.selector 但是不同的 .spec.template.metadata.labels 和不同的 .spec.tempalte.spec 字段，两个 ReplicaSet 都将忽略另外一个 ReplicaSet 创建的 Pod Replicas.spec.replicas 字段用于指定同时运行的 Pod 的副本数。ReplicaSet 将创建或者删除由其管理的 Pod，以便使副本数与该字段指定的值匹配。 如果不指定，默认值为 1 使用 ReplicaSet删除ReplicaSet及其Pod使用 kubectl delete 可删除 ReplicaSet， Garbage Collector将自动删除该 ReplicaSet 所有从属的 Pod。 只删除ReplicaSet使用 kubectl delete --cascade=false 命令，可以删除 ReplicaSet，但是仍然保留其 Pod。 一旦原来的 ReplicaSet 被删除，可以创建新的 ReplicaSet 作为替代。只要新 ReplicaSet 的 .spec.selector 字段与旧 ReplicaSet 的 .spec.selector 字段相同，则新的 ReplicaSet 将接管旧 ReplicaSet 遗留下来的 Pod。但是，新的 ReplicaSet 中定义的 .spec.template 对遗留下来的 Pod 不会产生任何影响。 将Pod从ReplicaSet中隔离修改 Pod 的标签，可以使 Pod 脱离 ReplicaSet 的管理。这个小技巧在如下场景可能非常有用： 将 Pod 从 Service 中移除，以便 Debug 或者做数据恢复 通过这种方式从 ReplicaSet 移除了 Pod 之后，ReplicaSet 将立刻自动创建一个新的 Pod 以维持其指定的 replicas 副本数。 ReplicaSet的自动伸缩可以使用 Horizontal Pod Autoscalers(HPA) 对 ReplicaSet 执行自动的水平伸缩。下面例子中的 HPA 可以用来对前面例子中的 ReplicaSet 执行自动的水平伸缩： 1234567891011apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: frontend-scalerspec: scaleTargetRef: kind: ReplicaSet name: frontend minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 50 此外，也可以使用 kubectl autoscale 命令达到相同的效果： 1kubectl autoscale rs frontend --max=10","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"K8s应用健康状态","slug":"K8s应用健康状态","date":"2020-04-01T06:10:31.000Z","updated":"2020-04-01T06:52:54.190Z","comments":true,"path":"2020/04/01/K8s应用健康状态/","link":"","permalink":"http://yoursite.com/2020/04/01/K8s%E5%BA%94%E7%94%A8%E5%81%A5%E5%BA%B7%E7%8A%B6%E6%80%81/","excerpt":"应用健康状态和常见应用异常；","text":"应用健康状态和常见应用异常； 应用健康状态应用健康状态-使用方式探测方式Liveness 指针和 Readiness 指针支持三种不同的探测方式： 第一种是 httpGet。它是通过发送 http Get 请求来进行判断的，当返回码是 200-399 之间的状态码时，标识这个应用是健康的； 第二种探测方式是 Exec。它是通过执行容器中的一个命令来判断当前的服务是否是正常的，当命令行的返回结果是 0，则标识容器是健康的； 第三种探测方式是 tcpSocket 。它是通过探测容器的 IP 和 Port 进行 TCP 健康检查，如果这个 TCP 的链接能够正常被建立，那么标识当前这个容器是健康的。 探测结果从探测结果来讲主要分为三种： 第一种是 success，当状态是 success 的时候，表示 container 通过了健康检查，也就是 Liveness probe 或 Readiness probe 是正常的一个状态； 第二种是 Failure，Failure 表示的是这个 container 没有通过健康检查，如果没有通过健康检查的话，那么此时就会进行相应的一个处理，那在 Readiness 处理的一个方式就是通过 service。service 层将没有通过 Readiness 的 pod 进行摘除，而 Liveness 就是将这个 pod 进行重新拉起，或者是删除。 第三种状态是 Unknown，Unknown 是表示说当前的执行的机制没有进行完整的一个执行，可能是因为类似像超时或者像一些脚本没有及时返回，那么此时 Readiness-probe 或 Liveness-probe 会不做任何的一个操作，会等待下一次的机制来进行检验。 那在 kubelet 里面有一个叫 ProbeManager 的组件，这个组件里面会包含 Liveness-probe 或 Readiness-probe，这两个 probe 会将相应的 Liveness 诊断和 Readiness 诊断作用在 pod 之上，来实现一个具体的判断。 应用健康状态-Pod Probe Specexec：如下图所示，可以看到这是一个 Liveness probe，它里面配置了一个 exec 的一个诊断。接下来，它又配置了一个 command 的字段，这个 command 字段里面通过 cat 一个具体的文件来判断当前 Liveness probe 的状态，当这个文件里面返回的结果是 0 时，或者说这个命令返回是 0 时，它会认为此时这个 pod 是处在健康的一个状态。 httpGet：httpGet 里面有一个字段是路径，第二个字段是 port，第三个是 headers。这个地方有时需要通过类似像 header 头的一个机制做 health 的一个判断时，需要配置这个 header，通常情况下，可能只需要通过 health 和 port 的方式就可以了。 tcpSocket：tcpSocket 的使用方式其实也比较简单，只需要设置一个检测的端口，像下图使用的是 8080 端口，当这个 8080 端口 tcp connect 审核正常被建立的时候，那 tecSocket，Probe 会认为是健康的一个状态。 此外还有如下的五个参数，是 Global 的参数。 initialDelaySeconds：它表示的是说这个 pod 启动延迟多久进行一次检查，比如说现在有一个 Java 的应用，它启动的时间可能会比较长，因为涉及到 jvm 的启动，包括 Java 自身 jar 的加载。所以前期，可能有一段时间是没有办法被检测的，而这个时间又是可预期的，那这时可能要设置一下 initialDelaySeconds； periodSeconds：它表示的是检测的时间间隔，正常默认的这个值是 10 秒； timeoutSeconds：它表示的是检测的超时时间，当超时时间之内没有检测成功，那它会认为是失败的一个状态； successThreshold：它表示的是：当这个 pod 从探测失败到再一次判断探测成功，所需要的阈值次数，默认情况下是 1 次，表示原本是失败的，那接下来探测这一次成功了，就会认为这个 pod 是处在一个探针状态正常的一个状态； failureThreshold：它表示的是探测失败的重试次数，默认值是 3，表示的是当从一个健康的状态连续探测 3 次失败，那此时会判断当前这个pod的状态处在一个失败的状态。 应用健康状态-Liveness 与 Readiness 总结 Liveness Readness 介绍 用于判断容器是否存活，即Pod状态是否为Running，如果Liveness指针判断容器不健康则会触发kubelet杀掉容器，并根据配置的策略判断是否重启容器，如果默认不配置Liveness探针，则任务返回值默认为成功 用于判断容器是否启动完成，即Pod的Condition是否为Ready，如果探测结果不成功，则会将Pod从Endpoint中移除，直至下次判断成功，再将Pod挂回到Endpoint上。 检测失败 杀掉Pod 切断上层流量到Pod 适用场景 支持重新拉起的应用 启动后无法立即对外服务的应用 注意事项 不论是Liveness还是Readness探针，选择适合的探测方式可以防止被误操作 1.调大判断的差事阙值，防止在容器压力较高的情况下出现偶发超时 2.调整判断的次数阙值，3次的默认值在短周期下不一定是最佳实践 3.exec的如果执行的是shell脚本判断，在容器中可能调用时间会非常长 4.使用tcpSocket的方式遇到TLS的场景，需要业务层判断是否有影响 应用故障排查-了解状态机制因为 K8S 的设计是面向状态机的，它里面通过 yaml 的方式来定义的是一个期望到达的一个状态，而真正这个 yaml 在执行过程中会由各种各样的 controller来负责整体的状态之间的一个转换。 Phase 描述 Pending Kubernetes 已经创建并确认该 Pod。此时可能有两种情况：Pod 还未完成调度（例如没有合适的节点）正在从 docker registry 下载镜像 Running 该 Pod 已经被绑定到一个节点，并且该 Pod 所有的容器都已经成功创建。其中至少有一个容器正在运行，或者正在启动/重启 Succeeded Pod 中的所有容器都已经成功终止，并且不会再被重启 Failed Pod 中的所有容器都已经终止，至少一个容器终止于失败状态：容器的进程退出码不是 0，或者被系统 kill Unknown 因为某些未知原因，不能确定 Pod 的状态，通常的原因是 master 与 Pod 所在节点之间的通信故障 用故障排查-常见应用异常pod 上面可能会停留几个常见的状态。 Pod 停留在 Pending第一个就是 pending 状态，pending 表示调度器没有进行介入。此时可以通过 kubectl describe pod 来查看相应的事件，如果由于资源或者说端口占用，或者是由于 node selector 造成 pod 无法调度的时候，可以在相应的事件里面看到相应的结果，这个结果里面会表示说有多少个不满足的 node，有多少是因为 CPU 不满足，有多少是由于 node 不满足，有多少是由于 tag 打标造成的不满足。 Pod 停留在 waiting那第二个状态就是 pod 可能会停留在 waiting 的状态，pod 的 states 处在 waiting 的时候，通常表示说这个 pod 的镜像没有正常拉取，原因可能是由于这个镜像是私有镜像，但是没有配置 Pod secret；那第二种是说可能由于这个镜像地址是不存在的，造成这个镜像拉取不下来；还有一个是说这个镜像可能是一个公网的镜像，造成镜像的拉取失败。 Pod 不断被拉取并且可以看到 crashing第三种是 pod 不断被拉起，而且可以看到类似像 backoff 。这个通常表示说 pod 已经被调度完成了，但是启动失败，那这个时候通常要关注的应该是这个应用自身的一个状态，并不是说配置是否正确、权限是否正确，此时需要查看的应该是 pod 的具体日志。 Pod 处在 Runing 但是没有正常工作第四种 pod 处在 running 状态，但是没有正常对外服务。那此时比较常见的一个点就可能是由于一些非常细碎的配置，类似像有一些字段可能拼写错误，造成了 yaml 下发下去了，但是有一段没有正常地生效，从而使得这个 pod 处在 running 的状态没有对外服务，那此时可以通过 apply-validate-f pod.yaml 的方式来进行判断当前 yaml 是否是正常的，如果 yaml 没有问题，那么接下来可能要诊断配置的端口是否是正常的，以及 Liveness 或 Readiness 是否已经配置正确。 Service 无法正常的工作最后一种就是 service 无法正常工作的时候，该怎么去判断呢？那比较常见的 service 出现问题的时候，是自己的使用上面出现了问题。因为 service 和底层的 pod 之间的关联关系是通过 selector 的方式来匹配的，也就是说 pod 上面配置了一些 label，然后 service 通过 match label 的方式和这个 pod 进行相互关联。如果这个 label 配置的有问题，可能会造成这个 service 无法找到后面的 endpoint，从而造成相应的 service 没有办法对外提供服务，那如果 service 出现异常的时候，第一个要看的是这个 service 后面是不是有一个真正的 endpoint，其次来看这个 endpoint 是否可以对外提供正常的服务。 开源的调试工具 - kubectl-debug kubectl-debug 这个工具是依赖于 Linux namespace 的方式来去做的，它可以 datash 一个 Linux namespace 到一个额外的 container，然后在这个 container 里面执行任何的 debug 动作，其实和直接去 debug 这个 Linux namespace 是一致的。 总结 关于 Liveness 和 Readiness 的指针。Liveness probe 就是保活指针，它是用来看 pod 是否存活的，而 Readiness probe 是就绪指针，它是判断这个 pod 是否就绪的，如果就绪了，就可以对外提供服务。这个就是 Liveness 和 Readiness 需要记住的部分； 应用诊断的三个步骤：首先 describe 相应的一个状态；然后提供状态来排查具体的一个诊断方向；最后来查看相应对象的一个 event 获取更详细的一个信息； 提供 pod 一个日志来定位应用的自身的一个状态； 远程调试的一个策略，如果想把本地的应用代理到远程集群，此时可以通过 Telepresence 这样的工具来实现，如果想把远程的应用代理到本地，然后在本地进行调用或者是调试，可以用类似像 port-forward 这种机制来实现","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"K8s容器组 Pod","slug":"K8s容器组","date":"2020-03-31T07:06:33.000Z","updated":"2020-04-01T06:07:02.600Z","comments":true,"path":"2020/03/31/K8s容器组/","link":"","permalink":"http://yoursite.com/2020/03/31/K8s%E5%AE%B9%E5%99%A8%E7%BB%84/","excerpt":"Pod的相关理解","text":"Pod的相关理解 一、Pod 容器组_概述什么是 Pod 容器组Pod（容器组）是 Kubernetes 中最小的可部署单元。 一个 Pod（容器组）包含了一个应用程序容器（某些情况下是多个容器）、存储资源、一个唯一的网络 IP 地址、以及一些确定容器该如何运行的选项。 Pod 容器组代表了 Kubernetes 中一个独立的应用程序运行实例，该实例可能由单个容器或者几个紧耦合在一起的容器组成。 Kubernetes 集群中的 Pod 存在如下两种使用途径： 一个 Pod 中只运行一个容器。”one-container-per-pod” 是 Kubernetes 中最常见的使用方式。Kubernetes 通过 Pod 管理容器，而不是直接管理容器。 一个 Pod 中运行多个需要互相协作的容器。可以将多个紧密耦合、共享资源且始终在一起运行的容器编排在同一个 Pod 中。 每一个 Pod 都是用来运行某一特定应用程序的一个实例。如果想要水平扩展应用程序（运行多个实例），运行多个 Pod 容器组，每一个代表应用程序的一个实例。 Kubernetes 中，称其为 replication（复制副本）。 Kubernetes 中 Controller（控制器）负责为应用程序创建和管理这些复制的副本。 Pod 如何管理多个容器Pod 的设计目的是用来支持多个互相协同的容器，使得形成一个有意义的服务单元。一个 Pod 中的多个容器很自然就可以随 Pod 被一起调度到集群中的同一个物理机或虚拟机上。Pod 中的容器可以： 共享资源、依赖 互相通信 相互协商何时以何种方式结束运行 Pod 为其成员容器提供了两种类型的共享资源：网络和存储 网络 Networking 每一个 Pod 被分配一个独立的 IP 地址。Pod 中的所有容器共享一个网络名称空间： ​ 同一个 Pod 中的所有容器 IP 地址都相同 ​ 同一个 Pod 中的不同容器不能使用相同的端口，否则会导致端口冲突 ​ 同一个 Pod 中的不同容器可以通过 localhost:port 进行通信 ​ 同一个 Pod 中的不同容器可以通过使用常规的进程间通信手段，例如 SystemV semaphores 或者 POSIX 共享内存 存储 Storage Pod 中可以定义一组共享的数据卷。Pod 中所有的容器都可以访问这些共享数据卷，以便共享数据。Pod 中数据卷的数据也可以存储持久化的数据，使得容器在重启后仍然可以访问到之前存入到数据卷中的数据。 不同 Pod 上的两个容器如果要通信，必须使用对方 Pod 的 IP 地址 + 对方容器的端口号进行网络通信 使用 Pod 在 Pod 被创建后（直接创建，或者间接通过 Controller 创建），将被调度到集群中的一个节点上运行。Pod 将一直保留在该节点上，直到 Pod 以下情况发生： Pod 中的容器全部结束运行 Pod 被删除 由于节点资源不够，Pod 被驱逐 节点出现故障（例如死机） Pod 本身并不会运行，Pod 仅仅是容器运行的一个环境 Pod 本身并不能自愈（self-healing）。如果一个 Pod 所在的 Node （节点）出现故障，或者调度程序自身出现故障，Pod 将被删除；同理，当因为节点资源不够或节点维护而驱逐 Pod 时，Pod 也将被删除。Kubernetes 通过引入 Controller（控制器）的概念来管理 Pod 实例。在 Kubernetes 中，更为推荐的做法是使用 Controller 来管理 Pod，而不是直接创建 Pod。 容器组和控制器用户应该始终使用控制器来创建 Pod，而不是直接创建 Pod，控制器可以提供如下特性： 水平扩展（运行 Pod 的多个副本） rollout（版本更新） self-healing（故障恢复） 例如：当一个节点出现故障，控制器可以自动地在另一个节点调度一个配置完全一样的 Pod，以替换故障节点上的 Pod。 在 Kubernetes 中，广泛使用的控制器有： Deployment StatefulSet DaemonSet Termination of Pods（终止Pod）Pod 代表了运行在集群节点上的进程，而进程的终止有两种方式： gracefully terminate （优雅地终止） 直接 kill，此时进程没有机会执行清理动作 当用户发起删除 Pod 的指令时，Kubernetes 需要： 让用户知道 Pod 何时被删除 确保删除 Pod 的指令最终能够完成 Kubernetes 收到用户删除 Pod 的指令后： 记录强制终止前的等待时长（grace period） 向 Pod 中所有容器的主进程发送 TERM 信号 一旦等待超时，向超时的容器主进程发送 KILL 信号 删除 Pod 在 API Server 中的记录 默认情况下，删除 Pod 的 grace period（等待时长）是 30 秒。 可以通过 kubectl delete 命令的选项 --grace-period= 自己指定 grace period（等待时长）。 强制删除 Pod，必须为 kubectl delete 命令同时指定两个选项 --grace-period=0 和 --force 二、Pod 容器组_声明周期Pod phase Phase 描述 Pending Kubernetes 已经创建并确认该 Pod。此时可能有两种情况：Pod 还未完成调度（例如没有合适的节点）正在从 docker registry 下载镜像 Running 该 Pod 已经被绑定到一个节点，并且该 Pod 所有的容器都已经成功创建。其中至少有一个容器正在运行，或者正在启动/重启 Succeeded Pod 中的所有容器都已经成功终止，并且不会再被重启 Failed Pod 中的所有容器都已经终止，至少一个容器终止于失败状态：容器的进程退出码不是 0，或者被系统 kill Unknown 因为某些未知原因，不能确定 Pod 的状态，通常的原因是 master 与 Pod 所在节点之间的通信故障 容器的检查Probe 是指 kubelet 周期性地检查容器的状况。 有三种类型的 Probe： ExecAction： 在容器内执行一个指定的命令。如果该命令的退出状态码为 0，则成功 TCPSocketAction： 探测容器的指定 TCP 端口，如果该端口处于 open 状态，则成功 HTTPGetAction： 探测容器指定端口/路径上的 HTTP Get 请求，如果 HTTP 响应状态码在 200 到 400（不包含400）之间，则成功 Probe 有三种可能的结果： Success： 容器通过检测 Failure： 容器未通过检测 Unknown： 检测执行失败，此时 kubelet 不做任何处理 Kubelet 可以在两种情况下对运行中的容器执行 Probe： 就绪检查 readinessProbe： 确定容器是否已经就绪并接收服务请求。如果就绪检查失败，kubernetes 将该 Pod 的 IP 地址从所有匹配的 Service 的资源池中移除掉。 健康检查 livenessProbe： 确定容器是否正在运行。如果健康检查失败，kubelete 将结束该容器，并根据 restart policy（重启策略）确定是否重启该容器。 何时使用 健康检查/就绪检查？ 如果容器中的进程在碰到问题时可以自己 crash，并不需要执行健康检查；kubelet 可以自动的根据 Pod 的 restart policy（重启策略）执行对应的动作 如果希望在容器的进程无响应后，将容器 kill 掉并重启，则指定一个健康检查 liveness probe，并同时指定 restart policy（重启策略）为 Always 或者 OnFailure 如果想在探测 Pod 确实就绪之后才向其分发服务请求，请指定一个就绪检查 readiness probe。此时，就绪检查的内容可能和健康检查相同。就绪检查适合如下几类容器： 初始化时需要加载大量的数据、配置文件 启动时需要执行迁移任务 容器的状态一旦 Pod 被调度到节点上，kubelet 便开始使用容器引擎（通常是 docker）创建容器。容器有三种可能的状态：Waiting / Running / Terminated： Waiting： 容器的初始状态。处于 Waiting 状态的容器，仍然有对应的操作在执行，例如：拉取镜像、应用 Secrets等。 Running： 容器处于正常运行的状态。容器进入 Running 状态之后，如果指定了 postStart hook，该钩子将被执行。 Terminated： 容器处于结束运行的状态。容器进入 Terminated 状态之前，如果指定了 preStop hook，该钩子将被执行。 重启策略定义 Pod 或工作负载时，可以指定 restartPolicy，可选的值有： Always （默认值） OnFailure Never restartPolicy 将作用于 Pod 中的所有容器。kubelete 将在五分钟内，按照递延的时间间隔（10s, 20s, 40s ……）尝试重启已退出的容器，并在十分钟后再次启动这个循环，直到容器成功启动，或者 Pod 被删除。 容器组的存活期通常，如果没有人或者控制器删除 Pod，Pod 不会自己消失。 只有一种例外，那就是 Pod 处于 Scucceeded 或 Failed 的 phase，并超过了垃圾回收的时长（在 kubernetes master 中通过 terminated-pod-gc-threshold 参数指定），kubelet 自动将其删除。 三、Pod 容器组_初始化容器初始化容器的行为 Pod 的启动时，首先初始化网络和数据卷，然后按顺序执行每一个初始化容器。任何一个初始化容器都必须成功退出，才能开始下一个初始化容器。如果某一个容器启动失败或者执行失败，kubelet 将根据 Pod 的 restartPolicy 决定是否重新启动 Pod。 只有所有的初始化容器全都执行成功，Pod 才能进入 ready 状态。初始化容器的端口是不能够通过 kubernetes Service 访问的。Pod 在初始化过程中处于 Pending 状态，并且同时有一个 type 为 initializing status 为 True 的 Condition 如果 Pod 重启，所有的初始化容器也将被重新执行。 可以组合使用就绪检查和 activeDeadlineSeconds ，以防止初始化容器始终失败。 Pod 中不能包含两个同名的容器（初始化容器和工作容器也不能同名）。 Pod 重启的原因Pod 重启时，所有的初始化容器都会重新执行，Pod 重启的原因可能有： 用户更新了 Pod 的定义，并改变了初始化容器的镜像 改变任何一个初始化容器的镜像，将导致整个 Pod 重启 改变工作容器的镜像，将只重启该工作容器，而不重启 Pod Pod 容器基础设施被重启（例如 docker engine），这种情况不常见，通常只有 node 节点的 root 用户才可以执行此操作 Pod 中所有容器都已经结束，restartPolicy 是 Always，且初始化容器执行的记录已经被垃圾回收，此时将重启整个 Pod 配置初始化容器Pod的配置文件如下 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: name: init-demospec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html # These containers are run during pod initialization initContainers: - name: install image: busybox command: - wget - \"-O\" - \"/work-dir/index.html\" - https://kuboard.cn volumeMounts: - name: workdir mountPath: \"/work-dir\" dnsPolicy: Default volumes: - name: workdir emptyDir: &#123;&#125; 从配置文件可以看出，Pod 中初始化容器和应用程序共享了同一个数据卷。初始化容器将该共享数据卷挂载到 /work-dir 路径，应用程序容器将共享数据卷挂载到 /usr/share/nginx/html 路径。初始化容器执行如下命令后，就退出执行： wget -O /work-dir/index.html https://kuboard.cn 验证： 12345[root@k8s-master k8s-yamls]# kubectl get pod init-demoNAME READY STATUS RESTARTS AGEinit-demo 1&#x2F;1 Running 0 22s[root@k8s-master k8s-yamls]# kubectl exec -it init-demo -- &#x2F;bin&#x2F;bashroot@init-demo:&#x2F;# apt-get update 四、Pod 容器组_Debug初始化容器查看 Pod 的状态： 12345kubectl get pod &lt;pod-name&gt;NAME READY STATUS RESTARTS AGE&lt;pod-name&gt; 0/1 Init:1/2 0 7s例如，状态如果是 Init:1/2，则表明了两个初始化容器当中的一个已经成功执行： 查看初始化容器的详情 1kubectl describe pod &lt;pod-name&gt; 理解 Pod 状态如果 Pod 的状态以 Init: 开头，表示该 Pod 正在执行初始化容器。下表描述了 Debug 初始化容器的过程中，一些可能出现的 Pod 状态： 状态 描述 Init:N/M Pod 中包含 M 个初始化容器，其中 N 个初始化容器已经成功执行 Init:Error Pod 中有一个初始化容器执行失败 Init:CrashLoopBackOff Pod 中有一个初始化容器反复执行失败 Pending Pod 还未开始执行初始化容器 PodInitializing or Running Pod 已经完成初始化容器的执行 五、Pod 容器组_配置PodDisruptionBudget​ 在Kubernetes中，为了保证业务不中断或业务SLA不降级，需要将应用进行集群化部署。通过PodDisruptionBudget控制器可以设置应用 Pod 集群处于运行状态最低个数，也可以设置应用Pod 集群处于运行状态的最低百分比，这样可以保证在主动销毁应用Pod的时候，不会一次性销毁太多的应用Pod，从而保证业务不中断或业务SLA不降级。 在Kubernetes 1.5中，kubectl drain命令已经支持了PodDisruptionBudget控制器，在进行kubectl drain操作时会根据PodDisruptionBudget控制器判断应用Pod集群数量，进而保证在业务不中断或业务SLA不降级的情况下进行应用Pod销毁。 确定需要PDB保护的应用通常如下几种 Kubernetes 控制器创建的应用程序可以使用 PDB： Deployment ReplicationController ReplicaSet StatefulSet PDB 中 .spec.selector 字段的内容必须与控制器中 .spec.selector 字段的内容相同。 当毁坏发生时，在短时间内，应用程序最多可以容许多少个实例被终止？ 无状态的前端： 关注点：不能让服务能力（serving capacity）降低超过 10% 解决方案：在 PDB 中配置 minAvailable 90% 单实例有状态应用： 关注点：未经同意不能关闭此应用程序 解决方案1： 不使用 PDB，并且容忍偶尔的停机 解决方案2： 在 PDB 中设置 maxUnavailable=0。与集群管理员达成一致（不是通过Kubernetes，而是邮件、电话或面对面），请集群管理员在终止应用之前与你沟通。当集群管理员联系你时，准备好停机时间，删除 PDB 以表示已准备好应对毁坏。并做后续处理 多实例有状态应用，例如 consul、zookeeper、etcd： 关注点：不能将实例数降低到某个数值，否则写入会失败 解决方案1： 在 PDB 中设置 maxUnavailable 为 1 （如果副本数会发生变化，可以使用此设置） 解决方案2： 在 PDB 中设置 minAvailable 为最低数量（例如，当总副本数为 5 时，设置为3）（可以同时容忍更多的毁坏数） 可以重新开始的批处理任务： 关注点：当发生自愿毁坏时，Job仍然需要完成其执行任务 解决方案： 不创建 PDB。Job 控制器将会创建一个 Pod 用于替换被毁坏的 Pod 指定百分比时的舍入逻辑minAvailable 或 maxUnavailable 可以指定为整数或者百分比。 当指定一个整数时，代表 Pod 的数量。例如，设置 minAvailable 为 10，则至少 10 个 Pod 必须始终可用，即便是在毁坏发生时 当指定一个百分比时（例如，50%），代表总 Pod 数量的一个百分比。例如，设置 maxUnavailable 为 50%，则最多可以有 50% 的 Pod 可以被毁坏 如果指定这些值为一个百分数，其计算结果可能不会正好是一个整数。例如，假设有 7 个 Pod，minAvailable 设置为 50%，你将很难判断，到底是 3 个还是 4 个 Pod 必须始终保持可用。Kubernetes 将向上舍入（round up to the nearest integer），因此，此处必须有 4 个 Pod 始终可用。 定义PodDisruptionBudgetPodDisruptionBudget 包含三个字段： 标签选择器 .spec.selector 用于指定 PDB 适用的 Pod。此字段为必填 .spec.minAvailable：当完成驱逐时，最少仍然要保留多少个 Pod 可用。该字段可以是一个整数，也可以是一个百分比 .spec.maxUnavailable： 当完成驱逐时，最多可以有多少个 Pod 被终止。该字段可以是一个整数，也可以是一个百分比 在一个 PodDisruptionBudget 中，只能指定 maxUnavailable 和 minAvailable 中的一个。 maxUnavailable 只能应用到那些有控制器的 Pod 上。下面的例子中，“期望的副本数” 是 PodDisruptionBudget 对应 Pod 的控制器的 .spec.replicas 字段： 例子1： minAvailable 为 5 时，只要 PodDisruptionBudget 的 selector 匹配的 Pod 中有超过 5 个仍然可用，就可以继续驱逐 Pod 例子2： minAvailable 为 30% 时，至少保证期望副本数的 30% 可用 例子3： maxUnavailable 为 5 时，最多可以有 5 个副本不可用（unthealthy） 例子4： maxUnavailable 为 30% 时，最多可以有期望副本数的 30% 不可用 通常，一个 PDB 对应一个控制器创建的 Pod，例如，Deployment、ReplicaSet或StatefulSet。 使用 minAvailable12345678910111213141516apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: zk-pdbspec: minAvailable: 2 selector: matchLabels: app: zookeeper [root@k8s-master k8s-yamls]# kubectl apply -f zk-pdb.yaml poddisruptionbudget.policy/zk-pdb created[root@k8s-master k8s-yamls]# kubectl get poddisruptionbudgetsNAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGEzk-pdb 2 N/A 0 26s 使用 maxUnavailable12345678910111213141516apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: zk-pdbspec: maxUnavailable: 1 selector: matchLabels: app: zookeeper [root@k8s-master k8s-yamls]# kubectl apply -f max_zk-pdb.yaml poddisruptionbudget.policy/zk-pdb configured[root@k8s-master k8s-yamls]# kubectl get poddisruptionbudgetsNAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGEzk-pdb N/A 1 0 98s","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":" Kubernetes容器","slug":"Kubernetes容器","date":"2020-03-28T06:58:54.000Z","updated":"2020-03-28T07:35:24.859Z","comments":true,"path":"2020/03/28/Kubernetes容器/","link":"","permalink":"http://yoursite.com/2020/03/28/Kubernetes%E5%AE%B9%E5%99%A8/","excerpt":"Kubernetes中容器的相关","text":"Kubernetes中容器的相关 一、容器容器镜像在 Kubernetes 的 Pod 中使用容器镜像之前，您必须将其推送到一个镜像仓库（或者使用仓库中已经有的容器镜像）。在 Kubernetes 的 Pod 定义中定义容器时，必须指定容器所使用的镜像，容器中的 image 字段支持与 docker 命令一样的语法，包括私有镜像仓库和标签。 例如：my-registry.example.com:5000/example/web-example:v1.0.1 由如下几个部分组成： my-registry.example.com:5000/example/web-example:v1.0.1 my-registry.example.com：registry 地址 :5000：registry 端口 example：repository 名字 web-example：image 名字 v1.0.1：image 标签 如果使用 hub.dokcer.com Registry 中的镜像，可以省略 registry 地址和 registry 端口。 例如：nginx:latest，eipwork/kuboard 更新镜像Kubernetes中，默认的镜像抓取策略是 IfNotPresent，使用此策略，kubelet在发现本机有镜像的情况下，不会向镜像仓库抓取镜像。如果想每次启动 Pod 时，都强制从镜像仓库抓取镜像，可以尝试如下方式： 设置 container 中的 imagePullPolicy 为 Always 省略 imagePullPolicy 字段，并使用 :latest tag 的镜像 省略 imagePullPolicy 字段和镜像的 tag 激活 AlwaysPullImages 管理控制器 imagePullPolicy 字段和 image tag的可能取值将影响到 kubelet 如何抓取镜像： imagePullPolicy: IfNotPresent 仅在节点上没有该镜像时，从镜像仓库抓取 imagePullPolicy: Always 每次启动 Pod 时，从镜像仓库抓取 imagePullPolicy 未填写，镜像 tag 为 :latest 或者未填写，则同 Always 每次启动 Pod 时，从镜像仓库抓取 imagePullPolicy 未填写，镜像 tag 已填写但不是 :latest，则同 IfNotPresent 仅在节点上没有该镜像时，从镜像仓库抓取 imagePullPolicy: Never，Kubernetes 假设本地存在该镜像，并且不会尝试从镜像仓库抓取镜像 二、容器的环境变量Kubernetes为容器提供了一系列重要的资源： 由镜像、一个或多个数据卷合并组成的文件系统 容器自身的信息 集群中其他重要对象的信息 集群的信息在容器创建时，集群中所有的 Service 的连接信息将以环境变量的形式注入到容器中。例如，已创建了一个名为 Foo 的 Service，此时再创建任何容器时，该容器将包含如下环境变量： 12FOO_SERVICE_HOST &#x3D; &lt;Service的ClusterIP&gt;FOO_SERVICE_PORT &#x3D; &lt;Service的端口&gt; 三、容器生命周期容器钩子Kubernetes中为容器提供了两个 hook（钩子函数）： PostStart 此钩子函数在容器创建后将立刻执行。但是，并不能保证该钩子函数在容器的 ENTRYPOINT 之前执行。该钩子函数没有输入参数。 PreStop 此钩子函数在容器被 terminate（终止）之前执行，例如： 通过接口调用删除容器所在 Pod 某些管理事件的发生：健康检查失败、资源紧缺等 如果容器已经被关闭或者进入了 completed 状态，preStop 钩子函数的调用将失败。该函数的执行是同步的，即，kubernetes 将在该函数完成执行之后才删除容器。该钩子函数没有输入参数。 Hook handler的实现容器只要实现并注册 hook handler 便可以使用钩子函数。Kubernetes 中，容器可以实现两种类型的 hook handler： Exec - 在容器的名称空进和 cgroups 中执行一个指定的命令，例如 pre-stop.sh。该命令所消耗的 CPU、内存等资源，将计入容器可以使用的资源限制。 HTTP - 向容器的指定端口发送一个 HTTP 请求 Hook handler的执行当容器的生命周期事件发生时，Kubernetes 在容器中执行该钩子函数注册的 handler。 对于 Pod 而言，hook handler 的调用是同步的。即，如果是 PostStart hook，容器的 ENTRYPOINT 和 hook 是同时出发的，然而如果 hook 执行的时间过长或者挂起了，容器将不能进入到 Running 状态。 PreStop hook 的行为与此相似。如果 hook 在执行过程中挂起了，Pod phase 将停留在 Terminating 的状态，并且在 terminationGracePeriodSeconds 超时之后，Pod被删除。如果 PostStart 或者 PreStop hook 执行失败，则 Kubernetes 将 kill（杀掉）该容器。 用户应该使其 hook handler 越轻量级越好。例如，对于长时间运行的任务，在停止容器前，调用 PreStop 钩子函数，以保存当时的计算状态和数据。 Hook触发的保证Hook 将至少被触发一次，即，当指定事件 PostStart 或 PreStop 发生时，hook 有可能被多次触发。hook handler 的实现需要保证即使多次触发，执行也不会出错。 通常来说，hook 实际值被触发一次。例如：如果 HTTP hook 的服务端已经停机，或者因为网络的问题不能接收到请求，请求将不会被再次发送。在极少数的情况下， 触发两次 hook 的事情会发生。例如，如果 kueblet 在触发 hook 的过程中重启了，该 hook 将在 Kubelet 重启后被再次触发。 调试 hook handlerHook handler 的日志并没有在 Pod 的 events 中发布。如果 handler 因为某些原因失败了，kubernetes 将广播一个事件 PostStart hook 发送 FailedPreStopHook 事件。 可以执行命令 kubectl describe pod $(pod_name) 以查看这些事件。 四、容器生命周期事件处理Kubernetes 中支持容器的 postStart 和 preStop 事件，本文阐述了如何向容器添加生命周期事件处理程序（handler）。 postStart 容器启动时，Kubernetes 立刻发送 postStart 事件，但不确保对应的 handler 是否能在容器的 EntryPoint 之前执行 preStop 容器停止前，Kubernetes 发送 preStop 事件 定义postStart和preStop处理程序创建一个包含单一容器的 Pod，并为该容器关联 postStart 和 preStop 处理程序（handler）。Pod 的yaml文件定义如下： 123456789101112131415apiVersion: v1kind: Podmetadata: name: lifecycle-demospec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler &gt; /usr/share/message\"] preStop: exec: command: [\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"] 在该例子中，请注意： postStart 命令向 usr/share/message 文件写入了一行文字 preStop 命令优雅地关闭了 nginx 如果容器碰到问题，被 Kubernetes 关闭，这个操作是非常有帮助的，可以使得您的程序在关闭前执行必要的清理任务 创建pod 12345[root@k8s-master k8s-yamls]# kubectl apply -f lifecycle-demo.yaml pod/lifecycle-demo created[root@k8s-master k8s-yamls]# kubectl get pod -ANAMESPACE NAME READY STATUS RESTARTS AGEdefault lifecycle-demo 1/1 Running 0 50s 进入容器的命令行终端： 123[root@k8s-master k8s-yamls]# kubectl exec -it lifecycle-demo -- /bin/bashroot@lifecycle-demo:/# cat /usr/share/messageHello from the postStart handler 总结Kubernetes 在容器启动后立刻发送 postStart 事件，但是并不能确保 postStart 事件处理程序在容器的 EntryPoint 之前执行。 postStart 事件处理程序相对于容器中的进程来说是异步的（同时执行），然而，Kubernetes 在管理容器时，将一直等到 postStart 事件处理程序结束之后，才会将容器的状态标记为 Running。 Kubernetes 在决定关闭容器时，立刻发送 preStop 事件，并且，将一直等到 preStop 事件处理程序结束或者 Pod 的 --grace-period 超时，才删除容器。 注意： Kubernetes 只在 Pod Teminated 状态时才发送 preStop 事件，这意味着，如果 Pod 已经进入了 Completed 状态， preStop 事件处理程序将不会被调用","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s标签和选择器","slug":"k8s标签和选择器","date":"2020-03-28T06:26:59.000Z","updated":"2020-03-28T06:56:51.605Z","comments":true,"path":"2020/03/28/k8s标签和选择器/","link":"","permalink":"http://yoursite.com/2020/03/28/k8s%E6%A0%87%E7%AD%BE%E5%92%8C%E9%80%89%E6%8B%A9%E5%99%A8/","excerpt":"标签和标签选择器的作用","text":"标签和标签选择器的作用 标签和选择器标签（Label）是附加在Kubernetes对象上的一组名值对，其意图是按照对用户有意义的方式来标识Kubernetes对象，同时，又不对Kubernetes的核心逻辑产生影响。 标签可以用来组织和选择一组Kubernetes对象。 可以在创建Kubernetes对象时为其添加标签，也可以在创建以后再为其添加标签。 使用标签（Label）可以高效地查询和监听Kubernetes对象 每个Kubernetes对象可以有多个标签，同一个对象的标签的 Key 必须唯一，例如： 1234metadata: labels: key1: value1 key2: value2 为什么要使用标签使用标签，用户可以按照自己期望的形式组织 Kubernetes 对象之间的结构，而无需对 Kubernetes 有任何修改。 应用程序的部署或者批处理程序的部署通常都是多维度的（例如，多个高可用分区、多个程序版本、多个微服务分层）。管理这些对象时，很多时候要针对某一个维度的条件做整体操作，例如，将某个版本的程序整体删除，这种情况下，如果用户能够事先规划好标签的使用，再通过标签进行选择，就会非常地便捷。 标签的例子有： release: stable、release: canary environment: dev、environment: qa、environment: production tier: frontend、tier: backend、tier: cache partition: customerA、partition: customerB track: daily、track: weekly 上面只是一些使用比较普遍的标签，可以根据您自己的情况建立合适的使用标签 句法和字符集 标签是一组名值对（key/value pair）。标签的 key 可以有两个部分：可选的前缀和标签名，通过 / 分隔。 例如，下面的例子中的Pod包含两个标签 environment: production 和 app:nginx 12345678910111213apiVersion: v1kind: Podmetadata: name: label-demo labels: environment: production app: nginxspec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 标签选择器与 name 和 UID 不同，标签不一定是唯一的。通常来讲，会有多个Kubernetes对象包含相同的标签。通过使用标签选择器（label selector），用户/客户端可以选择一组对象。标签选择器（label selector）是 Kubernetes 中最主要的分类和筛选手段。 Kubernetes api server支持两种形式的标签选择器，equality-based 基于等式的 和 set-based 基于集合的。标签选择器可以包含多个条件，并使用逗号分隔，此时只有满足所有条件的 Kubernetes 对象才会被选中。 如果使用空的标签选择器或者不指定选择器，其含义由具体的 API 接口决定。 基于等式的选择方式 Equality- 或者 inequality-based 选择器可以使用标签的名和值来执行过滤选择。只有匹配所有条件的对象才被选中（被选中的对象可以包含未指定的标签）。可以使用三种操作符 =、==、!=。前两个操作符含义是一样的，都代表相等，后一个操作符代表不相等。例如： 1234# 选择了标签名为 `environment` 且 标签值为 `production` 的Kubernetes对象environment = production# 选择了标签名为 `tier` 且标签值不等于 `frontend` 的对象，以及不包含标签 `tier` 的对象tier != frontend 也可以使用逗号分隔的两个等式 environment=production,tier!=frontend，此时将选中所有 environment 为 production 且 tier 不为 frontend 的对象。 以Pod 的节点选择器为例，下面的 Pod 可以被调度到包含标签 accelerator=nvidia-tesla-p100 的节点上： 12345678910111213apiVersion: v1kind: Podmetadata: name: cuda-testspec: containers: - name: cuda-test image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 基于集合的选择方式 Set-based 标签选择器可以根据标签名的一组值进行筛选。支持的操作符有三种：in、notin、exists。例如： 12345678# 选择所有的包含 `environment` 标签且值为 `production` 或 `qa` 的对象environment in (production, qa)# 选择所有的 `tier` 标签不为 `frontend` 和 `backend`的对象，或不含 `tier` 标签的对象tier notin (frontend, backend)# 选择所有包含 `partition` 标签的对象partition# 选择所有不包含 `partition` 标签的对象!partition 可以组合多个选择器，用 , 分隔，, 相当于 AND 操作符。例如： 12# 选择包含 `partition` 标签（不检查标签值）且 `environment` 不是 `qa` 的对象partition,environment notin (qa) 基于集合的选择方式是一个更宽泛的基于等式的选择方式，例如，environment=production 等价于 environment in (production)；environment!=production 等价于 environment notin (production)。 基于集合的选择方式可以和基于等式的选择方式可以混合使用，例如： partition in (customerA, customerB),environment!=qa API查询条件LIST 和 WATCH 操作时，可指定标签选择器作为查询条件，以筛选指定的对象集合。两种选择方式都可以使用，但是要符合 URL 编码，例如： 基于等式的选择方式： ?labelSelector=environment%3Dproduction,tier%3Dfrontend 基于集合的选择方式： ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29 两种选择方式都可以在 kubectl 的 list 和 watch 命令中使用，例如： 使用基于等式的选择方式 1kubectl get pods -l environment=production,tier=frontend 使用基于集合的选择方式 1kubectl get pods -l 'environment in (production),tier in (frontend)' Kubernetes对象引用某些 Kubernetes 对象中（例如，Service和Deployment），使用标签选择器指定一组其他类型的 Kubernetes 对象（例如，Pod） ServiceService 中通过 spec.selector 字段来选择一组 Pod，并将服务请求转发到选中的 Pod 上。 在 yaml 或 json 文件中，标签选择器用一个 map 来定义，且支持基于等式的选择方式，例如： 12345678\"selector\": &#123; \"component\" : \"redis\",&#125;或selector: component: redis 上面的例子中定义的标签选择器等价于 component=redis 或 component in (redis) 有些对象支持基于集合的选择方式Job、Deployment、ReplicaSet 和 DaemonSet 同时支持基于等式的选择方式和基于集合的选择方式。例如： 123456selector: matchLabels: component: redis matchExpressions: - &#123;key: tier, operator: In, values: [cache]&#125; - &#123;key: environment, operator: NotIn, values: [dev]&#125; matchLabels 是一个 {key,value} 组成的 map。map 中的一个 {key,value} 条目相当于 matchExpressions 中的一个元素，其 key 为 map 的 key，operator 为 In， values 数组则只包含 value 一个元素。matchExpression 等价于基于集合的选择方式，支持的 operator 有 In、NotIn、Exists 和 DoesNotExist。当 operator 为 In 或 NotIn 时，values 数组不能为空。所有的选择条件都以 AND 的形式合并计算，即所有的条件都满足才可以算是匹配 。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"使用名称空间共享集群","slug":"使用名称空间共享集群","date":"2020-03-28T05:42:22.000Z","updated":"2020-03-28T06:23:37.173Z","comments":true,"path":"2020/03/28/使用名称空间共享集群/","link":"","permalink":"http://yoursite.com/2020/03/28/%E4%BD%BF%E7%94%A8%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E5%85%B1%E4%BA%AB%E9%9B%86%E7%BE%A4/","excerpt":"查看、创建、删除、使用namespace","text":"查看、创建、删除、使用namespace 使用名称空间共享集群一、查看名称空间查看集群中的名称空间列表： 123456[root@k8s-master k8s-yamls]# kubectl get namespacesNAME STATUS AGEdefault Active 23hkube-node-lease Active 23hkube-public Active 23hkube-system Active 23h Kubernetes 安装成功后，默认有初始化了三个名称空间： default 默认名称空间，如果 Kubernetes 对象中不定义 metadata.namespace 字段，该对象将放在此名称空间下 kube-system Kubernetes系统创建的对象放在此名称空间下 kube-public 此名称空间自动在安装集群是自动创建，并且所有用户都是可以读取的（即使是那些未登录的用户）。主要是为集群预留的，例如，某些情况下，某些Kubernetes对象应该被所有集群用户看到。 查看名称空间详细信息： 123456789[root@k8s-master k8s-yamls]# kubectl describe namespaces kube-systemName: kube-systemLabels: &lt;none&gt;Annotations: &lt;none&gt;Status: ActiveNo resource quota.No resource limits. Resource quota 汇总了名称空间中使用的资源总量，并指定了集群管理员定义该名称空间最多可以使用的资源量 Limit range 定义了名称空间中某种具体的资源类型的最大、最小值 名称空间可能有两种状态（phase）： Active 名称空间正在使用中 Termining 名称空间正在被删除，不能再向其中创建新的对象 二、创建名称空间使用kubectl有两种方式创建名称空间 1.通过 yaml 文件，创建文件 my-namespace.yaml 1234567apiVersion: v1kind: Namespacemetadata: name: &lt;名称空间的名字&gt;#执行命令kubectl create -f ./my-namespace.yaml 直接使用命令创建名称空间： 1kubectl create namespace &lt;名称空间的名字&gt; 注意： 名称空间可以定义一个可选项字段 finalizers，在名称空间被删除时，用来清理相关的资源。 如果定义了一个不存在的 finalizer，仍然可以成功创建名称空间，但是当删除该名称空间时，将卡在 Terminating 状态。 三、删除名称空间1kubectl delete namespaces &lt;名称空间的名字&gt; 注意： 该操作将删除名称空间中的所有内容（ 此删除操作是异步的，名称空间会停留在 Terminating 状态一段时间。 ） 四、使用名称空间切分集群理解 default 名称空间默认情况下，安装Kubernetes集群时，会初始化一个 default 名称空间，用来将承载那些未指定名称空间的 Pod、Service、Deployment等对象 创建新的名称空间假设企业使用同一个集群作为开发环境和生产环境（注意：通常开发环境和生产环境是物理隔绝的）： 开发团队期望有一个集群中的空间，以便他们可以查看查看和使用他们创建的 Pod、Service、Deployment等。在此空间中，Kubernetes对象被创建又被删除，为了适应敏捷开发的过程，团队中的许多人都可以在此空间内做他们想做的事情。 运维团队也期望有一个集群中的空间，在这里，将有严格的流程控制谁可以操作 Pod、Service、Deployment等对象，因为这些对象都直接服务于生产环境。 此时，可以将一个Kubernetes集群切分成两个名称空间：development 和 production。创建名称空间的 yaml 文件如下所示： 123456apiVersion: v1kind: Namespacemetadata: name: development labels: name: development 执行命令以创建 development 名称空间： 12345678910[root@k8s-master k8s-yamls]# kubectl create -f dev.yaml namespace/development created[root@k8s-master k8s-yamls]# kubectl get namespaces --show-labelsNAME STATUS AGE LABELSdefault Active 23h &lt;none&gt;development Active 2m10s name=developmentkube-node-lease Active 23h &lt;none&gt;kube-public Active 23h &lt;none&gt;kube-system Active 23h &lt;none&gt;production Active 67s name=production 在每个名称空间中创建 PodKubernetes名称空间为集群中的 Pod、Service、Deployment 提供了一个作用域。可以限定使用某个名称空间的用户不能看到另外一个名称空间中的内容。我们可以在 development 名称空间中创建一个简单的 Deployment 和 Pod 来演示这个特性。 1.执行命令以检查当前的 kubectl 上下文 1234567891011121314151617181920[root@k8s-master k8s-yamls]# kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority-data: DATA+OMITTED server: https://192.168.154.144:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 2.执行命令 12[root@k8s-master k8s-yamls]# kubectl config current-contextkubernetes-admin@kubernetes 3.接下来，为 kubectl 定义一个上下文，以便在不同的名称空间中工作。cluster 和 user 字段的取值从前面的 current context 复制过来： 1234[root@k8s-master k8s-yamls]# kubectl config set-context dev --namespace=development --cluster=kubernetes-admin@kubernetes --user=kubernetes-admin@kubernetesContext \"dev\" created.[root@k8s-master k8s-yamls]# kubectl config set-context prod --namespace=production --cluster=kubernetes-admin@kubernetes --user=kubernetes-admin@kubernetesContext \"prod\" created. 上面的命令创建了两个 kubectl 的上下文，可以在两个不同的名称空间中工作： 切换到 development 名称空间： 123456[root@k8s-master k8s-yamls]# kubectl config use-context devSwitched to context \"dev\".#验证[root@k8s-master k8s-yamls]# kubectl config current-contextdev 此时，通过 kubectl 向 Kubernetes 集群发出的所有指令都限定在名称空间 development 里 在不同的namespace里工作创建一个 nginx 1234567891011kubectl run snowflake --image=nginx:1.7.9 --replicas=2#刚刚创建的 Deployment 副本数为 2，运行了一个 nginx 容器。kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEsnowflake 2 2 2 2 2mkubectl get pods -l run=snowflakeNAME READY STATUS RESTARTS AGEsnowflake-3968820950-9dgr8 1/1 Running 0 2msnowflake-3968820950-vgc4n 1/1 Running 0 2m 此时，开发人员可以做任何他想要做的操作，所有操作都限定在名称空间 development 里，而无需担心影响到 production 名称空间中的内容 用户在一个名称空间创建的内容对于另外一个名称空间来说是不可见的。同时也可以为不同的名称空间定义不同的访问权限控制。 为什么需要名称空间一个Kubernetes集群应该可以满足多组用户的不同需要。Kubernetes名称空间可以使不同的项目、团队或客户共享同一个 Kubernetes 集群。实现的方式是，提供： namespace的作用域 为不同的名称空间定义不同的授权方式和资源分配策略 Resource Quota 和 resource limit range 每一个用户组都期望独立于其他用户组进行工作。通过名称空间，每个用户组拥有自己的： Kubernetes 对象（Pod、Service、Deployment等） 授权（谁可以在该名称空间中执行操作） 资源分配（该用户组或名称空间可以使用集群中的多少计算资源） 可能的使用情况有： 集群管理员通过一个Kubernetes集群支持多个用户组 集群管理员将集群中某个名称空间的权限分配给用户组中的受信任的成员 集群管理员可以限定某一个用户组可以消耗的资源数量，以避免其他用户组受到影响 集群用户可以使用自己的Kubernetes对象，而不会与集群中的其他用户组相互干扰","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"操作Kubernetes","slug":"操作Kubernetes","date":"2020-03-28T02:24:30.000Z","updated":"2020-03-28T03:52:42.373Z","comments":true,"path":"2020/03/28/操作Kubernetes/","link":"","permalink":"http://yoursite.com/2020/03/28/%E6%93%8D%E4%BD%9CKubernetes/","excerpt":"什么是Kubernetes对象，管理Kubernetes对象，名称（name），名称空间（namespace）的概念","text":"什么是Kubernetes对象，管理Kubernetes对象，名称（name），名称空间（namespace）的概念 一、什么是Kubernetes对象​ Kubernetes对象指的是Kubernetes系统的持久化实体，所有这些对象合起来，代表了集群的实际情况。常规的应用里，应用程序的数据存储在数据库中，Kubernetes将其数据以Kubernetes对象的形式通过 api server存储在 etcd 中。具体来说，这些数据（Kubernetes对象）描述了： 集群中运行了哪些容器化应用程序（以及在哪个节点上运行） 集群中对应用程序可用的资源 应用程序相关的策略定义，例如，重启策略、升级策略、容错策略 其他Kubernetes管理应用程序时所需要的信息 操作 Kubernetes 对象（创建、修改、删除）的方法主要有： kubectl 命令行工具 kuboard 图形界面工具 kubectl、kuboard 最终都通过调用 kubernetes API 来实现对 Kubernetes 对象的操作。 对象的spec和status每一个 Kubernetes 对象都包含了两个重要的字段： spec 必须由您来提供，描述了您对该对象所期望的 目标状态 status 只能由 Kubernetes 系统来修改，描述了该对象在 Kubernetes 系统中的 实际状态 Kubernetes通过对应的控制器，不断地使实际状态趋向于期望的目标状态。 ​ 例如，一个 Kubernetes Deployment 对象可以代表一个应用程序在集群中的运行状态。当创建 Deployment 对象时，可以通过 Deployment 的 spec 字段指定需要运行应用程序副本数（假设为3）。Kubernetes 从 Deployment 的 spec 中读取这些信息，并创建指定容器化应用程序的 3 个副本，再将实际的状态更新到 Deployment 的 status 字段。Kubernetes 系统将不断地比较 实际状态 staus 和 目标状态 spec 之间的差异，并根据差异做出对应的调整。例如，如果任何一个副本运行失败了，Kubernetes 将启动一个新的副本，以替代失败的副本 描述Kubernetes对象在 Kubernetes 中创建一个对象时，必须提供 该对象的 spec 字段，通过该字段描述您期望的 目标状态 该对象的一些基本信息，例如名字 如果使用 kubectl 创建对象，必须编写 .yaml 格式的文件下面是一个 kubectl 可以使用的 .yaml 文件： 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 # 运行 2 个容器化应用程序副本 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 使用 kube apply 命令可以创建该 .yaml 文件中的 Deployment 对象： 12345678910[root@k8s-master k8s-yamls]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deployment created[root@k8s-master k8s-yamls]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-deployment-54f57cf6bf-528xn 1/1 Running 0 4m35snginx-deployment-54f57cf6bf-bgclh 1/1 Running 0 4m35s#删除[root@k8s-master k8s-yamls]# kubectl delete -f nginx-deployment.yaml deployment.apps \"nginx-deployment\" deleted 必填字段在上述的 .yaml 文件中，如下字段是必须填写的： apiVersion 用来创建对象时所使用的Kubernetes API版本 kind 被创建对象的类型 metadata 用于唯一确定该对象的元数据：包括 name 和 namespace，如果 namespace 为空，则默认值为 default spec 描述您对该对象的期望状态 二、管理Kubernetes对象三种管理方式 管理方式 操作对象 推荐的环境 参与编辑的人数 学习曲线 指令性的命令行 Kubernetes对象 开发环境 1+ 最低 指令性的对象配置 单个 yaml 文件 生产环境 1 适中 声明式的对象配置 包含多个 yaml 文件的多个目录 生产环境 1+ 最高 指令性的命令行当使用指令性的命令行（imperative commands）时，用户通过向 kubectl 命令提供参数的方式，直接操作集群中的 Kubernetes 对象。此时，用户无需编写或修改 .yaml 文件。 这是在 Kubernetes 集群中执行一次性任务的一个简便的办法。由于这种方式直接修改 Kubernetes 对象，也就无法提供历史配置查看的功能。 例子创建一个 Deployment 对象，以运行一个 nginx 实例： 1kubectl run nginx --image nginx 下面的命令完成了相同的任务，但是命令格式不同： 1kubectl create deployment nginx --image nginx 优缺点与编写 .yaml 文件进行配置的方式相比的优势： 命令简单，易学易记，只需要一个步骤，就可以对集群执行变更 缺点： 使用命令，无法进行变更review的管理；不提供日志审计；没有创建新对象的模板 指令性的对象配置使用指令性的对象配置（imperative object configuration）时，需要向 kubectl 命令指定具体的操作（create,replace,apply,delete等），可选参数以及至少一个配置文件的名字。配置文件中必须包括一个完整的对象的定义，可以是 yaml 格式，也可以是 json 格式。 通过配置文件创建对象 1kubectl create -f nginx.yaml 删除两个配置文件中的对象 1kubectl delete -f nginx.yaml -f redis.yaml 直接使用配置文件中的对象定义，替换Kubernetes中对应的对象： 1kubectl replace -f nginx.yaml 声明式的对象配置当使用声明式的对象配置时，用户操作本地存储的Kubernetes对象配置文件，然而，在将文件传递给 kubectl 命令时，并不指定具体的操作，由 kubectl 自动检查每一个对象的状态并自行决定是创建、更新、还是删除该对象。使用这种方法时，可以直接针对一个或多个文件目录进行操作（对不同的对象可能需要执行不同的操作）。 处理 configs 目录中所有配置文件中的Kubernetes对象，根据情况创建对象、或更新Kubernetes中已经存在的对象。可以先执行 diff 指令查看具体的变更，然后执行 apply 指令执行变更： 123456kubectl diff -f configs/kubectl apply -f configs/#递归处理目录中的内容kubectl diff -R -f configs/kubectl apply -R -f configs/ 三、名称 Kubernetes REST API 中，所有的对象都是通过 name 和 UID 唯一性确定。 Names 同一个名称空间下，同一个类型的对象，可以通过 name 唯一性确定。如果删除该对象之后，可以再重新创建一个同名对象。 例如，下面的配置文件定义了一个 name 为 nginx-demo 的 Pod，该 Pod 包含一个 name 为 nginx 的容器： 12345678910apiVersion: v1kind: Podmetadata: name: nginx-demospec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 UIDsUID 是由 Kubernetes 系统生成的，唯一标识某个 Kubernetes 对象的字符串。 Kubernetes集群中，每创建一个对象，都有一个唯一的 UID。用于区分多次创建的同名对象（如前所述，按照名字删除对象后，重新再创建同名对象时，两次创建的对象 name 相同，但是 UID 不同。） 四、名称空间何时使用名称空间 名称空间的用途是，为不同团队的用户（或项目）提供虚拟的集群空间（划分集群的资源），也可以用来区分开发环境/测试环境、准上线环境/生产环境。 名称空间为名称提供了作用域。名称空间内部的同类型对象不能重名，但是跨名称空间可以有同名同类型对象。名称空间不可以嵌套，任何一个Kubernetes对象只能在一个名称空间中。 在 Kubernetes 将来的版本中，同名称空间下的对象将默认使用相同的访问控制策略。 当Kubernetes对象之间的差异不大时，无需使用名称空间来区分，例如，同一个软件的不同版本，只需要使用 labels 来区分即可。 如何使用名称空间查看名称空间执行命令 kubectl get namespaces 可以查看名称空间 Kubernetes 安装成功后，默认有初始化了三个名称空间： default 默认名称空间，如果 Kubernetes 对象中不定义 metadata.namespace 字段，该对象将放在此名称空间下 kube-system Kubernetes系统创建的对象放在此名称空间下 kube-public 此名称空间自动在安装集群是自动创建，并且所有用户都是可以读取的（即使是那些未登录的用户）。主要是为集群预留的，例如，某些情况下，某些Kubernetes对象应该被所有集群用户看到。 在执行请求时设定namespace执行 kubectl 命令时，可以使用 --namespace 参数指定名称空间，例如： 12kubectl run nginx --image=nginx --namespace=&lt;your_namespace&gt;kubectl get pods --namespace=&lt;your_namespace&gt; 设置名称空间偏好可以通过 set-context 命令改变当前kubectl 上下文的名称空间，后续所有命令都默认在此名称空间下执行。 123kubectl config set-context --current --namespace=&lt;your_namespace&gt;# 验证结果kubectl config view --minify | grep namespace: 名称空间与DNS当创建一个 Service 时，Kubernetes 为其创建一个对应的DNS 条目。该 DNS 记录的格式为 &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local ，也就是说，如果在容器中只使用 &lt;service-name&gt; ，其DNS将解析到同名称空间下的 Service。这个特点在多环境的情况下非常有用，例如将开发环境、测试环境、生产环境部署在不同的名称空间下，应用程序只需要使用&lt;service-name&gt;即可进行服务发现，无需为不同的环境修改配置。如果跨名称空间访问服务，则必须使用完整的域名（fully qualified domain name，FQDN）。 并非所有对象都在名称空间里大部分的 Kubernetes 对象（例如，Pod、Service、Deployment、StatefulSet等）都必须在名称空间里。但是某些更低层级的对象，是不在任何名称空间中的，例如nodes、persistentVolumes、storageClass等 执行一下命令可查看哪些 Kubernetes 对象在名称空间里，哪些不在： 12345# 在名称空间里kubectl api-resources --namespaced=true# 不在名称空间里kubectl api-resources --namespaced=false","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"kubeadm安装Kubernetes1.16.3","slug":"kubeadm安装Kubernetes1-16-3","date":"2020-03-27T07:11:48.000Z","updated":"2020-03-27T07:39:50.190Z","comments":true,"path":"2020/03/27/kubeadm安装Kubernetes1-16-3/","link":"","permalink":"http://yoursite.com/2020/03/27/kubeadm%E5%AE%89%E8%A3%85Kubernetes1-16-3/","excerpt":"系统环境： CentOS 版本：7.7 Docker 版本：18.09.9-3 Calico 版本：v3.10 Kubernetes 版本：1.16.3 Kubernetes Newwork 模式：IPVS Kubernetes Dashboard 版本：dashboard:v2.0.0-beta6","text":"系统环境： CentOS 版本：7.7 Docker 版本：18.09.9-3 Calico 版本：v3.10 Kubernetes 版本：1.16.3 Kubernetes Newwork 模式：IPVS Kubernetes Dashboard 版本：dashboard:v2.0.0-beta6 一、更新系统内核（全部节点）由于 Docker 对系统内核有一定的要求，所以我们最好使用 yum 来更新系统软件及其内核。 1234567891011#备份本地 yum 源$ mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo_bak # 获取阿里 yum 源配置文件$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #清理 yum$ yum clean all#更新软件版本并且更新现有软件$ yum -y update 二、基础环境设置（全部节点）Kubernetes 需要一定的环境来保证正常运行，如各个节点时间同步，主机名称解析，关闭防火墙等等。 1、修改 Host分布式系统环境中的多主机通信通常基于主机名称进行，这在 IP 地址存在变化的可能 性时为主机提供了固定的访问人口，因此一般需要有专用的 DNS 服务负责解析各节点主机。考虑到此处部署的是测试集群，因此为了降低系复杂度，这里将基于 hosts 的文件进行主机名称解析。 1$ vim /etc/hosts 加入下面内容： 123192.168.2.11 k8s-master192.168.2.12 k8s-node-01192.168.2.13 k8s-node-02 2、修改 Hostnamekubernetes 中会以各个服务的 hostname 为其节点命名，所以需要进入不同的服务器修改 hostname 名称。 1234567891011#修改 192.168.2.11 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-master$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts#修改 192.168.2.12 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-node-01$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts#修改 192.168.2.13 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-node-02$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts 3、主机时间同步将各个服务器的时间同步，并设置开机启动同步时间服务。 1$ systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service 4、关闭防火墙服务关闭防火墙，并禁止开启启动。 1$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld 5、关闭并禁用SELinux关闭SELinux，并编辑／etc/sysconfig selinux 文件，以彻底禁用 SELinux 12$ setenforce 0$ sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config 查看selinux状态 1$ getenforce 6、禁用 Swap 设备关闭当前已启用的所有 Swap 设备： 1$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 编辑 fstab 配置文件，注释掉标识为 Swap 设备的所有行： 123$ vi /etc/fstab#/dev/mapper/centos-swap swap swap defaults 0 0 7、设置内核参数配置内核参数： 123456789$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10EOF 使配置生效: 12345678#挂载 br_netfilter$ modprobe br_netfilter#使配置生效$ sysctl -p /etc/sysctl.d/k8s.conf#查看是否生成相关文件$ ls /proc/sys/net/bridge 8、配置 IPVS 模块由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块： ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4 12345678910$ cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF 执行脚本并查看是否正常加载内核模块： 12345678#修改脚本权限$ chmod 755 /etc/sysconfig/modules/ipvs.modules #执行脚本$ bash /etc/sysconfig/modules/ipvs.modules #查看是否已经正确加载所需的内核模块$ lsmod | grep -e ip_vs -e nf_conntrack_ipv4 安装 ipset 1$ yum install -y ipset 9、配置资源限制123456echo \"* soft nofile 65536\" &gt;&gt; /etc/security/limits.confecho \"* hard nofile 65536\" &gt;&gt; /etc/security/limits.confecho \"* soft nproc 65536\" &gt;&gt; /etc/security/limits.confecho \"* hard nproc 65536\" &gt;&gt; /etc/security/limits.confecho \"* soft memlock unlimited\" &gt;&gt; /etc/security/limits.confecho \"* hard memlock unlimited\" &gt;&gt; /etc/security/limits.conf 10、安装依赖包以及相关工具12$ yum install -y epel-release$ yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim ntpdate libseccomp libtool-ltdl 三、安装Docker（全部节点）1、移除之前安装过的Docker1234567891011$ sudo yum -y remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-ce-cli \\ docker-engine 查看还有没有存在的 Docker 组件，一定要确保删除干净： 1$ rpm -qa | grep docker 有则通过命令 yum -y remove XXX 来删除，比如： 1$ yum remove docker-ce-cli 2、更换 Docker 的 yum 源由于官方下载速度比较慢，所以需要更改 Docker 安装的 yum 源，这里推荐用阿里镜像源： 1$ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3、显示 docker 所有可安装版本：1$ yum list docker-ce --showduplicates | sort -r 、安装指定版本 docker 注意：安装前一定要提前查询将要安装的 Kubernetes 版本是否和 Docker 版本对应。 1$ yum install -y docker-ce-18.09.9-3.el7 设置镜像存储目录，找到大点的挂载的目录进行存储 1234$ vi /lib/systemd/system/docker.service#找到这行，往后面加上存储目录，例如这里是 --graph /apps/dockerExecStart=/usr/bin/docker --graph /apps/docker 5、配置 Docker 参数和镜像加速器Kubernetes 推荐的一些 Docker 配置参数，这里配置一下。还有就是由于国内访问 Docker 仓库速度很慢，所以国内几家云厂商推出镜像加速下载的代理加速器，这里也需要配置一下。 创建 Docker 配置文件的目录： 1$ mkdir -p /etc/docker 添加配置文件： 123456789101112131415161718192021$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [ \"https://dockerhub.azk8s.cn\", \"http://hub-mirror.c.163.com\", \"https://registry.docker-cn.com\" ], \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\":\"5\" &#125;&#125;EOF 6、启动 docker 并设置 docker 开机启动启动 Docker： 1$ systemctl start docker &amp;&amp; systemctl enable docker 如果 Docker 已经启动，则需要重启 Docker： 12$ systemctl daemon-reload$ systemctl restart docker 四、安装 kubelet、kubectl、kubeadm（全部节点）1、配置可用的国内 yum 源1234567891011$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 2、安装 kubelet、kubectl、kubeadm kubelet: 在集群中的每个节点上用来启动 pod 和 container 等。 kubectl: 用来与集群通信的命令行工具。 kubeadm: 用来初始化集群的指令。 注意安装顺序，一定不要先安装 kubeadm，因为 kubeadm 会自动安装最新版本的 kubelet 与 kubectl，导致版本不一致问题。 12345678#安装 kubelet$ yum install -y kubelet-1.16.3-0#安装 kubectl$ yum install -y kubectl-1.16.3-0#安装 kubeadm$ yum install -y kubeadm-1.16.3-0 3、启动 kubelet 并配置开机启动1$ systemctl start kubelet &amp;&amp; systemctl enable kubelet 检查状态时会发现 kubelet 是 failed 状态，等初 master 节点初始化完成后即可显示正常。 五、重启服务器（全部节点）为了防止发生某些未知错误，这里我们重启下服务器，方便进行后续操作 1$ reboot 六、kubeadm 安装 kubernetes（Master 节点）创建 kubeadm 配置文件 kubeadm-config.yaml，然后需要配置一些参数： 配置 localAPIEndpoint.advertiseAddress 参数，调整为你的 Master 服务器地址。 配置 imageRepository 参数，调整 kubernetes 镜像下载地址为阿里云。 配置 networking.podSubnet 参数，调整为你要设置的网络范围。 kubeadm-config.yaml 123456789101112131415161718192021222324$ cat &gt; kubeadm-config.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta2kind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.2.11 bindPort: 6443nodeRegistration: taints: - effect: PreferNoSchedule key: node-role.kubernetes.io/master---apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationimageRepository: registry.aliyuncs.com/google_containerskubernetesVersion: v1.16.3networking: podSubnet: 10.244.0.0/16---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvsEOF kubeadm 初始化 kubernetes 集群 1$ kubeadm init --config kubeadm-config.yaml 部署日志信息： 1234567891011121314151617181920......[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.2.11:6443 --token 4udy8a.f77ai0zun477kx0p \\ --discovery-token-ca-cert-hash sha256:4645472f24b438e0ecf5964b6dcd64913f68e0f9f7458768cfb96a9ab16b4212 上面记录了完成的初始化输出的内容，根据输出的内容基本上可以看出手动初始化安装一个Kubernetes集群所需要的关键步骤。 其中有以下关键内容： [kubelet] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml” [certificates]生成相关的各种证书 [kubeconfig]生成相关的kubeconfig文件 [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到 在此处看日志可以知道，可以通过下面命令，添加 kubernetes 相关环境变量： 123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 或者直接只用命令 12345kubeadm init \\ --kubernetes-version=v1.16.3 \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.2.11 \\ --ignore-preflight-errors=Swap 提前拉取镜像 1kubeadm config images pull --config kubeadm-master.config 安装过程中遇到异常： 12Copy[preflight] Some fatal errors occurred: [ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty 直接删除/var/lib/etcd文件夹 如果初始化过程出现问题，使用如下命令重置： 1kubeadm reset 七、工作节点加入集群（Work Node 节点）根据上面 Master 节点创建 Kubernetes 集群时的日志信息，可以知道在各个节点上执行下面命令来让工作节点加入主节点： 12$ kubeadm join 192.168.2.11:6443 --token 4udy8a.f77ai0zun477kx0p \\ --discovery-token-ca-cert-hash sha256:4645472f24b438e0ecf5964b6dcd64913f68e0f9f7458768cfb96a9ab16b4212 八、部署网络插件（Master 节点）Kubernetes 中可以部署很多种网络插件，不过比较流行也推荐的有两种： Flannel： Flannel 是基于 Overlay 网络模型的网络插件，能够方便部署，一般部署后只要不出问题，一般不需要管它。 Calico： 与 Flannel 不同，Calico 是一个三层的数据中心网络方案，使用 BGP 路由协议在主机之间路由数据包，可以灵活配置网络策略。 这两种网络根据环境任选其一即可，这里使用的是 Calico，可以按下面步骤部署： 1、部署 Calico 网络插件下载 Calico 部署文件，并替换里面的网络范围为上面 kubeadm 中 networking.podSubnet 配置的值。 12345678#下载 calico 部署文件$ wget https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml #替换 calico 部署文件的 IP 为 kubeadm 中的 networking.podSubnet 参数 10.244.0.0。$ sed -i 's/192.168.0.0/10.244.0.0/g' calico.yaml#部署 Calico 插件$ kubectl apply -f calico.yaml 2、查看 Pod 是否成功启动12345678910111213141516$ kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcalico-kube-controllers-6b64bcd855-jn8pz 1/1 Running 0 2m40scalico-node-5wssd 1/1 Running 0 2m40scalico-node-7tw94 1/1 Running 0 2m40scalico-node-xzfp4 1/1 Running 0 2m40scoredns-58cc8c89f4-hv4fn 1/1 Running 0 21mcoredns-58cc8c89f4-k97x6 1/1 Running 0 21metcd-k8s-master 1/1 Running 0 20mkube-apiserver-k8s-master 1/1 Running 0 20mkube-controller-manager-k8s-master 1/1 Running 0 20mkube-proxy-9dlpz 1/1 Running 0 14mkube-proxy-krd5n 1/1 Running 0 14mkube-proxy-tntpr 1/1 Running 0 21mkube-scheduler-k8s-master 1/1 Running 0 20m 可以看到所以 Pod 都已经成功启动。 九、配置 Kubectl 命令自动补全（Master 节点）1$ yum install -y bash-completion 添加补全配置 123$ source &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion$ source &lt;(kubectl completion bash)$ echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~&#x2F;.bashrc 添加完成就可与通过输入 kubectl 后，按补全键（一般为 tab）会自动补全对应的命令。 十、查看是否开启 IPVS（Master 节点）上面全部组件都已经部署完成，不过还需要确认是否成功将网络模式设置为 IPVS，可以查看 kube-proxy 日志，在日志信息中查找是否存在 IPVS 关键字信息来确认。 12345$ kubectl get pod -n kube-system | grep kube-proxykube-proxy-9dlpz 1/1 Running 0 42mkube-proxy-krd5n 1/1 Running 0 42mkube-proxy-tntpr 1/1 Running 0 49m 选择其中一个 Pod ，查看该 Pod 中的日志信息中是否存在 ipvs 信息： 12345678910111213141516$ kubectl logs kube-proxy-9dlpz -n kube-systemI1120 18:13:46.357178 1 node.go:135] Successfully retrieved node IP: 192.168.2.13I1120 18:13:46.357265 1 server_others.go:176] Using ipvs Proxier.W1120 18:13:46.358005 1 proxier.go:420] IPVS scheduler not specified, use rr by defaultI1120 18:13:46.358919 1 server.go:529] Version: v1.16.3I1120 18:13:46.359327 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_max&#39; to 131072I1120 18:13:46.359379 1 conntrack.go:52] Setting nf_conntrack_max to 131072I1120 18:13:46.359426 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_tcp_timeout_established&#39; to 86400I1120 18:13:46.359452 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_tcp_timeout_close_wait&#39; to 3600I1120 18:13:46.359626 1 config.go:313] Starting service config controllerI1120 18:13:46.359685 1 shared_informer.go:197] Waiting for caches to sync for service configI1120 18:13:46.359833 1 config.go:131] Starting endpoints config controllerI1120 18:13:46.359889 1 shared_informer.go:197] Waiting for caches to sync for endpoints configI1120 18:13:46.460013 1 shared_informer.go:204] Caches are synced for service config I1120 18:13:46.460062 1 shared_informer.go:204] Caches are synced for endpoints config 如上，在日志中查到了 IPVS 字样，则代表使用了 IPVS 模式。 十一、集群中移除Node 如果需要从集群中移除node2这个Node执行下面的命令： 在master节点上执行： 123kubectl drain node2 --delete-local-data --force --ignore-daemonsets kubectl delete node node2 在node2上执行： 123456kubeadm reset ifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf &#x2F;var&#x2F;lib&#x2F;cni&#x2F; 十二、部署 Kubernetes Dashboard接下来我们将部署 Kubernetes 的控制看板，由于集群为 1.16.3，而 Dashboard 的稳定版本还是基于 Kubernetes 1.10 版本，所以我们直接使用 Kubernetes Dashboard 2.0.0 Bate 版本，虽然为测试版本，不过它比较新，对新版本的兼容还是比旧版本好些，坐等它稳定，这里先部署尝尝鲜。 1、创建 Dashboard 部署文件k8s-dashboard-deploy.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184$ cat &gt; k8s-dashboard-deploy.yaml &lt;&lt; EOFapiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemrules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\", \"kubernetes-dashboard-csrf\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\", \"dashboard-metrics-scraper\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\", \"dashboard-metrics-scraper\", \"http:dashboard-metrics-scraper\"] verbs: [\"get\"]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboardrules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kube-systemtype: Opaquedata: csrf: \"\"---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kube-systemtype: Opaque---kind: ConfigMapapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kube-system---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.0.0-beta6 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kube-system #设置为当前namespace volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule EOF 部署 Kubernetes Dashboard 1$ kubectl apply -f k8s-dashboard-deploy.yaml 2、创建监控信息 kubernetes-metrics-scraperk8s-dashboard-metrics.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ cat &gt; k8s-dashboard-metrics.yaml &lt;&lt; EOFkind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-metrics-scraper name: dashboard-metrics-scraper namespace: kube-systemspec: ports: - port: 8000 targetPort: 8000 selector: k8s-app: kubernetes-metrics-scraper---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-metrics-scraper name: kubernetes-metrics-scraper namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-metrics-scraper template: metadata: labels: k8s-app: kubernetes-metrics-scraper spec: containers: - name: kubernetes-metrics-scraper image: kubernetesui/metrics-scraper:v1.0.1 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 serviceAccountName: kubernetes-dashboard tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule EOF 部署 Dashboard Metrics 1$ kubectl apply -f k8s-dashboard-metrics.yaml 3、创建 Dashboard ServiceAccount访问 Kubernetes Dashboard 时会验证身份，需要提前在 Kubernetes 中创建一个一定权限的 ServiceAccount，然后获取其 Token 串，这里为了简单方便，直接创建一个与管理员绑定的服务账户，获取其 Token 串。 k8s-dashboard-rbac.yaml 123456789101112131415161718192021222324252627$ cat &gt; k8s-dashboard-token.yaml &lt;&lt; EOFkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\"roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: admin namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile EOF 部署访问的 ServiceAccount： 1$ kubectl apply -f k8s-dashboard-rbac.yaml 获取 Token： 1$ kubectl describe secret/$(kubectl get secret -n kube-system |grep admin|awk '&#123;print $1&#125;') -n kube-system 然后输入 Kubernetes 集群任意节点地址配置上面配置的 Service 的 NodePort 30001 来访问看板。输入地址https://192.168.2.11:30001进入看板页面，输入上面获取的 Token 串进行验证登录。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-控制器","slug":"k8s进阶-架构-控制器","date":"2020-03-26T08:49:58.000Z","updated":"2020-03-26T09:17:26.418Z","comments":true,"path":"2020/03/26/k8s进阶-架构-控制器/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84-%E6%8E%A7%E5%88%B6%E5%99%A8/","excerpt":"控制器不断地尝试着将当前的状态调整到目标状态","text":"控制器不断地尝试着将当前的状态调整到目标状态 控制器在 Kubernetes 中，控制器 就是上面所说的 控制循环，它不断监控着集群的状态，并对集群做出对应的变更调整。每一个控制器都不断地尝试着将 当前状态 调整到 目标状态。 控制器模式在 Kubernetes 中，每个控制器至少追踪一种类型的资源。这些资源对象中有一个 spec 字段代表了目标状态。资源对象对应的控制器负责不断地将当前状态调整到目标状态。 理论上，控制器可以自己直接执行调整动作，然而，在Kubernetes 中，更普遍的做法是，控制器发送消息到 API Server，而不是直接自己执行调整动作。 通过APIServer进行控制以 Kubernetes 中自带的一个控制器 Job Controller 为例。Kubernetes 自带的控制器都是通过与集群中 API Server 交互来达到调整状态的目的。 Job 是一种 Kubernetes API 对象，一个 Job 将运行一个（或多个）Pod，执行一项任务，然后停止。当新的 Job 对象被创建时，Job Controller 将确保集群中有合适数量的节点上的 kubelet 启动了指定个数的 Pod，以完成 Job 的执行任务。Job Controller 自己并不执行任何 Pod 或容器，而是发消息给 API Server，由其他的控制组件配合 API Server，以执行创建或删除 Pod 的实际动作。 当新的 Job 对象被创建时，目标状态是指定的任务被执行完成。Job Controller 调整集群的当前状态以达到目标状态：创建 Pod 以执行 Job 中指定的任务 控制器同样也会更新其关注的 API 对象。例如：一旦 Job 的任务执行结束，Job Controller 将更新 Job 的 API 对象，将其标注为 Finished。 直接控制某些特殊的控制器需要对集群外部的东西做调整。例如，想用一个控制器确保集群中有足够的节点，此时控制器需要调用云供应商的接口以创建新的节点或移除旧的节点。这类控制器将从 API Server 中读取关于目标状态的信息，并直接调用外部接口以实现调整目标。 目标状态 vs 当前状态Kubernetes 使用了 云原生（cloud-native）的视角来看待系统，并且可以持续应对变化。集群在运行的过程中，任何时候都有可能发生突发事件，而控制器则自动地修正这些问题。这就意味着，集群永远不会达到一个稳定不变的状态。 这种通过控制器监控集群状态并利用负反馈原理不断接近目标状态的系统，相较于那种完成安装后就不再改变的系统，是一种更高级的系统形态。 运行控制器的方式Kubernetes 在 kube-controller-manager 中运行了大量的内建控制器（例如，Deployment Controller、Job Controller、StatefulSet Controller、DaemonSet Controller 等）。这些内建控制器提供了 Kubernetes 非常重要的核心功能。Kubernetes 可以运行一个 master 集群，以实现内建控制器的高可用。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-集群内的通信","slug":"k8s进阶-架构-集群内的通信","date":"2020-03-26T08:24:42.000Z","updated":"2020-03-27T07:24:09.675Z","comments":true,"path":"2020/03/26/k8s进阶-架构-集群内的通信/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84-%E9%9B%86%E7%BE%A4%E5%86%85%E7%9A%84%E9%80%9A%E4%BF%A1/","excerpt":"Master-Node之间的通信","text":"Master-Node之间的通信 Master-Node之间的通信Master-Node 之间的通信可以分为如下两类： Cluster to Master Master to Cluster Cluster to Master所有从集群访问 Master 节点的通信，都是针对 apiserver 的（没有任何其他 master 组件发布远程调用接口）。通常安装 Kubernetes 时，apiserver 监听 HTTPS 端口（443），并且配置了一种或多种客户端认证方式 authentication。至少需要配置一种形式的授权方式 authorization，尤其是匿名访问 anonymous requests或 Service Account Tokens被启用的情况下。 节点上必须配置集群（apiserver）的公钥根证书（public root certificate），此时，在提供有效的客户端身份认证的情况下，节点可以安全地访问 APIServer。例如，在 Google Kubernetes Engine 的一个默认 Kubernetes 安装里，通过客户端证书为 kubelet 提供客户端身份认证。 对于需要调用 APIServer 接口的 Pod，应该为其关联 Service Account，此时，Kubernetes将在创建Pod时自动为其注入公钥根证书（public root certificate）以及一个有效的 bearer token（放在HTTP请求头Authorization字段）。所有名称空间中，都默认配置了名为 kubernetes Kubernetes Service，该 Service对应一个虚拟 IP（默认为 10.96.0.1），发送到该地址的请求将由 kube-proxy 转发到 apiserver 的 HTTPS 端口上。 默认情况下，从集群（节点以及节点上运行的 Pod）访问 master 的连接是安全的，因此，可以通过不受信的网络或公网连接 Kubernetes 集群 Master to Cluster从 master（apiserver）到Cluster存在着两条主要的通信路径： apiserver 访问集群中每个节点上的 kubelet 进程 使用 apiserver 的 proxy 功能，从 apiserver 访问集群中的任意节点、Pod、Service apiserver to kubeletapiserver 在如下情况下访问 kubelet： 抓取 Pod 的日志 通过 kubectl exec -it 指令（或 kuboard 的终端界面）获得容器的命令行终端 提供 kubectl port-forward 功能 这些连接的访问端点是 kubelet 的 HTTPS 端口。默认情况下，apiserver 不校验 kubelet 的 HTTPS 证书，这种情况下，连接可能会收到 man-in-the-middle 攻击，因此该连接如果在不受信网络或者公网上运行时，是 不安全 的。 如果要校验 kubelet 的 HTTPS 证书，可以通过 --kubelet-certificate-authority 参数为 apiserver 提供校验 kubelet 证书的根证书。 如果不能完成这个配置，又需要通过不受信网络或公网将节点加入集群，则需要使用SSH隧道连接 apiserver 和 kubelet。 同时，Kubelet authentication/authorization需要激活，以保护 kubelet API apiserver to nodes, pods, services从 apiserver 到 节点/Pod/Service 的连接使用的是 HTTP 连接，没有进行身份认证，也没有进行加密传输。您也可以通过增加 https 作为 节点/Pod/Service 请求 URL 的前缀，但是 HTTPS 证书并不会被校验，也无需客户端身份认证，因此该连接是无法保证一致性的。目前，此类连接如果运行在非受信网络或公网上时，是 不安全 的 SSH隧道Kubernetes 支持 SSH隧道（tunnel）来保护 Master –&gt; Cluster 访问路径。此时，apiserver 将向集群中的每一个节点建立一个 SSH隧道（连接到端口22的ssh服务）并通过隧道传递所有发向 kubelet、node、pod、service 的请求。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-节点","slug":"k8s进阶-架构","date":"2020-03-26T02:21:12.000Z","updated":"2020-03-26T09:17:36.969Z","comments":true,"path":"2020/03/26/k8s进阶-架构/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84/","excerpt":"节点 节点管理","text":"节点 节点管理 节点节点状态节点的状态包含如下信息： Addresses Conditions Capacity and Allocatable Info 执行以下命令可查看所有节点的列表 1kubectl get nodes -o wide 执行以下命令可查看节点状态以及节点的其他详细信息： 1kubectl describe node &lt;your-node-name&gt; Addresses HostName： 在节点命令行界面上执行 hostname 命令所获得的值。启动 kubelet 时，可以通过参数 --hostname-override 覆盖 ExternalIP：通常是节点的外部IP（可以从集群外访问的内网IP地址；上面的例子中，此字段为空） InternalIP：通常是从节点内部可以访问的 IP 地址 ConditionsConditions 描述了节点的状态。Condition的例子有： Node Condition 描述 OutOfDisk 如果节点上的空白磁盘空间不够，不能够再添加新的节点时，该字段为 True，其他情况为 False Ready 如果节点是健康的且已经就绪可以接受新的 Pod。则节点Ready字段为 True。False表明了该节点不健康，不能够接受新的 Pod。 MemoryPressure 如果节点内存紧张，则该字段为 True，否则为False PIDPressure 如果节点上进程过多，则该字段为 True，否则为 False DiskPressure 如果节点磁盘空间紧张，则该字段为 True，否则为 False NetworkUnvailable 如果节点的网络配置有问题，则该字段为 True，否则为 False Capacity and Allocatable（容量和可分配量）容量和可分配量（Capacity and Allocatable）描述了节点上的可用资源的情况： CPU 内存 该节点可调度的最大 pod 数量 Capacity 中的字段表示节点上的资源总数，Allocatable 中的字段表示该节点上可分配给普通 Pod 的资源总数。 Info描述了节点的基本信息，例如： Linux 内核版本 Kubernetes 版本（kubelet 和 kube-proxy 的版本） Docker 版本 操作系统名称 这些信息由节点上的 kubelet 收集。 节点管理与 Pod 和 Service 不一样，节点并不是由 Kubernetes 创建的，节点由云供应商（例如，Google Compute Engine、阿里云等）创建，或者节点已经存在于您的物理机/虚拟机的资源池。向 Kubernetes 中创建节点时，仅仅是创建了一个描述该节点的 API 对象。节点 API 对象创建成功后，Kubernetes将检查该节点是否有效。 节点控制器（Node Controller）节点控制器是一个负责管理节点的 Kubernetes master 组件。在节点的生命周期中，节点控制器起到了许多作用。 节点控制器在注册节点时为节点分配 CIDR 地址块 节点控制器通过云供应商（cloud-controller-manager）接口检查节点列表中每一个节点对象对应的虚拟机是否可用。在云环境中，只要节点状态异常，节点控制器检查其虚拟机在云供应商的状态，如果虚拟机不可用，自动将节点对象从 APIServer 中删除。 节点控制器监控节点的健康状况。当节点变得不可触达时（例如，由于节点已停机，节点控制器不再收到来自节点的心跳信号），节点控制器将节点API对象的 NodeStatus Condition 取值从 NodeReady 更新为 Unknown；然后在等待 pod-eviction-timeout 时间后，将节点上的所有 Pod 从节点驱逐。 默认40秒未收到心跳，修改 NodeStatus Condition 为 Unknown； 默认 pod-eviction-timeout 为 5分钟 节点控制器每隔 --node-monitor-period 秒检查一次节点的状态 节点自注册（Self-Registration）如果 kubelet 的启动参数 --register-node为 true（默认为 true），kubelet 会尝试将自己注册到 API Server。kubelet自行注册时，将使用如下选项： --kubeconfig：向 apiserver 进行认证时所用身份信息的路径 --cloud-provider：向云供应商读取节点自身元数据 --register-node：自动向 API Server 注册节点 --register-with-taints：注册节点时，为节点添加污点（逗号分隔，格式为 =: --node-ip：节点的 IP 地址 --node-labels：注册节点时，为节点添加标签 --node-status-update-frequency：向 master 节点发送心跳信息的时间间隔 如果 Node authorization mode 和 NodeRestriction admission plugin 被启用，kubelet 只拥有创建/修改其自身所对应的节点 API 对象的权限。 手动管理节点如果想要手工创建节点API对象，可以将 kubelet 的启动参数 --register-node 设置为 false。 管理员可以修改节点API对象（不管是否设置了 --register-node 参数）。可以修改的内容有： 增加/减少标签 标记节点为不可调度（unschedulable） 节点的标签与 Pod 上的节点选择器（node selector）配合，可以控制调度方式，例如，限定 Pod 只能在某一组节点上运行。 执行如下命令可将节点标记为不可调度（unschedulable），此时将阻止新的 Pod 被调度到该节点上，但是不影响任何已经在该节点上运行的 Pod。这在准备重启节点之前非常有用。 1kubectl cordon $NODENAME 节点容量（Node Capacity）Kubernetes 调度器在调度 Pod 到节点上时，将确保节点上有足够的资源。具体来说，调度器检查节点上所有容器的资源请求之和不大于节点的容量。此时，只能检查由 kubelet 启动的容器，不包括直接由容器引擎启动的容器，更不包括不在容器里运行的进程","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"伸缩服务和滚动更新","slug":"伸缩服务","date":"2020-03-25T08:43:30.000Z","updated":"2020-03-25T09:09:44.425Z","comments":true,"path":"2020/03/25/伸缩服务/","link":"","permalink":"http://yoursite.com/2020/03/25/%E4%BC%B8%E7%BC%A9%E6%9C%8D%E5%8A%A1/","excerpt":"通过更改部署中的 replicas（副本数）来完成伸缩","text":"通过更改部署中的 replicas（副本数）来完成伸缩 Scaling（伸缩）应用程序 伸缩 的实现可以通过更改 nginx-deployment.yaml 文件中部署的 replicas（副本数）来完成 12spec: replicas: 4 #使用该Deployment创建两个应用程序实例 修改了 Deployment 的 replicas 为 4 后，Kubernetes 又为该 Deployment 创建了 3 新的 Pod，这 4 个 Pod 有相同的标签。因此Service A通过标签选择器与新的 Pod建立了对应关系，将访问流量通过负载均衡在 4 个 Pod 之间进行转发。 将 nginx Deployment 扩容到 4 个副本修改 nginx-deployment.yaml 文件，将 replicas 修改为 4 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 4 #通过更改部署中的 replicas（副本数）来完成扩展 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 执行命令： 1234kubectl apply -f nginx-deployment.yaml#查看结果watch kubectl get pods -o wide 滚动更新用户期望应用程序始终可用，为此开发者/运维者在更新应用程序时要分多次完成。在 Kubernetes 中，这是通过 Rolling Update 滚动更新完成的。Rolling Update滚动更新 通过使用新版本的 Pod 逐步替代旧版本的 Pod 来实现 Deployment 的更新，从而实现零停机。新的 Pod 将在具有可用资源的 Node（节点）上进行调度。 Kubernetes 更新多副本的 Deployment 的版本时，会逐步的创建新版本的 Pod，逐步的停止旧版本的 Pod，以便使应用一直处于可用状态。这个过程中，Service 能够监视 Pod 的状态，将流量始终转发到可用的 Pod 上。 滚动更新步骤1.原本 Service A 将流量负载均衡到 4 个旧版本的 Pod （当中的容器为 绿色）上 1.原本 Service A 将流量负载均衡到 4 个旧版本的 Pod （当中的容器为 绿色）上 2.更新完 Deployment 部署文件中的镜像版本后，master 节点选择了一个 worker 节点，并根据新的镜像版本创建 Pod（紫色容器）。新 Pod 拥有唯一的新的 IP。同时，master 节点选择一个旧版本的 Pod 将其移除。 此时，Service A 将新 Pod 纳入到负载均衡中，将旧Pod移除 同步骤2，再创建一个新的 Pod 替换一个原有的 Pod 如此 Rolling Update 滚动更新，直到所有旧版本 Pod 均移除，新版本 Pod 也达到 Deployment 部署文件中定义的副本数，则滚动更新完成 更新 nginx Deployment 修改 nginx-deployment.yaml 文件123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 4 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.8 #使用镜像nginx:1.8替换原来的nginx:1.7.9 ports: - containerPort: 80 执行命令 1234kubectl apply -f nginx-deployment.yaml#查看过程及结果，可观察到 pod 逐个被替换的过程。watch kubectl get pods -l app=nginx","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"Service服务","slug":"Service服务","date":"2020-03-25T07:09:33.000Z","updated":"2020-03-25T08:29:26.546Z","comments":true,"path":"2020/03/25/Service服务/","link":"","permalink":"http://yoursite.com/2020/03/25/Service%E6%9C%8D%E5%8A%A1/","excerpt":"Kubernetes Service（服务）概述","text":"Kubernetes Service（服务）概述 Service（服务）概述Pod 有自己的生命周期。当 worker node（节点）故障时，节点上运行的 Pod（容器组）也会消失。然后，Deployment 可以通过创建新的 Pod（容器组）来动态地将群集调整回原来的状态，以使应用程序保持运行。 Service的作用：由于 Kubernetes 集群中每个 Pod（容器组）都有一个唯一的 IP 地址（即使是同一个 Node 上的不同 Pod），service 可以解决为前端系统屏蔽后端系统的 Pod（容器组）在销毁、创建过程中所带来的 IP 地址的变化。 Kubernetes 中的 Service（服务） 提供了这样的一个抽象层，它选择具备某些特征的 Pod（容器组）并为它们定义一个访问方式。Service（服务）使 Pod（容器组）之间的相互依赖解耦（原本从一个 Pod 中访问另外一个 Pod，需要知道对方的 IP 地址）。一个 Service（服务）选定哪些 Pod（容器组） 通常由 LabelSelector(标签选择器) 来决定。 在创建Service的时候，通过设置配置文件中的 spec.type 字段的值，可以以不同方式向外部暴露应用程序： ClusterIP（默认） 在群集中的内部IP上公布服务，这种方式的 Service（服务）只在集群内部可以访问到 NodePort 使用 NAT 在集群中每个的同一端口上公布服务。这种方式下，可以通过访问集群中任意节点+端口号的方式访问服务 :。此时 ClusterIP 的访问方式仍然可用。 LoadBalancer 在云环境中（需要云供应商可以支持）创建一个集群外部的负载均衡器，并为使用该负载均衡器的 IP 地址作为服务的访问地址。此时 ClusterIP 和 NodePort 的访问方式仍然可用。 Service是一个抽象层，它通过 LabelSelector 选择了一组 Pod（容器组），把这些 Pod 的指定端口公布到到集群外部，并支持负载均衡和服务发现。 公布 Pod 的端口以使其可访问 在多个 Pod 间实现负载均衡 使用 Label 和 LabelSelector 服务和标签 下图中有两个服务Service A(黄色虚线)和Service B(蓝色虚线) Service A 将请求转发到 IP 为 10.10.10.1 的Pod上， Service B 将请求转发到 IP 为 10.10.10.2、10.10.10.3、10.10.10.4 的Pod上。 Service 将外部请求路由到一组 Pod 中，它提供了一个抽象层，使得 Kubernetes 可以在不影响服务调用者的情况下，动态调度容器组 Service使用 Labels、LabelSelector (标签和选择器 匹配一组 Pod。Labels（标签）是附加到 Kubernetes 对象的键/值对，其用途有多种： 将 Kubernetes 对象（Node、Deployment、Pod、Service等）指派用于开发环境、测试环境或生产环境 嵌入版本标签，使用标签区别不同应用软件版本 使用标签对 Kubernetes 对象进行分类 Labels（标签）和 LabelSelector（标签选择器）之间的关联关系 Deployment B 含有 LabelSelector 为 app=B 通过此方式声明含有 app=B 标签的 Pod 与之关联 通过 Deployment B 创建的 Pod 包含标签为 app=B Service B 通过标签选择器 app=B 选择可以路由的 Pod Labels（标签）可以在创建 Kubernetes 对象时附加上去，也可以在创建之后再附加上去。任何时候都可以修改一个 Kubernetes 对象的 Labels（标签） nginx Deployment 创建一个 Service创建nginx的Deployment中定义了Labels，如下：1234metadata: #译名为元数据，即Deployment的一些基本属性和信息 name: nginx-deployment #Deployment的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组 app: nginx #为该Deployment设置key为app，value为nginx的标签 创建文件 nginx-service.yaml12345678910111213141516apiVersion: v1kind: Servicemetadata: name: nginx-service #Service 的名称 labels: #Service 自己的标签 app: nginx #为该 Service 设置 key 为 app，value 为 nginx 的标签spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: nginx #选择包含标签 app:nginx 的 Pod ports: - name: nginx-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 32600 #通过任意节点的 32600 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer 执行命令 1234567kubectl apply -f nginx-service.yaml#检查执行结果kubectl get services -o wide #可查看到名称为 nginx-service 的服务。#访问服务curl &lt;任意节点的 IP&gt;:32600","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"查看Pods/Nodes","slug":"查看Pods-Nodes","date":"2020-03-25T06:05:21.000Z","updated":"2020-03-25T06:21:04.946Z","comments":true,"path":"2020/03/25/查看Pods-Nodes/","link":"","permalink":"http://yoursite.com/2020/03/25/%E6%9F%A5%E7%9C%8BPods-Nodes/","excerpt":"了解 Pod 和 Node","text":"了解 Pod 和 Node Pod​ Pod中的容器共享 IP 地址和端口空间（同一 Pod 中的不同 container 端口不能相互冲突），始终位于同一位置并共同调度，并在同一节点上的共享上下文中运行。（同一个Pod内的容器可以localhost + 端口互相访问）。 ​ 当在 k8s 上创建 Deployment 时，会在集群上创建包含容器的 Pod (而不是直接创建容器)。每个Pod都与运行它的 worker 节点（Node）绑定，并保持在那里直到终止或被删除。如果节点（Node）发生故障，则会在群集中的其他可用节点（Node）上运行相同的 Pod（从同样的镜像创建 Container，使用同样的配置，IP 地址不同，Pod 名字不同）。 Pod（容器组）是 k8s 集群上的最基本的单元。 Pod 是一组容器（可包含一个或多个应用程序容器），以及共享存储（卷 Volumes）、IP 地址和有关如何运行容器的信息。 如果多个容器紧密耦合并且需要共享磁盘等资源，则应该被部署在同一个Pod（容器组）中 Node Pod（容器组）总是在 Node（节点） 上运行。 Node（节点）是 kubernetes 集群中的计算机，可以是虚拟机或物理机。 每个 Node（节点）都由 master 管理。 一个 Node（节点）可以有多个Pod（容器组），kubernetes master 会根据每个 Node（节点）上可用资源的情况，自动调度 Pod（容器组）到最佳的 Node（节点）上。 每个Node（节点）至少运行： Kubelet，负责 master 节点和 worker 节点之间通信的进程；管理 Pod（容器组）和 Pod（容器组）内运行的 Container（容器）。 容器运行环境（如Docker）负责下载镜像、创建和运行容器等。 相关命令操作kubectl get - 显示资源列表1234567891011121314151617# kubectl get 资源类型#获取类型为Deployment的资源列表kubectl get deployments#获取类型为Pod的资源列表kubectl get pods#获取类型为Node的资源列表kubectl get nodes# 查看所有名称空间的 Deploymentkubectl get deployments -Akubectl get deployments --all-namespaces# 查看 kube-system 名称空间的 Deploymentkubectl get deployments -n kube-system kubectl describe - 显示有关资源的详细信息1234567# kubectl describe 资源类型 资源名称#查看名称为nginx-XXXXXX的Pod的信息kubectl describe pod nginx-XXXXXX #查看名称为nginx的Deployment的信息kubectl describe deployment nginx kubectl logs - 查看pod中的容器的打印日志（和命令docker logs 类似）12# kubectl logs Pod名称kubectl logs -f nginx-pod-XXXXXXX kubectl exec - 在pod中的容器环境内执行命令(和命令docker exec 类似)1234# kubectl exec Pod名称 操作命令# 在名称为nginx-pod-xxxxxx的Pod中运行bashkubectl exec -it nginx-pod-xxxxxx /bin/bash","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s部署一个应用程序","slug":"k8s部署一个应用程序","date":"2020-03-25T05:49:00.000Z","updated":"2020-03-25T05:59:21.447Z","comments":true,"path":"2020/03/25/k8s部署一个应用程序/","link":"","permalink":"http://yoursite.com/2020/03/25/k8s%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"使用 kubectl 在 k8s 上部署第一个应用程序。","text":"使用 kubectl 在 k8s 上部署第一个应用程序。 Deployment概念​ 通过发布 Deployment，可以创建应用程序 (docker image) 的实例 (docker container)，这个实例会被包含在称为 Pod 的概念中，Pod 是 k8s 中最小可管理单元。 ​ 在 k8s 集群中发布 Deployment 后，Deployment 将指示 k8s 如何创建和更新应用程序的实例，master 节点将应用程序实例调度到集群中的具体的节点上。 ​ 创建应用程序实例后，Kubernetes Deployment Controller 会持续监控这些实例。如果运行实例的 worker 节点关机或被删除，则 Kubernetes Deployment Controller 将在群集中资源最优的另一个 worker 节点上重新创建一个新的实例。这提供了一种自我修复机制来解决机器故障或维护问题。 ​ 通过创建应用程序实例并确保它们在集群节点中的运行实例个数，Kubernetes Deployment 提供了一种完全不同的方式来管理应用程序。 Deployment 处于 master 节点上，通过发布 Deployment，master 节点会选择合适的 worker 节点创建 Container（即图中的正方体），Container 会被包含在 Pod （即蓝色圆圈）里。 部署 nginx Deployment创建文件 nginx-deployment.yaml12345678910111213141516171819apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本kind: Deployment #该配置的类型，我们使用的是 Deploymentmetadata: #译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx #为该Deployment设置key为app，value为nginx的标签spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: nginx #container的名称 image: nginx:1.7.9 #使用镜像nginx:1.7.9创建container，该container默认80端口可访问 应用 YAML 文件1kubectl apply -f nginx-deployment.yaml 查看部署结果12345# 查看 Deploymentkubectl get deployments# 查看 Podkubectl get pods","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"kubectl 命令技巧大全","slug":"kubectl 命令技巧大全","date":"2020-03-20T03:13:20.000Z","updated":"2020-03-20T07:20:09.424Z","comments":true,"path":"2020/03/20/kubectl 命令技巧大全/","link":"","permalink":"http://yoursite.com/2020/03/20/kubectl%20%E5%91%BD%E4%BB%A4%E6%8A%80%E5%B7%A7%E5%A4%A7%E5%85%A8/","excerpt":"一些基本的kubernets操作命令","text":"一些基本的kubernets操作命令 kubectl 命令技巧大全Kubectl 自动补全12345yum install -y bash-completionsource &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completionsource &lt;(kubectl completion bash) 创建对象Kubernetes 的清单文件可以使用 json 或 yaml 格式定义。可以以 .yaml、.yml、或者 .json 为扩展名。 1234567891011$ kubectl create -f .&#x2F;my-manifest.yaml # 创建资源$ kubectl create -f .&#x2F;my1.yaml -f .&#x2F;my2.yaml # 使用多个文件创建资源$ kubectl create -f .&#x2F;dir # 使用目录下的所有清单文件来创建资源$ kubectl create -f https:&#x2F;&#x2F;git.io&#x2F;vPieo # 使用 url 来创建资源$ kubectl run nginx --image&#x3D;nginx # 启动一个 nginx 实例$ kubectl explain pods,svc # 获取 pod 和 svc 的文档 显示和查找资源 列出所有 namespace 中的所有service 1$ kubectl get services 列出所有 namespace 中的所有 pod 1$ kubectl get pods --all-namespaces 列出所有 pod 并显示详细信息 1$ kubectl get pods -o wide 列出指定 deployment 1$ kubectl get deployment my-dep 列出该 namespace 中的所有 pod 包括未初始化的 1$ kubectl get pods --include-uninitialized 使用详细输出来描述命令 12345 $ kubectl describe nodes my-node $ kubectl describe pods my-pod $ kubectl get services --sort-by&#x3D;.metadata.name 根据重启次数排序列出 pod 1$ kubectl get pods --sort-by&#x3D;&#39;.status.containerStatuses[0].restartCount&#39; 获取所有具有 app=cassandra 的 pod 中的 version 标签 1$ kubectl get pods --selector&#x3D;app&#x3D;cassandra rc -o \\ jsonpath&#x3D;&#39;&#123;.items[*].metadata.labels.version&#125;&#39; 获取所有节点的 ExternalIP 1$ kubectl get nodes -o jsonpath&#x3D;&#39;&#123;.items[*].status.addresses[?(@.type&#x3D;&#x3D;&quot;ExternalIP&quot;)].address&#125;&#39; 列出属于某个 PC 的 Pod 的名字，“jq”命令用于转换复杂的 jsonpath，参考 https://stedolan.github.io/jq/ 123 $ sel&#x3D;$&#123;$(kubectl get rc my-rc --output&#x3D;json | jq -j &#39;.spec.selector | to_entries | .[] | &quot;\\(.key)&#x3D;\\(.value),&quot;&#39;)%?&#125; $ echo $(kubectl get pods --selector&#x3D;$sel --output&#x3D;jsonpath&#x3D;&#123;.items..metadata.name&#125;) 查看哪些节点已就绪 1$ JSONPATH&#x3D;&#39;&#123;range .items[*]&#125;&#123;@.metadata.name&#125;:&#123;range @.status.conditions[*]&#125;&#123;@.type&#125;&#x3D;&#123;@.status&#125;;&#123;end&#125;&#123;end&#125;&#39; \\ &amp;&amp; kubectl get nodes -o jsonpath&#x3D;&quot;$JSONPATH&quot; | grep &quot;Ready&#x3D;True&quot; 列出当前 Pod 中使用的 Secret 1$ kubectl get pods -o json | jq &#39;.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name&#39; | grep -v null | sort | uniq 更新资源滚动更新 pod frontend-v1 1$ kubectl rolling-update frontend-v1 -f frontend-v2.json 更新资源名称并更新镜像 1$ kubectl rolling-update frontend-v1 frontend-v2 --image&#x3D;image:v2 更新 frontend pod 中的镜像 1$ kubectl rolling-update frontend --image&#x3D;image:v2 退出已存在的进行中的滚动更新 1$ kubectl rolling-update frontend-v1 frontend-v2 --rollback 基于 stdin 输入的 JSON 替换 pod 1$ cat pod.json | kubectl replace -f - 强制替换，删除后重新创建资源。会导致服务中断。 1$ kubectl replace --force -f .&#x2F;pod.json 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口 1$ kubectl expose rc nginx --port&#x3D;80 --target-port&#x3D;8000 更新单容器 pod 的镜像版本（tag）到 v4 1$ kubectl get pod mypod -o yaml | sed &#39;s&#x2F;\\(image: myimage\\):.*$&#x2F;\\1:v4&#x2F;&#39; | kubectl replace -f - 添加标签 1$ kubectl label pods my-pod new-label&#x3D;awesome 添加注解 1$ kubectl annotate pods my-pod icon-url&#x3D;http:&#x2F;&#x2F;goo.gl&#x2F;XXBTWq 自动扩展 deployment “foo” 1$ kubectl autoscale deployment foo --min&#x3D;2 --max&#x3D;10 删除资源# 删除 pod.json 文件中定义的类型和名称的 pod 1$ kubectl delete -f .&#x2F;pod.json 删除名为“baz”的 pod 和名为“foo”的 service 1$ kubectl delete pod,service baz foo 删除具有 name=myLabel 标签的 pod 和 serivce 1$ kubectl delete pods,services -l name&#x3D;myLabel 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的 1$ kubectl delete pods,services -l name&#x3D;myLabel --include-uninitialized 删除 my-ns namespace 下的所有 pod 和 serivce，包括尚未初始化的 1$ kubectl -n my-ns delete po,svc --all 与运行中的Pod交互# dump 输出 pod 的日志（stdout） 1$ kubectl logs my-pod dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） 1$ kubectl logs my-pod -c my-container 流式输出 pod 的日志（stdout） 1$ kubectl logs -f my-pod 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） 1$ kubectl logs -f my-pod -c my-container 交互式 shell 的方式运行 pod 1$ kubectl run -i --tty busybox --image&#x3D;busybox -- sh 连接到运行中的容器 1$ kubectl attach my-pod -i 转发 pod 中的 6000 端口到本地的 5000 端口 1$ kubectl port-forward my-pod 5000:6000 在已存在的容器中执行命令（只有一个容器的情况下） 1$ kubectl exec my-pod -- ls &#x2F; 在已存在的容器中执行命令（pod 中有多个容器的情况下） 1$ kubectl exec my-pod -c my-container -- ls &#x2F; 显示指定 pod 和容器的指标度量 1$ kubectl top pod POD_NAME --containers 与节点和集群交互# 标记 my-node 不可调度 1$ kubectl cordon my-node 清空 my-node 以待维护 1$ kubectl drain my-node 标记 my-node 可调度 1$ kubectl uncordon my-node 显示 my-node 的指标度量 1$ kubectl top node my-node 显示 master 和服务的地址 1$ kubectl cluster-info 将当前集群状态输出到 stdout 1$ kubectl cluster-info dump 将当前集群状态输出到 /path/to/cluster-state 1$ kubectl cluster-info dump --output-directory&#x3D;&#x2F;path&#x2F;to&#x2F;cluster-state 如果该键和影响的污点（taint）已存在，则使用指定的值替换 1$ kubectl taint nodes foo dedicated&#x3D;special-user:NoSchedule 资源类型 缩写别名 clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies nodes no statefulsets persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses kubectl get - 显示资源列表#获取类型为Deployment的资源列表 1kubectl get deployments #获取类型为Pod的资源列表 1kubectl get pods #获取类型为Node的资源列表 1kubectl get nodes 名称空间在命令后增加 -A 或 –all-namespaces 可查看所有名称空间中的对象，使用参数 -n 可查看指定名称空间的对象，例如 # 查看所有名称空间的 Deployment 12kubectl get deployments -A kubectl get deployments --all-namespaces 查看 kube-system 名称空间的 Deployment 1kubectl get deployments -n kube-system 检查 kubectl 是否知道集群地址及凭证 1$ kubectl config view 通过 kubectl cluster-info 命令获得这些服务列表： 12345[root@ebs ~]# kubectl cluster-info Kubernetes master is running at https:&#x2F;&#x2F;172.16.121.88:6443 KubeDNS is running at https:&#x2F;&#x2F;172.16.121.88:6443&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"Kubernetes核心概念","slug":"Kubernetes核心概念","date":"2020-03-20T03:10:20.000Z","updated":"2020-03-25T09:26:08.403Z","comments":true,"path":"2020/03/20/Kubernetes核心概念/","link":"","permalink":"http://yoursite.com/2020/03/20/Kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","excerpt":"Cluster，Pod，Label， Replication Controller ，Service …","text":"Cluster，Pod，Label， Replication Controller ，Service … 什么是Kubernetes？Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。如果你曾经用过Docker容器技术部署容器，那么可以将Docker看成Kubernetes内部使用的低级别组件。Kubernetes不仅仅支持Docker，还支持Rocket，这是另一种容器技术。 使用Kubernetes可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它，等等… 集群集群是一组节点，这些节点可以是物理服务器或者虚拟机，之上安装了Kubernetes平台。下图展示这样的集群。注意该图为了强调核心概念有所简化。这里可以看到一个典型的Kubernetes架构图。 上图可以看到如下组件，使用特别的图标表示Service和Label： PodContainer（容器） Labe （标签） Replication Controller（复制控制器） Service（服务） Node（节点） Kubernetes Master（Kubernetes主节点） PodPod（上图绿色方框）安排在节点上，包含一组容器和卷。同一个Pod里的容器共享同一个网络命名空间，可以使用localhost互相通信。Pod是短暂的，不是持续性实体。你可能会有这些问题： 如果Pod是短暂的，那么我怎么才能持久化容器数据使其能够跨重启而存在呢？ 是的，Kubernetes支持 卷 的概念，因此可以使用持久化的卷类型。 是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么？可以手动创建单个Pod，但是也可以使用Replication Controller使用Pod模板创建出多份拷贝，下文会详细介绍。 如果Pod是短暂的，那么重启时IP地址可能会改变，那么怎么才能从前端容器正确可靠地指向后台容器呢？这时可以使用Service，下文会详细介绍。 Label正如图所示，一些Pod有Label 。一个Label是attach到Pod的一对键/值对，用来传递用户定义的属性。比如，你可能创建了一个”tier”和“app”标签，通过Label（tier=frontend, app=myapp）来标记前端Pod容器，使用Label（tier=backend, app=myapp）标记后台Pod。然后可以使用 [Selectors] 选择带有特定Label的Pod，并且将Service或者Replication Controller应用到上面。 Replication Controller是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么，能否将Pods划到逻辑组里？ Replication Controller确保任意时间都有指定数量的Pod“副本”在运行。如果为某个Pod创建了Replication Controller并且指定3个副本，它会创建3个Pod，并且持续监控它们。如果某个Pod不响应，那么Replication Controller会替换它，保持总数为3.如下面的动画所示： 如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动 [升级] 时很有用。 当创建Replication Controller时，需要指定两个东西： Pod模板：用来创建Pod副本的模板 Label：Replication Controller需要监控的Pod的标签。现在已经创建了Pod的一些副本，那么在这些副本上如何均衡负载呢？我们需要的是Service。 TIP 最新 Kubernetes 版本里，推荐使用 Deployment Service如果Pods是短暂的，那么重启时IP地址可能会改变，怎么才能从前端容器正确可靠地指向后台容器呢？ [Service] 抽象 现在，假定有2个后台Pod，并且定义后台Service的名称为‘backend-service’，label选择器为(tier=backend, app=myapp) 的Service会完成如下两件重要的事情： 会为Service创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。 现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod的哪一个呢？Service在这2个后台Pod之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个Node上运行的代理（kube-proxy）完成。 下述动画展示了Service的功能。注意该图作了很多简化。如果不进入网络配置，那么达到透明的负载均衡目标所涉及的底层网络和路由相对先进。如果有兴趣，有更深入的介绍。 每个节点都运行如下Kubernetes关键组件： Kubelet：是主节点代理。 Kube-proxy：Service使用其将链接路由到Pod，如上文所述。 Docker或Rocket：Kubernetes使用的容器技术来创建容器。 Kubernetes Master集群拥有一个Kubernetes Master（紫色方框）。Kubernetes Master提供集群的独特视角，并且拥有一系列组件，比如Kubernetes API Server。API Server提供可以用来和集群交互的REST端点。master节点包括用来创建和复制Pod的Replication Controller。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"hexo 使用","slug":"hexo使用","date":"2020-03-19T09:18:36.000Z","updated":"2020-03-23T07:14:57.409Z","comments":true,"path":"2020/03/19/hexo使用/","link":"","permalink":"http://yoursite.com/2020/03/19/hexo%E4%BD%BF%E7%94%A8/","excerpt":"hexo 搭建、部署、操作；","text":"hexo 搭建、部署、操作； _config.yml配置git 1234deploy: type: &#39;git&#39; repo: git@github.com:yourname&#x2F;yourname.github.io.git branch: master 上传github，推送文件步骤：hexo clean hexo c 清除缓存文件 (db.json) 和已生成的静态文件 (public) hexo generate hexo g 生成静态文件 hexo deploy hexo d 部署网站 执行端口启动： 1hexo s -i 0.0.0.0 -p 8080 绑定个人域名： 解析域名注意，博客网址中必须使用你github的用户名. 布局（Layout） Hexo 有三种默认布局：post、page 和 draft。在创建者三种不同类型的文件时，它们将会被保存到不同的路径；而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。 布局 路径 post source/_posts page source draft source/_drafts hexo init 1hexo init [ folder] 新建一个网站。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站。 hexo new 1hexo new [layout] &lt;title&gt; 新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。 hexo new “post title with whitespace” 参数 描述 -p, –path 自定义新文章的路径 -r, –replace 如果存在同名文章，将其替换 -s, –slug 文章的 Slug，作为新文章的文件名和发布后的 URL 默认情况下，Hexo 会使用文章的标题来决定文章文件的路径。对于独立页面来说，Hexo 会创建一个以标题为名字的目录，并在目录中放置一个 index.md 文件。你可以使用 –path 参数来覆盖上述行为、自行决定文件的目录： 1hexo new page --path about&#x2F;me &quot;About me&quot; 以上命令会创建一个 source/about/me.md 文件，同时 Front Matter 中的 title 为 “About me” 注意！title 是必须指定的！如果你这么做并不能达到你的目的： 1hexo new page --path about&#x2F;me 此时 Hexo 会创建 source/_posts/about/me.md，同时 me.md 的 Front Matter 中的 title 为 “page”。这是因为在上述命令中，hexo-cli 将 page 视为指定文章的标题、并采用默认的 layout。","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]}]}