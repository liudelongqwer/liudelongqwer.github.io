{"meta":{"title":"拒绝再玩","subtitle":"","description":"","author":"duoyu","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"使用名称空间共享集群","slug":"使用名称空间共享集群","date":"2020-03-28T05:42:22.000Z","updated":"2020-03-28T06:23:37.173Z","comments":true,"path":"2020/03/28/使用名称空间共享集群/","link":"","permalink":"http://yoursite.com/2020/03/28/%E4%BD%BF%E7%94%A8%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E5%85%B1%E4%BA%AB%E9%9B%86%E7%BE%A4/","excerpt":"查看、创建、删除、使用namespace","text":"查看、创建、删除、使用namespace 使用名称空间共享集群一、查看名称空间查看集群中的名称空间列表： 123456[root@k8s-master k8s-yamls]# kubectl get namespacesNAME STATUS AGEdefault Active 23hkube-node-lease Active 23hkube-public Active 23hkube-system Active 23h Kubernetes 安装成功后，默认有初始化了三个名称空间： default 默认名称空间，如果 Kubernetes 对象中不定义 metadata.namespace 字段，该对象将放在此名称空间下 kube-system Kubernetes系统创建的对象放在此名称空间下 kube-public 此名称空间自动在安装集群是自动创建，并且所有用户都是可以读取的（即使是那些未登录的用户）。主要是为集群预留的，例如，某些情况下，某些Kubernetes对象应该被所有集群用户看到。 查看名称空间详细信息： 123456789[root@k8s-master k8s-yamls]# kubectl describe namespaces kube-systemName: kube-systemLabels: &lt;none&gt;Annotations: &lt;none&gt;Status: ActiveNo resource quota.No resource limits. Resource quota 汇总了名称空间中使用的资源总量，并指定了集群管理员定义该名称空间最多可以使用的资源量 Limit range 定义了名称空间中某种具体的资源类型的最大、最小值 名称空间可能有两种状态（phase）： Active 名称空间正在使用中 Termining 名称空间正在被删除，不能再向其中创建新的对象 二、创建名称空间使用kubectl有两种方式创建名称空间 1.通过 yaml 文件，创建文件 my-namespace.yaml 1234567apiVersion: v1kind: Namespacemetadata: name: &lt;名称空间的名字&gt;#执行命令kubectl create -f ./my-namespace.yaml 直接使用命令创建名称空间： 1kubectl create namespace &lt;名称空间的名字&gt; 注意： 名称空间可以定义一个可选项字段 finalizers，在名称空间被删除时，用来清理相关的资源。 如果定义了一个不存在的 finalizer，仍然可以成功创建名称空间，但是当删除该名称空间时，将卡在 Terminating 状态。 三、删除名称空间1kubectl delete namespaces &lt;名称空间的名字&gt; 注意： 该操作将删除名称空间中的所有内容（ 此删除操作是异步的，名称空间会停留在 Terminating 状态一段时间。 ） 四、使用名称空间切分集群理解 default 名称空间默认情况下，安装Kubernetes集群时，会初始化一个 default 名称空间，用来将承载那些未指定名称空间的 Pod、Service、Deployment等对象 创建新的名称空间假设企业使用同一个集群作为开发环境和生产环境（注意：通常开发环境和生产环境是物理隔绝的）： 开发团队期望有一个集群中的空间，以便他们可以查看查看和使用他们创建的 Pod、Service、Deployment等。在此空间中，Kubernetes对象被创建又被删除，为了适应敏捷开发的过程，团队中的许多人都可以在此空间内做他们想做的事情。 运维团队也期望有一个集群中的空间，在这里，将有严格的流程控制谁可以操作 Pod、Service、Deployment等对象，因为这些对象都直接服务于生产环境。 此时，可以将一个Kubernetes集群切分成两个名称空间：development 和 production。创建名称空间的 yaml 文件如下所示： 123456apiVersion: v1kind: Namespacemetadata: name: development labels: name: development 执行命令以创建 development 名称空间： 12345678910[root@k8s-master k8s-yamls]# kubectl create -f dev.yaml namespace/development created[root@k8s-master k8s-yamls]# kubectl get namespaces --show-labelsNAME STATUS AGE LABELSdefault Active 23h &lt;none&gt;development Active 2m10s name=developmentkube-node-lease Active 23h &lt;none&gt;kube-public Active 23h &lt;none&gt;kube-system Active 23h &lt;none&gt;production Active 67s name=production 在每个名称空间中创建 PodKubernetes名称空间为集群中的 Pod、Service、Deployment 提供了一个作用域。可以限定使用某个名称空间的用户不能看到另外一个名称空间中的内容。我们可以在 development 名称空间中创建一个简单的 Deployment 和 Pod 来演示这个特性。 1.执行命令以检查当前的 kubectl 上下文 1234567891011121314151617181920[root@k8s-master k8s-yamls]# kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority-data: DATA+OMITTED server: https://192.168.154.144:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 2.执行命令 12[root@k8s-master k8s-yamls]# kubectl config current-contextkubernetes-admin@kubernetes 3.接下来，为 kubectl 定义一个上下文，以便在不同的名称空间中工作。cluster 和 user 字段的取值从前面的 current context 复制过来： 1234[root@k8s-master k8s-yamls]# kubectl config set-context dev --namespace=development --cluster=kubernetes-admin@kubernetes --user=kubernetes-admin@kubernetesContext \"dev\" created.[root@k8s-master k8s-yamls]# kubectl config set-context prod --namespace=production --cluster=kubernetes-admin@kubernetes --user=kubernetes-admin@kubernetesContext \"prod\" created. 上面的命令创建了两个 kubectl 的上下文，可以在两个不同的名称空间中工作： 切换到 development 名称空间： 123456[root@k8s-master k8s-yamls]# kubectl config use-context devSwitched to context \"dev\".#验证[root@k8s-master k8s-yamls]# kubectl config current-contextdev 此时，通过 kubectl 向 Kubernetes 集群发出的所有指令都限定在名称空间 development 里 在不同的namespace里工作创建一个 nginx 1234567891011kubectl run snowflake --image=nginx:1.7.9 --replicas=2#刚刚创建的 Deployment 副本数为 2，运行了一个 nginx 容器。kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEsnowflake 2 2 2 2 2mkubectl get pods -l run=snowflakeNAME READY STATUS RESTARTS AGEsnowflake-3968820950-9dgr8 1/1 Running 0 2msnowflake-3968820950-vgc4n 1/1 Running 0 2m 此时，开发人员可以做任何他想要做的操作，所有操作都限定在名称空间 development 里，而无需担心影响到 production 名称空间中的内容 用户在一个名称空间创建的内容对于另外一个名称空间来说是不可见的。同时也可以为不同的名称空间定义不同的访问权限控制。 为什么需要名称空间一个Kubernetes集群应该可以满足多组用户的不同需要。Kubernetes名称空间可以使不同的项目、团队或客户共享同一个 Kubernetes 集群。实现的方式是，提供： namespace的作用域 为不同的名称空间定义不同的授权方式和资源分配策略 Resource Quota 和 resource limit range 每一个用户组都期望独立于其他用户组进行工作。通过名称空间，每个用户组拥有自己的： Kubernetes 对象（Pod、Service、Deployment等） 授权（谁可以在该名称空间中执行操作） 资源分配（该用户组或名称空间可以使用集群中的多少计算资源） 可能的使用情况有： 集群管理员通过一个Kubernetes集群支持多个用户组 集群管理员将集群中某个名称空间的权限分配给用户组中的受信任的成员 集群管理员可以限定某一个用户组可以消耗的资源数量，以避免其他用户组受到影响 集群用户可以使用自己的Kubernetes对象，而不会与集群中的其他用户组相互干扰","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"操作Kubernetes","slug":"操作Kubernetes","date":"2020-03-28T02:24:30.000Z","updated":"2020-03-28T03:52:42.373Z","comments":true,"path":"2020/03/28/操作Kubernetes/","link":"","permalink":"http://yoursite.com/2020/03/28/%E6%93%8D%E4%BD%9CKubernetes/","excerpt":"什么是Kubernetes对象，管理Kubernetes对象，名称（name），名称空间（namespace）的概念","text":"什么是Kubernetes对象，管理Kubernetes对象，名称（name），名称空间（namespace）的概念 一、什么是Kubernetes对象​ Kubernetes对象指的是Kubernetes系统的持久化实体，所有这些对象合起来，代表了集群的实际情况。常规的应用里，应用程序的数据存储在数据库中，Kubernetes将其数据以Kubernetes对象的形式通过 api server存储在 etcd 中。具体来说，这些数据（Kubernetes对象）描述了： 集群中运行了哪些容器化应用程序（以及在哪个节点上运行） 集群中对应用程序可用的资源 应用程序相关的策略定义，例如，重启策略、升级策略、容错策略 其他Kubernetes管理应用程序时所需要的信息 操作 Kubernetes 对象（创建、修改、删除）的方法主要有： kubectl 命令行工具 kuboard 图形界面工具 kubectl、kuboard 最终都通过调用 kubernetes API 来实现对 Kubernetes 对象的操作。 对象的spec和status每一个 Kubernetes 对象都包含了两个重要的字段： spec 必须由您来提供，描述了您对该对象所期望的 目标状态 status 只能由 Kubernetes 系统来修改，描述了该对象在 Kubernetes 系统中的 实际状态 Kubernetes通过对应的控制器，不断地使实际状态趋向于期望的目标状态。 ​ 例如，一个 Kubernetes Deployment 对象可以代表一个应用程序在集群中的运行状态。当创建 Deployment 对象时，可以通过 Deployment 的 spec 字段指定需要运行应用程序副本数（假设为3）。Kubernetes 从 Deployment 的 spec 中读取这些信息，并创建指定容器化应用程序的 3 个副本，再将实际的状态更新到 Deployment 的 status 字段。Kubernetes 系统将不断地比较 实际状态 staus 和 目标状态 spec 之间的差异，并根据差异做出对应的调整。例如，如果任何一个副本运行失败了，Kubernetes 将启动一个新的副本，以替代失败的副本 描述Kubernetes对象在 Kubernetes 中创建一个对象时，必须提供 该对象的 spec 字段，通过该字段描述您期望的 目标状态 该对象的一些基本信息，例如名字 如果使用 kubectl 创建对象，必须编写 .yaml 格式的文件下面是一个 kubectl 可以使用的 .yaml 文件： 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 # 运行 2 个容器化应用程序副本 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 使用 kube apply 命令可以创建该 .yaml 文件中的 Deployment 对象： 12345678910[root@k8s-master k8s-yamls]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deployment created[root@k8s-master k8s-yamls]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-deployment-54f57cf6bf-528xn 1/1 Running 0 4m35snginx-deployment-54f57cf6bf-bgclh 1/1 Running 0 4m35s#删除[root@k8s-master k8s-yamls]# kubectl delete -f nginx-deployment.yaml deployment.apps \"nginx-deployment\" deleted 必填字段在上述的 .yaml 文件中，如下字段是必须填写的： apiVersion 用来创建对象时所使用的Kubernetes API版本 kind 被创建对象的类型 metadata 用于唯一确定该对象的元数据：包括 name 和 namespace，如果 namespace 为空，则默认值为 default spec 描述您对该对象的期望状态 二、管理Kubernetes对象三种管理方式 管理方式 操作对象 推荐的环境 参与编辑的人数 学习曲线 指令性的命令行 Kubernetes对象 开发环境 1+ 最低 指令性的对象配置 单个 yaml 文件 生产环境 1 适中 声明式的对象配置 包含多个 yaml 文件的多个目录 生产环境 1+ 最高 指令性的命令行当使用指令性的命令行（imperative commands）时，用户通过向 kubectl 命令提供参数的方式，直接操作集群中的 Kubernetes 对象。此时，用户无需编写或修改 .yaml 文件。 这是在 Kubernetes 集群中执行一次性任务的一个简便的办法。由于这种方式直接修改 Kubernetes 对象，也就无法提供历史配置查看的功能。 例子创建一个 Deployment 对象，以运行一个 nginx 实例： 1kubectl run nginx --image nginx 下面的命令完成了相同的任务，但是命令格式不同： 1kubectl create deployment nginx --image nginx 优缺点与编写 .yaml 文件进行配置的方式相比的优势： 命令简单，易学易记，只需要一个步骤，就可以对集群执行变更 缺点： 使用命令，无法进行变更review的管理；不提供日志审计；没有创建新对象的模板 指令性的对象配置使用指令性的对象配置（imperative object configuration）时，需要向 kubectl 命令指定具体的操作（create,replace,apply,delete等），可选参数以及至少一个配置文件的名字。配置文件中必须包括一个完整的对象的定义，可以是 yaml 格式，也可以是 json 格式。 通过配置文件创建对象 1kubectl create -f nginx.yaml 删除两个配置文件中的对象 1kubectl delete -f nginx.yaml -f redis.yaml 直接使用配置文件中的对象定义，替换Kubernetes中对应的对象： 1kubectl replace -f nginx.yaml 声明式的对象配置当使用声明式的对象配置时，用户操作本地存储的Kubernetes对象配置文件，然而，在将文件传递给 kubectl 命令时，并不指定具体的操作，由 kubectl 自动检查每一个对象的状态并自行决定是创建、更新、还是删除该对象。使用这种方法时，可以直接针对一个或多个文件目录进行操作（对不同的对象可能需要执行不同的操作）。 处理 configs 目录中所有配置文件中的Kubernetes对象，根据情况创建对象、或更新Kubernetes中已经存在的对象。可以先执行 diff 指令查看具体的变更，然后执行 apply 指令执行变更： 123456kubectl diff -f configs/kubectl apply -f configs/#递归处理目录中的内容kubectl diff -R -f configs/kubectl apply -R -f configs/ 三、名称 Kubernetes REST API 中，所有的对象都是通过 name 和 UID 唯一性确定。 Names 同一个名称空间下，同一个类型的对象，可以通过 name 唯一性确定。如果删除该对象之后，可以再重新创建一个同名对象。 例如，下面的配置文件定义了一个 name 为 nginx-demo 的 Pod，该 Pod 包含一个 name 为 nginx 的容器： 12345678910apiVersion: v1kind: Podmetadata: name: nginx-demospec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 UIDsUID 是由 Kubernetes 系统生成的，唯一标识某个 Kubernetes 对象的字符串。 Kubernetes集群中，每创建一个对象，都有一个唯一的 UID。用于区分多次创建的同名对象（如前所述，按照名字删除对象后，重新再创建同名对象时，两次创建的对象 name 相同，但是 UID 不同。） 四、名称空间何时使用名称空间 名称空间的用途是，为不同团队的用户（或项目）提供虚拟的集群空间（划分集群的资源），也可以用来区分开发环境/测试环境、准上线环境/生产环境。 名称空间为名称提供了作用域。名称空间内部的同类型对象不能重名，但是跨名称空间可以有同名同类型对象。名称空间不可以嵌套，任何一个Kubernetes对象只能在一个名称空间中。 在 Kubernetes 将来的版本中，同名称空间下的对象将默认使用相同的访问控制策略。 当Kubernetes对象之间的差异不大时，无需使用名称空间来区分，例如，同一个软件的不同版本，只需要使用 labels 来区分即可。 如何使用名称空间查看名称空间执行命令 kubectl get namespaces 可以查看名称空间 Kubernetes 安装成功后，默认有初始化了三个名称空间： default 默认名称空间，如果 Kubernetes 对象中不定义 metadata.namespace 字段，该对象将放在此名称空间下 kube-system Kubernetes系统创建的对象放在此名称空间下 kube-public 此名称空间自动在安装集群是自动创建，并且所有用户都是可以读取的（即使是那些未登录的用户）。主要是为集群预留的，例如，某些情况下，某些Kubernetes对象应该被所有集群用户看到。 在执行请求时设定namespace执行 kubectl 命令时，可以使用 --namespace 参数指定名称空间，例如： 12kubectl run nginx --image=nginx --namespace=&lt;your_namespace&gt;kubectl get pods --namespace=&lt;your_namespace&gt; 设置名称空间偏好可以通过 set-context 命令改变当前kubectl 上下文的名称空间，后续所有命令都默认在此名称空间下执行。 123kubectl config set-context --current --namespace=&lt;your_namespace&gt;# 验证结果kubectl config view --minify | grep namespace: 名称空间与DNS当创建一个 Service 时，Kubernetes 为其创建一个对应的DNS 条目。该 DNS 记录的格式为 &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local ，也就是说，如果在容器中只使用 &lt;service-name&gt; ，其DNS将解析到同名称空间下的 Service。这个特点在多环境的情况下非常有用，例如将开发环境、测试环境、生产环境部署在不同的名称空间下，应用程序只需要使用&lt;service-name&gt;即可进行服务发现，无需为不同的环境修改配置。如果跨名称空间访问服务，则必须使用完整的域名（fully qualified domain name，FQDN）。 并非所有对象都在名称空间里大部分的 Kubernetes 对象（例如，Pod、Service、Deployment、StatefulSet等）都必须在名称空间里。但是某些更低层级的对象，是不在任何名称空间中的，例如nodes、persistentVolumes、storageClass等 执行一下命令可查看哪些 Kubernetes 对象在名称空间里，哪些不在： 12345# 在名称空间里kubectl api-resources --namespaced=true# 不在名称空间里kubectl api-resources --namespaced=false","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"kubeadm安装Kubernetes1.16.3","slug":"kubeadm安装Kubernetes1-16-3","date":"2020-03-27T07:11:48.000Z","updated":"2020-03-27T07:39:50.190Z","comments":true,"path":"2020/03/27/kubeadm安装Kubernetes1-16-3/","link":"","permalink":"http://yoursite.com/2020/03/27/kubeadm%E5%AE%89%E8%A3%85Kubernetes1-16-3/","excerpt":"系统环境： CentOS 版本：7.7 Docker 版本：18.09.9-3 Calico 版本：v3.10 Kubernetes 版本：1.16.3 Kubernetes Newwork 模式：IPVS Kubernetes Dashboard 版本：dashboard:v2.0.0-beta6","text":"系统环境： CentOS 版本：7.7 Docker 版本：18.09.9-3 Calico 版本：v3.10 Kubernetes 版本：1.16.3 Kubernetes Newwork 模式：IPVS Kubernetes Dashboard 版本：dashboard:v2.0.0-beta6 一、更新系统内核（全部节点）由于 Docker 对系统内核有一定的要求，所以我们最好使用 yum 来更新系统软件及其内核。 1234567891011#备份本地 yum 源$ mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo_bak # 获取阿里 yum 源配置文件$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #清理 yum$ yum clean all#更新软件版本并且更新现有软件$ yum -y update 二、基础环境设置（全部节点）Kubernetes 需要一定的环境来保证正常运行，如各个节点时间同步，主机名称解析，关闭防火墙等等。 1、修改 Host分布式系统环境中的多主机通信通常基于主机名称进行，这在 IP 地址存在变化的可能 性时为主机提供了固定的访问人口，因此一般需要有专用的 DNS 服务负责解析各节点主机。考虑到此处部署的是测试集群，因此为了降低系复杂度，这里将基于 hosts 的文件进行主机名称解析。 1$ vim /etc/hosts 加入下面内容： 123192.168.2.11 k8s-master192.168.2.12 k8s-node-01192.168.2.13 k8s-node-02 2、修改 Hostnamekubernetes 中会以各个服务的 hostname 为其节点命名，所以需要进入不同的服务器修改 hostname 名称。 1234567891011#修改 192.168.2.11 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-master$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts#修改 192.168.2.12 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-node-01$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts#修改 192.168.2.13 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-node-02$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts 3、主机时间同步将各个服务器的时间同步，并设置开机启动同步时间服务。 1$ systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service 4、关闭防火墙服务关闭防火墙，并禁止开启启动。 1$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld 5、关闭并禁用SELinux关闭SELinux，并编辑／etc/sysconfig selinux 文件，以彻底禁用 SELinux 12$ setenforce 0$ sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config 查看selinux状态 1$ getenforce 6、禁用 Swap 设备关闭当前已启用的所有 Swap 设备： 1$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 编辑 fstab 配置文件，注释掉标识为 Swap 设备的所有行： 123$ vi /etc/fstab#/dev/mapper/centos-swap swap swap defaults 0 0 7、设置内核参数配置内核参数： 123456789$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10EOF 使配置生效: 12345678#挂载 br_netfilter$ modprobe br_netfilter#使配置生效$ sysctl -p /etc/sysctl.d/k8s.conf#查看是否生成相关文件$ ls /proc/sys/net/bridge 8、配置 IPVS 模块由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块： ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4 12345678910$ cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF 执行脚本并查看是否正常加载内核模块： 12345678#修改脚本权限$ chmod 755 /etc/sysconfig/modules/ipvs.modules #执行脚本$ bash /etc/sysconfig/modules/ipvs.modules #查看是否已经正确加载所需的内核模块$ lsmod | grep -e ip_vs -e nf_conntrack_ipv4 安装 ipset 1$ yum install -y ipset 9、配置资源限制123456echo \"* soft nofile 65536\" &gt;&gt; /etc/security/limits.confecho \"* hard nofile 65536\" &gt;&gt; /etc/security/limits.confecho \"* soft nproc 65536\" &gt;&gt; /etc/security/limits.confecho \"* hard nproc 65536\" &gt;&gt; /etc/security/limits.confecho \"* soft memlock unlimited\" &gt;&gt; /etc/security/limits.confecho \"* hard memlock unlimited\" &gt;&gt; /etc/security/limits.conf 10、安装依赖包以及相关工具12$ yum install -y epel-release$ yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim ntpdate libseccomp libtool-ltdl 三、安装Docker（全部节点）1、移除之前安装过的Docker1234567891011$ sudo yum -y remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-ce-cli \\ docker-engine 查看还有没有存在的 Docker 组件，一定要确保删除干净： 1$ rpm -qa | grep docker 有则通过命令 yum -y remove XXX 来删除，比如： 1$ yum remove docker-ce-cli 2、更换 Docker 的 yum 源由于官方下载速度比较慢，所以需要更改 Docker 安装的 yum 源，这里推荐用阿里镜像源： 1$ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3、显示 docker 所有可安装版本：1$ yum list docker-ce --showduplicates | sort -r 、安装指定版本 docker 注意：安装前一定要提前查询将要安装的 Kubernetes 版本是否和 Docker 版本对应。 1$ yum install -y docker-ce-18.09.9-3.el7 设置镜像存储目录，找到大点的挂载的目录进行存储 1234$ vi /lib/systemd/system/docker.service#找到这行，往后面加上存储目录，例如这里是 --graph /apps/dockerExecStart=/usr/bin/docker --graph /apps/docker 5、配置 Docker 参数和镜像加速器Kubernetes 推荐的一些 Docker 配置参数，这里配置一下。还有就是由于国内访问 Docker 仓库速度很慢，所以国内几家云厂商推出镜像加速下载的代理加速器，这里也需要配置一下。 创建 Docker 配置文件的目录： 1$ mkdir -p /etc/docker 添加配置文件： 123456789101112131415161718192021$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [ \"https://dockerhub.azk8s.cn\", \"http://hub-mirror.c.163.com\", \"https://registry.docker-cn.com\" ], \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\":\"5\" &#125;&#125;EOF 6、启动 docker 并设置 docker 开机启动启动 Docker： 1$ systemctl start docker &amp;&amp; systemctl enable docker 如果 Docker 已经启动，则需要重启 Docker： 12$ systemctl daemon-reload$ systemctl restart docker 四、安装 kubelet、kubectl、kubeadm（全部节点）1、配置可用的国内 yum 源1234567891011$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 2、安装 kubelet、kubectl、kubeadm kubelet: 在集群中的每个节点上用来启动 pod 和 container 等。 kubectl: 用来与集群通信的命令行工具。 kubeadm: 用来初始化集群的指令。 注意安装顺序，一定不要先安装 kubeadm，因为 kubeadm 会自动安装最新版本的 kubelet 与 kubectl，导致版本不一致问题。 12345678#安装 kubelet$ yum install -y kubelet-1.16.3-0#安装 kubectl$ yum install -y kubectl-1.16.3-0#安装 kubeadm$ yum install -y kubeadm-1.16.3-0 3、启动 kubelet 并配置开机启动1$ systemctl start kubelet &amp;&amp; systemctl enable kubelet 检查状态时会发现 kubelet 是 failed 状态，等初 master 节点初始化完成后即可显示正常。 五、重启服务器（全部节点）为了防止发生某些未知错误，这里我们重启下服务器，方便进行后续操作 1$ reboot 六、kubeadm 安装 kubernetes（Master 节点）创建 kubeadm 配置文件 kubeadm-config.yaml，然后需要配置一些参数： 配置 localAPIEndpoint.advertiseAddress 参数，调整为你的 Master 服务器地址。 配置 imageRepository 参数，调整 kubernetes 镜像下载地址为阿里云。 配置 networking.podSubnet 参数，调整为你要设置的网络范围。 kubeadm-config.yaml 123456789101112131415161718192021222324$ cat &gt; kubeadm-config.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta2kind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.2.11 bindPort: 6443nodeRegistration: taints: - effect: PreferNoSchedule key: node-role.kubernetes.io/master---apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationimageRepository: registry.aliyuncs.com/google_containerskubernetesVersion: v1.16.3networking: podSubnet: 10.244.0.0/16---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvsEOF kubeadm 初始化 kubernetes 集群 1$ kubeadm init --config kubeadm-config.yaml 部署日志信息： 1234567891011121314151617181920......[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.2.11:6443 --token 4udy8a.f77ai0zun477kx0p \\ --discovery-token-ca-cert-hash sha256:4645472f24b438e0ecf5964b6dcd64913f68e0f9f7458768cfb96a9ab16b4212 上面记录了完成的初始化输出的内容，根据输出的内容基本上可以看出手动初始化安装一个Kubernetes集群所需要的关键步骤。 其中有以下关键内容： [kubelet] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml” [certificates]生成相关的各种证书 [kubeconfig]生成相关的kubeconfig文件 [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到 在此处看日志可以知道，可以通过下面命令，添加 kubernetes 相关环境变量： 123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 或者直接只用命令 12345kubeadm init \\ --kubernetes-version=v1.16.3 \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.2.11 \\ --ignore-preflight-errors=Swap 提前拉取镜像 1kubeadm config images pull --config kubeadm-master.config 安装过程中遇到异常： 12Copy[preflight] Some fatal errors occurred: [ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty 直接删除/var/lib/etcd文件夹 如果初始化过程出现问题，使用如下命令重置： 1kubeadm reset 七、工作节点加入集群（Work Node 节点）根据上面 Master 节点创建 Kubernetes 集群时的日志信息，可以知道在各个节点上执行下面命令来让工作节点加入主节点： 12$ kubeadm join 192.168.2.11:6443 --token 4udy8a.f77ai0zun477kx0p \\ --discovery-token-ca-cert-hash sha256:4645472f24b438e0ecf5964b6dcd64913f68e0f9f7458768cfb96a9ab16b4212 八、部署网络插件（Master 节点）Kubernetes 中可以部署很多种网络插件，不过比较流行也推荐的有两种： Flannel： Flannel 是基于 Overlay 网络模型的网络插件，能够方便部署，一般部署后只要不出问题，一般不需要管它。 Calico： 与 Flannel 不同，Calico 是一个三层的数据中心网络方案，使用 BGP 路由协议在主机之间路由数据包，可以灵活配置网络策略。 这两种网络根据环境任选其一即可，这里使用的是 Calico，可以按下面步骤部署： 1、部署 Calico 网络插件下载 Calico 部署文件，并替换里面的网络范围为上面 kubeadm 中 networking.podSubnet 配置的值。 12345678#下载 calico 部署文件$ wget https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml #替换 calico 部署文件的 IP 为 kubeadm 中的 networking.podSubnet 参数 10.244.0.0。$ sed -i 's/192.168.0.0/10.244.0.0/g' calico.yaml#部署 Calico 插件$ kubectl apply -f calico.yaml 2、查看 Pod 是否成功启动12345678910111213141516$ kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcalico-kube-controllers-6b64bcd855-jn8pz 1/1 Running 0 2m40scalico-node-5wssd 1/1 Running 0 2m40scalico-node-7tw94 1/1 Running 0 2m40scalico-node-xzfp4 1/1 Running 0 2m40scoredns-58cc8c89f4-hv4fn 1/1 Running 0 21mcoredns-58cc8c89f4-k97x6 1/1 Running 0 21metcd-k8s-master 1/1 Running 0 20mkube-apiserver-k8s-master 1/1 Running 0 20mkube-controller-manager-k8s-master 1/1 Running 0 20mkube-proxy-9dlpz 1/1 Running 0 14mkube-proxy-krd5n 1/1 Running 0 14mkube-proxy-tntpr 1/1 Running 0 21mkube-scheduler-k8s-master 1/1 Running 0 20m 可以看到所以 Pod 都已经成功启动。 九、配置 Kubectl 命令自动补全（Master 节点）1$ yum install -y bash-completion 添加补全配置 123$ source &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion$ source &lt;(kubectl completion bash)$ echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~&#x2F;.bashrc 添加完成就可与通过输入 kubectl 后，按补全键（一般为 tab）会自动补全对应的命令。 十、查看是否开启 IPVS（Master 节点）上面全部组件都已经部署完成，不过还需要确认是否成功将网络模式设置为 IPVS，可以查看 kube-proxy 日志，在日志信息中查找是否存在 IPVS 关键字信息来确认。 12345$ kubectl get pod -n kube-system | grep kube-proxykube-proxy-9dlpz 1/1 Running 0 42mkube-proxy-krd5n 1/1 Running 0 42mkube-proxy-tntpr 1/1 Running 0 49m 选择其中一个 Pod ，查看该 Pod 中的日志信息中是否存在 ipvs 信息： 12345678910111213141516$ kubectl logs kube-proxy-9dlpz -n kube-systemI1120 18:13:46.357178 1 node.go:135] Successfully retrieved node IP: 192.168.2.13I1120 18:13:46.357265 1 server_others.go:176] Using ipvs Proxier.W1120 18:13:46.358005 1 proxier.go:420] IPVS scheduler not specified, use rr by defaultI1120 18:13:46.358919 1 server.go:529] Version: v1.16.3I1120 18:13:46.359327 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_max&#39; to 131072I1120 18:13:46.359379 1 conntrack.go:52] Setting nf_conntrack_max to 131072I1120 18:13:46.359426 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_tcp_timeout_established&#39; to 86400I1120 18:13:46.359452 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_tcp_timeout_close_wait&#39; to 3600I1120 18:13:46.359626 1 config.go:313] Starting service config controllerI1120 18:13:46.359685 1 shared_informer.go:197] Waiting for caches to sync for service configI1120 18:13:46.359833 1 config.go:131] Starting endpoints config controllerI1120 18:13:46.359889 1 shared_informer.go:197] Waiting for caches to sync for endpoints configI1120 18:13:46.460013 1 shared_informer.go:204] Caches are synced for service config I1120 18:13:46.460062 1 shared_informer.go:204] Caches are synced for endpoints config 如上，在日志中查到了 IPVS 字样，则代表使用了 IPVS 模式。 十一、集群中移除Node 如果需要从集群中移除node2这个Node执行下面的命令： 在master节点上执行： 123kubectl drain node2 --delete-local-data --force --ignore-daemonsets kubectl delete node node2 在node2上执行： 123456kubeadm reset ifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf &#x2F;var&#x2F;lib&#x2F;cni&#x2F; 十二、部署 Kubernetes Dashboard接下来我们将部署 Kubernetes 的控制看板，由于集群为 1.16.3，而 Dashboard 的稳定版本还是基于 Kubernetes 1.10 版本，所以我们直接使用 Kubernetes Dashboard 2.0.0 Bate 版本，虽然为测试版本，不过它比较新，对新版本的兼容还是比旧版本好些，坐等它稳定，这里先部署尝尝鲜。 1、创建 Dashboard 部署文件k8s-dashboard-deploy.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184$ cat &gt; k8s-dashboard-deploy.yaml &lt;&lt; EOFapiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemrules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\", \"kubernetes-dashboard-csrf\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\", \"dashboard-metrics-scraper\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\", \"dashboard-metrics-scraper\", \"http:dashboard-metrics-scraper\"] verbs: [\"get\"]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboardrules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kube-systemtype: Opaquedata: csrf: \"\"---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kube-systemtype: Opaque---kind: ConfigMapapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kube-system---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.0.0-beta6 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kube-system #设置为当前namespace volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule EOF 部署 Kubernetes Dashboard 1$ kubectl apply -f k8s-dashboard-deploy.yaml 2、创建监控信息 kubernetes-metrics-scraperk8s-dashboard-metrics.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ cat &gt; k8s-dashboard-metrics.yaml &lt;&lt; EOFkind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-metrics-scraper name: dashboard-metrics-scraper namespace: kube-systemspec: ports: - port: 8000 targetPort: 8000 selector: k8s-app: kubernetes-metrics-scraper---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-metrics-scraper name: kubernetes-metrics-scraper namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-metrics-scraper template: metadata: labels: k8s-app: kubernetes-metrics-scraper spec: containers: - name: kubernetes-metrics-scraper image: kubernetesui/metrics-scraper:v1.0.1 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 serviceAccountName: kubernetes-dashboard tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule EOF 部署 Dashboard Metrics 1$ kubectl apply -f k8s-dashboard-metrics.yaml 3、创建 Dashboard ServiceAccount访问 Kubernetes Dashboard 时会验证身份，需要提前在 Kubernetes 中创建一个一定权限的 ServiceAccount，然后获取其 Token 串，这里为了简单方便，直接创建一个与管理员绑定的服务账户，获取其 Token 串。 k8s-dashboard-rbac.yaml 123456789101112131415161718192021222324252627$ cat &gt; k8s-dashboard-token.yaml &lt;&lt; EOFkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\"roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: admin namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile EOF 部署访问的 ServiceAccount： 1$ kubectl apply -f k8s-dashboard-rbac.yaml 获取 Token： 1$ kubectl describe secret/$(kubectl get secret -n kube-system |grep admin|awk '&#123;print $1&#125;') -n kube-system 然后输入 Kubernetes 集群任意节点地址配置上面配置的 Service 的 NodePort 30001 来访问看板。输入地址https://192.168.2.11:30001进入看板页面，输入上面获取的 Token 串进行验证登录。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-控制器","slug":"k8s进阶-架构-控制器","date":"2020-03-26T08:49:58.000Z","updated":"2020-03-26T09:17:26.418Z","comments":true,"path":"2020/03/26/k8s进阶-架构-控制器/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84-%E6%8E%A7%E5%88%B6%E5%99%A8/","excerpt":"控制器不断地尝试着将当前的状态调整到目标状态","text":"控制器不断地尝试着将当前的状态调整到目标状态 控制器在 Kubernetes 中，控制器 就是上面所说的 控制循环，它不断监控着集群的状态，并对集群做出对应的变更调整。每一个控制器都不断地尝试着将 当前状态 调整到 目标状态。 控制器模式在 Kubernetes 中，每个控制器至少追踪一种类型的资源。这些资源对象中有一个 spec 字段代表了目标状态。资源对象对应的控制器负责不断地将当前状态调整到目标状态。 理论上，控制器可以自己直接执行调整动作，然而，在Kubernetes 中，更普遍的做法是，控制器发送消息到 API Server，而不是直接自己执行调整动作。 通过APIServer进行控制以 Kubernetes 中自带的一个控制器 Job Controller 为例。Kubernetes 自带的控制器都是通过与集群中 API Server 交互来达到调整状态的目的。 Job 是一种 Kubernetes API 对象，一个 Job 将运行一个（或多个）Pod，执行一项任务，然后停止。当新的 Job 对象被创建时，Job Controller 将确保集群中有合适数量的节点上的 kubelet 启动了指定个数的 Pod，以完成 Job 的执行任务。Job Controller 自己并不执行任何 Pod 或容器，而是发消息给 API Server，由其他的控制组件配合 API Server，以执行创建或删除 Pod 的实际动作。 当新的 Job 对象被创建时，目标状态是指定的任务被执行完成。Job Controller 调整集群的当前状态以达到目标状态：创建 Pod 以执行 Job 中指定的任务 控制器同样也会更新其关注的 API 对象。例如：一旦 Job 的任务执行结束，Job Controller 将更新 Job 的 API 对象，将其标注为 Finished。 直接控制某些特殊的控制器需要对集群外部的东西做调整。例如，想用一个控制器确保集群中有足够的节点，此时控制器需要调用云供应商的接口以创建新的节点或移除旧的节点。这类控制器将从 API Server 中读取关于目标状态的信息，并直接调用外部接口以实现调整目标。 目标状态 vs 当前状态Kubernetes 使用了 云原生（cloud-native）的视角来看待系统，并且可以持续应对变化。集群在运行的过程中，任何时候都有可能发生突发事件，而控制器则自动地修正这些问题。这就意味着，集群永远不会达到一个稳定不变的状态。 这种通过控制器监控集群状态并利用负反馈原理不断接近目标状态的系统，相较于那种完成安装后就不再改变的系统，是一种更高级的系统形态。 运行控制器的方式Kubernetes 在 kube-controller-manager 中运行了大量的内建控制器（例如，Deployment Controller、Job Controller、StatefulSet Controller、DaemonSet Controller 等）。这些内建控制器提供了 Kubernetes 非常重要的核心功能。Kubernetes 可以运行一个 master 集群，以实现内建控制器的高可用。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-集群内的通信","slug":"k8s进阶-架构-集群内的通信","date":"2020-03-26T08:24:42.000Z","updated":"2020-03-27T07:24:09.675Z","comments":true,"path":"2020/03/26/k8s进阶-架构-集群内的通信/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84-%E9%9B%86%E7%BE%A4%E5%86%85%E7%9A%84%E9%80%9A%E4%BF%A1/","excerpt":"Master-Node之间的通信","text":"Master-Node之间的通信 Master-Node之间的通信Master-Node 之间的通信可以分为如下两类： Cluster to Master Master to Cluster Cluster to Master所有从集群访问 Master 节点的通信，都是针对 apiserver 的（没有任何其他 master 组件发布远程调用接口）。通常安装 Kubernetes 时，apiserver 监听 HTTPS 端口（443），并且配置了一种或多种客户端认证方式 authentication。至少需要配置一种形式的授权方式 authorization，尤其是匿名访问 anonymous requests或 Service Account Tokens被启用的情况下。 节点上必须配置集群（apiserver）的公钥根证书（public root certificate），此时，在提供有效的客户端身份认证的情况下，节点可以安全地访问 APIServer。例如，在 Google Kubernetes Engine 的一个默认 Kubernetes 安装里，通过客户端证书为 kubelet 提供客户端身份认证。 对于需要调用 APIServer 接口的 Pod，应该为其关联 Service Account，此时，Kubernetes将在创建Pod时自动为其注入公钥根证书（public root certificate）以及一个有效的 bearer token（放在HTTP请求头Authorization字段）。所有名称空间中，都默认配置了名为 kubernetes Kubernetes Service，该 Service对应一个虚拟 IP（默认为 10.96.0.1），发送到该地址的请求将由 kube-proxy 转发到 apiserver 的 HTTPS 端口上。 默认情况下，从集群（节点以及节点上运行的 Pod）访问 master 的连接是安全的，因此，可以通过不受信的网络或公网连接 Kubernetes 集群 Master to Cluster从 master（apiserver）到Cluster存在着两条主要的通信路径： apiserver 访问集群中每个节点上的 kubelet 进程 使用 apiserver 的 proxy 功能，从 apiserver 访问集群中的任意节点、Pod、Service apiserver to kubeletapiserver 在如下情况下访问 kubelet： 抓取 Pod 的日志 通过 kubectl exec -it 指令（或 kuboard 的终端界面）获得容器的命令行终端 提供 kubectl port-forward 功能 这些连接的访问端点是 kubelet 的 HTTPS 端口。默认情况下，apiserver 不校验 kubelet 的 HTTPS 证书，这种情况下，连接可能会收到 man-in-the-middle 攻击，因此该连接如果在不受信网络或者公网上运行时，是 不安全 的。 如果要校验 kubelet 的 HTTPS 证书，可以通过 --kubelet-certificate-authority 参数为 apiserver 提供校验 kubelet 证书的根证书。 如果不能完成这个配置，又需要通过不受信网络或公网将节点加入集群，则需要使用SSH隧道连接 apiserver 和 kubelet。 同时，Kubelet authentication/authorization需要激活，以保护 kubelet API apiserver to nodes, pods, services从 apiserver 到 节点/Pod/Service 的连接使用的是 HTTP 连接，没有进行身份认证，也没有进行加密传输。您也可以通过增加 https 作为 节点/Pod/Service 请求 URL 的前缀，但是 HTTPS 证书并不会被校验，也无需客户端身份认证，因此该连接是无法保证一致性的。目前，此类连接如果运行在非受信网络或公网上时，是 不安全 的 SSH隧道Kubernetes 支持 SSH隧道（tunnel）来保护 Master –&gt; Cluster 访问路径。此时，apiserver 将向集群中的每一个节点建立一个 SSH隧道（连接到端口22的ssh服务）并通过隧道传递所有发向 kubelet、node、pod、service 的请求。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-节点","slug":"k8s进阶-架构","date":"2020-03-26T02:21:12.000Z","updated":"2020-03-26T09:17:36.969Z","comments":true,"path":"2020/03/26/k8s进阶-架构/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84/","excerpt":"节点 节点管理","text":"节点 节点管理 节点节点状态节点的状态包含如下信息： Addresses Conditions Capacity and Allocatable Info 执行以下命令可查看所有节点的列表 1kubectl get nodes -o wide 执行以下命令可查看节点状态以及节点的其他详细信息： 1kubectl describe node &lt;your-node-name&gt; Addresses HostName： 在节点命令行界面上执行 hostname 命令所获得的值。启动 kubelet 时，可以通过参数 --hostname-override 覆盖 ExternalIP：通常是节点的外部IP（可以从集群外访问的内网IP地址；上面的例子中，此字段为空） InternalIP：通常是从节点内部可以访问的 IP 地址 ConditionsConditions 描述了节点的状态。Condition的例子有： Node Condition 描述 OutOfDisk 如果节点上的空白磁盘空间不够，不能够再添加新的节点时，该字段为 True，其他情况为 False Ready 如果节点是健康的且已经就绪可以接受新的 Pod。则节点Ready字段为 True。False表明了该节点不健康，不能够接受新的 Pod。 MemoryPressure 如果节点内存紧张，则该字段为 True，否则为False PIDPressure 如果节点上进程过多，则该字段为 True，否则为 False DiskPressure 如果节点磁盘空间紧张，则该字段为 True，否则为 False NetworkUnvailable 如果节点的网络配置有问题，则该字段为 True，否则为 False Capacity and Allocatable（容量和可分配量）容量和可分配量（Capacity and Allocatable）描述了节点上的可用资源的情况： CPU 内存 该节点可调度的最大 pod 数量 Capacity 中的字段表示节点上的资源总数，Allocatable 中的字段表示该节点上可分配给普通 Pod 的资源总数。 Info描述了节点的基本信息，例如： Linux 内核版本 Kubernetes 版本（kubelet 和 kube-proxy 的版本） Docker 版本 操作系统名称 这些信息由节点上的 kubelet 收集。 节点管理与 Pod 和 Service 不一样，节点并不是由 Kubernetes 创建的，节点由云供应商（例如，Google Compute Engine、阿里云等）创建，或者节点已经存在于您的物理机/虚拟机的资源池。向 Kubernetes 中创建节点时，仅仅是创建了一个描述该节点的 API 对象。节点 API 对象创建成功后，Kubernetes将检查该节点是否有效。 节点控制器（Node Controller）节点控制器是一个负责管理节点的 Kubernetes master 组件。在节点的生命周期中，节点控制器起到了许多作用。 节点控制器在注册节点时为节点分配 CIDR 地址块 节点控制器通过云供应商（cloud-controller-manager）接口检查节点列表中每一个节点对象对应的虚拟机是否可用。在云环境中，只要节点状态异常，节点控制器检查其虚拟机在云供应商的状态，如果虚拟机不可用，自动将节点对象从 APIServer 中删除。 节点控制器监控节点的健康状况。当节点变得不可触达时（例如，由于节点已停机，节点控制器不再收到来自节点的心跳信号），节点控制器将节点API对象的 NodeStatus Condition 取值从 NodeReady 更新为 Unknown；然后在等待 pod-eviction-timeout 时间后，将节点上的所有 Pod 从节点驱逐。 默认40秒未收到心跳，修改 NodeStatus Condition 为 Unknown； 默认 pod-eviction-timeout 为 5分钟 节点控制器每隔 --node-monitor-period 秒检查一次节点的状态 节点自注册（Self-Registration）如果 kubelet 的启动参数 --register-node为 true（默认为 true），kubelet 会尝试将自己注册到 API Server。kubelet自行注册时，将使用如下选项： --kubeconfig：向 apiserver 进行认证时所用身份信息的路径 --cloud-provider：向云供应商读取节点自身元数据 --register-node：自动向 API Server 注册节点 --register-with-taints：注册节点时，为节点添加污点（逗号分隔，格式为 =: --node-ip：节点的 IP 地址 --node-labels：注册节点时，为节点添加标签 --node-status-update-frequency：向 master 节点发送心跳信息的时间间隔 如果 Node authorization mode 和 NodeRestriction admission plugin 被启用，kubelet 只拥有创建/修改其自身所对应的节点 API 对象的权限。 手动管理节点如果想要手工创建节点API对象，可以将 kubelet 的启动参数 --register-node 设置为 false。 管理员可以修改节点API对象（不管是否设置了 --register-node 参数）。可以修改的内容有： 增加/减少标签 标记节点为不可调度（unschedulable） 节点的标签与 Pod 上的节点选择器（node selector）配合，可以控制调度方式，例如，限定 Pod 只能在某一组节点上运行。 执行如下命令可将节点标记为不可调度（unschedulable），此时将阻止新的 Pod 被调度到该节点上，但是不影响任何已经在该节点上运行的 Pod。这在准备重启节点之前非常有用。 1kubectl cordon $NODENAME 节点容量（Node Capacity）Kubernetes 调度器在调度 Pod 到节点上时，将确保节点上有足够的资源。具体来说，调度器检查节点上所有容器的资源请求之和不大于节点的容量。此时，只能检查由 kubelet 启动的容器，不包括直接由容器引擎启动的容器，更不包括不在容器里运行的进程","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"伸缩服务和滚动更新","slug":"伸缩服务","date":"2020-03-25T08:43:30.000Z","updated":"2020-03-25T09:09:44.425Z","comments":true,"path":"2020/03/25/伸缩服务/","link":"","permalink":"http://yoursite.com/2020/03/25/%E4%BC%B8%E7%BC%A9%E6%9C%8D%E5%8A%A1/","excerpt":"通过更改部署中的 replicas（副本数）来完成伸缩","text":"通过更改部署中的 replicas（副本数）来完成伸缩 Scaling（伸缩）应用程序 伸缩 的实现可以通过更改 nginx-deployment.yaml 文件中部署的 replicas（副本数）来完成 12spec: replicas: 4 #使用该Deployment创建两个应用程序实例 修改了 Deployment 的 replicas 为 4 后，Kubernetes 又为该 Deployment 创建了 3 新的 Pod，这 4 个 Pod 有相同的标签。因此Service A通过标签选择器与新的 Pod建立了对应关系，将访问流量通过负载均衡在 4 个 Pod 之间进行转发。 将 nginx Deployment 扩容到 4 个副本修改 nginx-deployment.yaml 文件，将 replicas 修改为 4 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 4 #通过更改部署中的 replicas（副本数）来完成扩展 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 执行命令： 1234kubectl apply -f nginx-deployment.yaml#查看结果watch kubectl get pods -o wide 滚动更新用户期望应用程序始终可用，为此开发者/运维者在更新应用程序时要分多次完成。在 Kubernetes 中，这是通过 Rolling Update 滚动更新完成的。Rolling Update滚动更新 通过使用新版本的 Pod 逐步替代旧版本的 Pod 来实现 Deployment 的更新，从而实现零停机。新的 Pod 将在具有可用资源的 Node（节点）上进行调度。 Kubernetes 更新多副本的 Deployment 的版本时，会逐步的创建新版本的 Pod，逐步的停止旧版本的 Pod，以便使应用一直处于可用状态。这个过程中，Service 能够监视 Pod 的状态，将流量始终转发到可用的 Pod 上。 滚动更新步骤1.原本 Service A 将流量负载均衡到 4 个旧版本的 Pod （当中的容器为 绿色）上 1.原本 Service A 将流量负载均衡到 4 个旧版本的 Pod （当中的容器为 绿色）上 2.更新完 Deployment 部署文件中的镜像版本后，master 节点选择了一个 worker 节点，并根据新的镜像版本创建 Pod（紫色容器）。新 Pod 拥有唯一的新的 IP。同时，master 节点选择一个旧版本的 Pod 将其移除。 此时，Service A 将新 Pod 纳入到负载均衡中，将旧Pod移除 同步骤2，再创建一个新的 Pod 替换一个原有的 Pod 如此 Rolling Update 滚动更新，直到所有旧版本 Pod 均移除，新版本 Pod 也达到 Deployment 部署文件中定义的副本数，则滚动更新完成 更新 nginx Deployment 修改 nginx-deployment.yaml 文件123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 4 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.8 #使用镜像nginx:1.8替换原来的nginx:1.7.9 ports: - containerPort: 80 执行命令 1234kubectl apply -f nginx-deployment.yaml#查看过程及结果，可观察到 pod 逐个被替换的过程。watch kubectl get pods -l app=nginx","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"Service服务","slug":"Service服务","date":"2020-03-25T07:09:33.000Z","updated":"2020-03-25T08:29:26.546Z","comments":true,"path":"2020/03/25/Service服务/","link":"","permalink":"http://yoursite.com/2020/03/25/Service%E6%9C%8D%E5%8A%A1/","excerpt":"Kubernetes Service（服务）概述","text":"Kubernetes Service（服务）概述 Service（服务）概述Pod 有自己的生命周期。当 worker node（节点）故障时，节点上运行的 Pod（容器组）也会消失。然后，Deployment 可以通过创建新的 Pod（容器组）来动态地将群集调整回原来的状态，以使应用程序保持运行。 Service的作用：由于 Kubernetes 集群中每个 Pod（容器组）都有一个唯一的 IP 地址（即使是同一个 Node 上的不同 Pod），service 可以解决为前端系统屏蔽后端系统的 Pod（容器组）在销毁、创建过程中所带来的 IP 地址的变化。 Kubernetes 中的 Service（服务） 提供了这样的一个抽象层，它选择具备某些特征的 Pod（容器组）并为它们定义一个访问方式。Service（服务）使 Pod（容器组）之间的相互依赖解耦（原本从一个 Pod 中访问另外一个 Pod，需要知道对方的 IP 地址）。一个 Service（服务）选定哪些 Pod（容器组） 通常由 LabelSelector(标签选择器) 来决定。 在创建Service的时候，通过设置配置文件中的 spec.type 字段的值，可以以不同方式向外部暴露应用程序： ClusterIP（默认） 在群集中的内部IP上公布服务，这种方式的 Service（服务）只在集群内部可以访问到 NodePort 使用 NAT 在集群中每个的同一端口上公布服务。这种方式下，可以通过访问集群中任意节点+端口号的方式访问服务 :。此时 ClusterIP 的访问方式仍然可用。 LoadBalancer 在云环境中（需要云供应商可以支持）创建一个集群外部的负载均衡器，并为使用该负载均衡器的 IP 地址作为服务的访问地址。此时 ClusterIP 和 NodePort 的访问方式仍然可用。 Service是一个抽象层，它通过 LabelSelector 选择了一组 Pod（容器组），把这些 Pod 的指定端口公布到到集群外部，并支持负载均衡和服务发现。 公布 Pod 的端口以使其可访问 在多个 Pod 间实现负载均衡 使用 Label 和 LabelSelector 服务和标签 下图中有两个服务Service A(黄色虚线)和Service B(蓝色虚线) Service A 将请求转发到 IP 为 10.10.10.1 的Pod上， Service B 将请求转发到 IP 为 10.10.10.2、10.10.10.3、10.10.10.4 的Pod上。 Service 将外部请求路由到一组 Pod 中，它提供了一个抽象层，使得 Kubernetes 可以在不影响服务调用者的情况下，动态调度容器组 Service使用 Labels、LabelSelector (标签和选择器 匹配一组 Pod。Labels（标签）是附加到 Kubernetes 对象的键/值对，其用途有多种： 将 Kubernetes 对象（Node、Deployment、Pod、Service等）指派用于开发环境、测试环境或生产环境 嵌入版本标签，使用标签区别不同应用软件版本 使用标签对 Kubernetes 对象进行分类 Labels（标签）和 LabelSelector（标签选择器）之间的关联关系 Deployment B 含有 LabelSelector 为 app=B 通过此方式声明含有 app=B 标签的 Pod 与之关联 通过 Deployment B 创建的 Pod 包含标签为 app=B Service B 通过标签选择器 app=B 选择可以路由的 Pod Labels（标签）可以在创建 Kubernetes 对象时附加上去，也可以在创建之后再附加上去。任何时候都可以修改一个 Kubernetes 对象的 Labels（标签） nginx Deployment 创建一个 Service创建nginx的Deployment中定义了Labels，如下：1234metadata: #译名为元数据，即Deployment的一些基本属性和信息 name: nginx-deployment #Deployment的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组 app: nginx #为该Deployment设置key为app，value为nginx的标签 创建文件 nginx-service.yaml12345678910111213141516apiVersion: v1kind: Servicemetadata: name: nginx-service #Service 的名称 labels: #Service 自己的标签 app: nginx #为该 Service 设置 key 为 app，value 为 nginx 的标签spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: nginx #选择包含标签 app:nginx 的 Pod ports: - name: nginx-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 32600 #通过任意节点的 32600 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer 执行命令 1234567kubectl apply -f nginx-service.yaml#检查执行结果kubectl get services -o wide #可查看到名称为 nginx-service 的服务。#访问服务curl &lt;任意节点的 IP&gt;:32600","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"查看Pods/Nodes","slug":"查看Pods-Nodes","date":"2020-03-25T06:05:21.000Z","updated":"2020-03-25T06:21:04.946Z","comments":true,"path":"2020/03/25/查看Pods-Nodes/","link":"","permalink":"http://yoursite.com/2020/03/25/%E6%9F%A5%E7%9C%8BPods-Nodes/","excerpt":"了解 Pod 和 Node","text":"了解 Pod 和 Node Pod​ Pod中的容器共享 IP 地址和端口空间（同一 Pod 中的不同 container 端口不能相互冲突），始终位于同一位置并共同调度，并在同一节点上的共享上下文中运行。（同一个Pod内的容器可以localhost + 端口互相访问）。 ​ 当在 k8s 上创建 Deployment 时，会在集群上创建包含容器的 Pod (而不是直接创建容器)。每个Pod都与运行它的 worker 节点（Node）绑定，并保持在那里直到终止或被删除。如果节点（Node）发生故障，则会在群集中的其他可用节点（Node）上运行相同的 Pod（从同样的镜像创建 Container，使用同样的配置，IP 地址不同，Pod 名字不同）。 Pod（容器组）是 k8s 集群上的最基本的单元。 Pod 是一组容器（可包含一个或多个应用程序容器），以及共享存储（卷 Volumes）、IP 地址和有关如何运行容器的信息。 如果多个容器紧密耦合并且需要共享磁盘等资源，则应该被部署在同一个Pod（容器组）中 Node Pod（容器组）总是在 Node（节点） 上运行。 Node（节点）是 kubernetes 集群中的计算机，可以是虚拟机或物理机。 每个 Node（节点）都由 master 管理。 一个 Node（节点）可以有多个Pod（容器组），kubernetes master 会根据每个 Node（节点）上可用资源的情况，自动调度 Pod（容器组）到最佳的 Node（节点）上。 每个Node（节点）至少运行： Kubelet，负责 master 节点和 worker 节点之间通信的进程；管理 Pod（容器组）和 Pod（容器组）内运行的 Container（容器）。 容器运行环境（如Docker）负责下载镜像、创建和运行容器等。 相关命令操作kubectl get - 显示资源列表1234567891011121314151617# kubectl get 资源类型#获取类型为Deployment的资源列表kubectl get deployments#获取类型为Pod的资源列表kubectl get pods#获取类型为Node的资源列表kubectl get nodes# 查看所有名称空间的 Deploymentkubectl get deployments -Akubectl get deployments --all-namespaces# 查看 kube-system 名称空间的 Deploymentkubectl get deployments -n kube-system kubectl describe - 显示有关资源的详细信息1234567# kubectl describe 资源类型 资源名称#查看名称为nginx-XXXXXX的Pod的信息kubectl describe pod nginx-XXXXXX #查看名称为nginx的Deployment的信息kubectl describe deployment nginx kubectl logs - 查看pod中的容器的打印日志（和命令docker logs 类似）12# kubectl logs Pod名称kubectl logs -f nginx-pod-XXXXXXX kubectl exec - 在pod中的容器环境内执行命令(和命令docker exec 类似)1234# kubectl exec Pod名称 操作命令# 在名称为nginx-pod-xxxxxx的Pod中运行bashkubectl exec -it nginx-pod-xxxxxx /bin/bash","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s部署一个应用程序","slug":"k8s部署一个应用程序","date":"2020-03-25T05:49:00.000Z","updated":"2020-03-25T05:59:21.447Z","comments":true,"path":"2020/03/25/k8s部署一个应用程序/","link":"","permalink":"http://yoursite.com/2020/03/25/k8s%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"使用 kubectl 在 k8s 上部署第一个应用程序。","text":"使用 kubectl 在 k8s 上部署第一个应用程序。 Deployment概念​ 通过发布 Deployment，可以创建应用程序 (docker image) 的实例 (docker container)，这个实例会被包含在称为 Pod 的概念中，Pod 是 k8s 中最小可管理单元。 ​ 在 k8s 集群中发布 Deployment 后，Deployment 将指示 k8s 如何创建和更新应用程序的实例，master 节点将应用程序实例调度到集群中的具体的节点上。 ​ 创建应用程序实例后，Kubernetes Deployment Controller 会持续监控这些实例。如果运行实例的 worker 节点关机或被删除，则 Kubernetes Deployment Controller 将在群集中资源最优的另一个 worker 节点上重新创建一个新的实例。这提供了一种自我修复机制来解决机器故障或维护问题。 ​ 通过创建应用程序实例并确保它们在集群节点中的运行实例个数，Kubernetes Deployment 提供了一种完全不同的方式来管理应用程序。 Deployment 处于 master 节点上，通过发布 Deployment，master 节点会选择合适的 worker 节点创建 Container（即图中的正方体），Container 会被包含在 Pod （即蓝色圆圈）里。 部署 nginx Deployment创建文件 nginx-deployment.yaml12345678910111213141516171819apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本kind: Deployment #该配置的类型，我们使用的是 Deploymentmetadata: #译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx #为该Deployment设置key为app，value为nginx的标签spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: nginx #container的名称 image: nginx:1.7.9 #使用镜像nginx:1.7.9创建container，该container默认80端口可访问 应用 YAML 文件1kubectl apply -f nginx-deployment.yaml 查看部署结果12345# 查看 Deploymentkubectl get deployments# 查看 Podkubectl get pods","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"kubectl 命令技巧大全","slug":"kubectl 命令技巧大全","date":"2020-03-20T03:13:20.000Z","updated":"2020-03-20T07:20:09.424Z","comments":true,"path":"2020/03/20/kubectl 命令技巧大全/","link":"","permalink":"http://yoursite.com/2020/03/20/kubectl%20%E5%91%BD%E4%BB%A4%E6%8A%80%E5%B7%A7%E5%A4%A7%E5%85%A8/","excerpt":"一些基本的kubernets操作命令","text":"一些基本的kubernets操作命令 kubectl 命令技巧大全Kubectl 自动补全12345yum install -y bash-completionsource &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completionsource &lt;(kubectl completion bash) 创建对象Kubernetes 的清单文件可以使用 json 或 yaml 格式定义。可以以 .yaml、.yml、或者 .json 为扩展名。 1234567891011$ kubectl create -f .&#x2F;my-manifest.yaml # 创建资源$ kubectl create -f .&#x2F;my1.yaml -f .&#x2F;my2.yaml # 使用多个文件创建资源$ kubectl create -f .&#x2F;dir # 使用目录下的所有清单文件来创建资源$ kubectl create -f https:&#x2F;&#x2F;git.io&#x2F;vPieo # 使用 url 来创建资源$ kubectl run nginx --image&#x3D;nginx # 启动一个 nginx 实例$ kubectl explain pods,svc # 获取 pod 和 svc 的文档 显示和查找资源 列出所有 namespace 中的所有service 1$ kubectl get services 列出所有 namespace 中的所有 pod 1$ kubectl get pods --all-namespaces 列出所有 pod 并显示详细信息 1$ kubectl get pods -o wide 列出指定 deployment 1$ kubectl get deployment my-dep 列出该 namespace 中的所有 pod 包括未初始化的 1$ kubectl get pods --include-uninitialized 使用详细输出来描述命令 12345 $ kubectl describe nodes my-node $ kubectl describe pods my-pod $ kubectl get services --sort-by&#x3D;.metadata.name 根据重启次数排序列出 pod 1$ kubectl get pods --sort-by&#x3D;&#39;.status.containerStatuses[0].restartCount&#39; 获取所有具有 app=cassandra 的 pod 中的 version 标签 1$ kubectl get pods --selector&#x3D;app&#x3D;cassandra rc -o \\ jsonpath&#x3D;&#39;&#123;.items[*].metadata.labels.version&#125;&#39; 获取所有节点的 ExternalIP 1$ kubectl get nodes -o jsonpath&#x3D;&#39;&#123;.items[*].status.addresses[?(@.type&#x3D;&#x3D;&quot;ExternalIP&quot;)].address&#125;&#39; 列出属于某个 PC 的 Pod 的名字，“jq”命令用于转换复杂的 jsonpath，参考 https://stedolan.github.io/jq/ 123 $ sel&#x3D;$&#123;$(kubectl get rc my-rc --output&#x3D;json | jq -j &#39;.spec.selector | to_entries | .[] | &quot;\\(.key)&#x3D;\\(.value),&quot;&#39;)%?&#125; $ echo $(kubectl get pods --selector&#x3D;$sel --output&#x3D;jsonpath&#x3D;&#123;.items..metadata.name&#125;) 查看哪些节点已就绪 1$ JSONPATH&#x3D;&#39;&#123;range .items[*]&#125;&#123;@.metadata.name&#125;:&#123;range @.status.conditions[*]&#125;&#123;@.type&#125;&#x3D;&#123;@.status&#125;;&#123;end&#125;&#123;end&#125;&#39; \\ &amp;&amp; kubectl get nodes -o jsonpath&#x3D;&quot;$JSONPATH&quot; | grep &quot;Ready&#x3D;True&quot; 列出当前 Pod 中使用的 Secret 1$ kubectl get pods -o json | jq &#39;.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name&#39; | grep -v null | sort | uniq 更新资源滚动更新 pod frontend-v1 1$ kubectl rolling-update frontend-v1 -f frontend-v2.json 更新资源名称并更新镜像 1$ kubectl rolling-update frontend-v1 frontend-v2 --image&#x3D;image:v2 更新 frontend pod 中的镜像 1$ kubectl rolling-update frontend --image&#x3D;image:v2 退出已存在的进行中的滚动更新 1$ kubectl rolling-update frontend-v1 frontend-v2 --rollback 基于 stdin 输入的 JSON 替换 pod 1$ cat pod.json | kubectl replace -f - 强制替换，删除后重新创建资源。会导致服务中断。 1$ kubectl replace --force -f .&#x2F;pod.json 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口 1$ kubectl expose rc nginx --port&#x3D;80 --target-port&#x3D;8000 更新单容器 pod 的镜像版本（tag）到 v4 1$ kubectl get pod mypod -o yaml | sed &#39;s&#x2F;\\(image: myimage\\):.*$&#x2F;\\1:v4&#x2F;&#39; | kubectl replace -f - 添加标签 1$ kubectl label pods my-pod new-label&#x3D;awesome 添加注解 1$ kubectl annotate pods my-pod icon-url&#x3D;http:&#x2F;&#x2F;goo.gl&#x2F;XXBTWq 自动扩展 deployment “foo” 1$ kubectl autoscale deployment foo --min&#x3D;2 --max&#x3D;10 删除资源# 删除 pod.json 文件中定义的类型和名称的 pod 1$ kubectl delete -f .&#x2F;pod.json 删除名为“baz”的 pod 和名为“foo”的 service 1$ kubectl delete pod,service baz foo 删除具有 name=myLabel 标签的 pod 和 serivce 1$ kubectl delete pods,services -l name&#x3D;myLabel 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的 1$ kubectl delete pods,services -l name&#x3D;myLabel --include-uninitialized 删除 my-ns namespace 下的所有 pod 和 serivce，包括尚未初始化的 1$ kubectl -n my-ns delete po,svc --all 与运行中的Pod交互# dump 输出 pod 的日志（stdout） 1$ kubectl logs my-pod dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） 1$ kubectl logs my-pod -c my-container 流式输出 pod 的日志（stdout） 1$ kubectl logs -f my-pod 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） 1$ kubectl logs -f my-pod -c my-container 交互式 shell 的方式运行 pod 1$ kubectl run -i --tty busybox --image&#x3D;busybox -- sh 连接到运行中的容器 1$ kubectl attach my-pod -i 转发 pod 中的 6000 端口到本地的 5000 端口 1$ kubectl port-forward my-pod 5000:6000 在已存在的容器中执行命令（只有一个容器的情况下） 1$ kubectl exec my-pod -- ls &#x2F; 在已存在的容器中执行命令（pod 中有多个容器的情况下） 1$ kubectl exec my-pod -c my-container -- ls &#x2F; 显示指定 pod 和容器的指标度量 1$ kubectl top pod POD_NAME --containers 与节点和集群交互# 标记 my-node 不可调度 1$ kubectl cordon my-node 清空 my-node 以待维护 1$ kubectl drain my-node 标记 my-node 可调度 1$ kubectl uncordon my-node 显示 my-node 的指标度量 1$ kubectl top node my-node 显示 master 和服务的地址 1$ kubectl cluster-info 将当前集群状态输出到 stdout 1$ kubectl cluster-info dump 将当前集群状态输出到 /path/to/cluster-state 1$ kubectl cluster-info dump --output-directory&#x3D;&#x2F;path&#x2F;to&#x2F;cluster-state 如果该键和影响的污点（taint）已存在，则使用指定的值替换 1$ kubectl taint nodes foo dedicated&#x3D;special-user:NoSchedule 资源类型 缩写别名 clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies nodes no statefulsets persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses kubectl get - 显示资源列表#获取类型为Deployment的资源列表 1kubectl get deployments #获取类型为Pod的资源列表 1kubectl get pods #获取类型为Node的资源列表 1kubectl get nodes 名称空间在命令后增加 -A 或 –all-namespaces 可查看所有名称空间中的对象，使用参数 -n 可查看指定名称空间的对象，例如 # 查看所有名称空间的 Deployment 12kubectl get deployments -A kubectl get deployments --all-namespaces 查看 kube-system 名称空间的 Deployment 1kubectl get deployments -n kube-system 检查 kubectl 是否知道集群地址及凭证 1$ kubectl config view 通过 kubectl cluster-info 命令获得这些服务列表： 12345[root@ebs ~]# kubectl cluster-info Kubernetes master is running at https:&#x2F;&#x2F;172.16.121.88:6443 KubeDNS is running at https:&#x2F;&#x2F;172.16.121.88:6443&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"Kubernetes核心概念","slug":"Kubernetes核心概念","date":"2020-03-20T03:10:20.000Z","updated":"2020-03-25T09:26:08.403Z","comments":true,"path":"2020/03/20/Kubernetes核心概念/","link":"","permalink":"http://yoursite.com/2020/03/20/Kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","excerpt":"Cluster，Pod，Label， Replication Controller ，Service …","text":"Cluster，Pod，Label， Replication Controller ，Service … 什么是Kubernetes？Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。如果你曾经用过Docker容器技术部署容器，那么可以将Docker看成Kubernetes内部使用的低级别组件。Kubernetes不仅仅支持Docker，还支持Rocket，这是另一种容器技术。 使用Kubernetes可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它，等等… 集群集群是一组节点，这些节点可以是物理服务器或者虚拟机，之上安装了Kubernetes平台。下图展示这样的集群。注意该图为了强调核心概念有所简化。这里可以看到一个典型的Kubernetes架构图。 上图可以看到如下组件，使用特别的图标表示Service和Label： PodContainer（容器） Labe （标签） Replication Controller（复制控制器） Service（服务） Node（节点） Kubernetes Master（Kubernetes主节点） PodPod（上图绿色方框）安排在节点上，包含一组容器和卷。同一个Pod里的容器共享同一个网络命名空间，可以使用localhost互相通信。Pod是短暂的，不是持续性实体。你可能会有这些问题： 如果Pod是短暂的，那么我怎么才能持久化容器数据使其能够跨重启而存在呢？ 是的，Kubernetes支持 卷 的概念，因此可以使用持久化的卷类型。 是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么？可以手动创建单个Pod，但是也可以使用Replication Controller使用Pod模板创建出多份拷贝，下文会详细介绍。 如果Pod是短暂的，那么重启时IP地址可能会改变，那么怎么才能从前端容器正确可靠地指向后台容器呢？这时可以使用Service，下文会详细介绍。 Label正如图所示，一些Pod有Label 。一个Label是attach到Pod的一对键/值对，用来传递用户定义的属性。比如，你可能创建了一个”tier”和“app”标签，通过Label（tier=frontend, app=myapp）来标记前端Pod容器，使用Label（tier=backend, app=myapp）标记后台Pod。然后可以使用 [Selectors] 选择带有特定Label的Pod，并且将Service或者Replication Controller应用到上面。 Replication Controller是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么，能否将Pods划到逻辑组里？ Replication Controller确保任意时间都有指定数量的Pod“副本”在运行。如果为某个Pod创建了Replication Controller并且指定3个副本，它会创建3个Pod，并且持续监控它们。如果某个Pod不响应，那么Replication Controller会替换它，保持总数为3.如下面的动画所示： 如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动 [升级] 时很有用。 当创建Replication Controller时，需要指定两个东西： Pod模板：用来创建Pod副本的模板 Label：Replication Controller需要监控的Pod的标签。现在已经创建了Pod的一些副本，那么在这些副本上如何均衡负载呢？我们需要的是Service。 TIP 最新 Kubernetes 版本里，推荐使用 Deployment Service如果Pods是短暂的，那么重启时IP地址可能会改变，怎么才能从前端容器正确可靠地指向后台容器呢？ [Service] 抽象 现在，假定有2个后台Pod，并且定义后台Service的名称为‘backend-service’，label选择器为(tier=backend, app=myapp) 的Service会完成如下两件重要的事情： 会为Service创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。 现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod的哪一个呢？Service在这2个后台Pod之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个Node上运行的代理（kube-proxy）完成。 下述动画展示了Service的功能。注意该图作了很多简化。如果不进入网络配置，那么达到透明的负载均衡目标所涉及的底层网络和路由相对先进。如果有兴趣，有更深入的介绍。 每个节点都运行如下Kubernetes关键组件： Kubelet：是主节点代理。 Kube-proxy：Service使用其将链接路由到Pod，如上文所述。 Docker或Rocket：Kubernetes使用的容器技术来创建容器。 Kubernetes Master集群拥有一个Kubernetes Master（紫色方框）。Kubernetes Master提供集群的独特视角，并且拥有一系列组件，比如Kubernetes API Server。API Server提供可以用来和集群交互的REST端点。master节点包括用来创建和复制Pod的Replication Controller。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"hexo 使用","slug":"hexo使用","date":"2020-03-19T09:18:36.000Z","updated":"2020-03-23T07:14:57.409Z","comments":true,"path":"2020/03/19/hexo使用/","link":"","permalink":"http://yoursite.com/2020/03/19/hexo%E4%BD%BF%E7%94%A8/","excerpt":"hexo 搭建、部署、操作；","text":"hexo 搭建、部署、操作； _config.yml配置git 1234deploy: type: &#39;git&#39; repo: git@github.com:yourname&#x2F;yourname.github.io.git branch: master 上传github，推送文件步骤：hexo clean hexo c 清除缓存文件 (db.json) 和已生成的静态文件 (public) hexo generate hexo g 生成静态文件 hexo deploy hexo d 部署网站 执行端口启动： 1hexo s -i 0.0.0.0 -p 8080 绑定个人域名： 解析域名注意，博客网址中必须使用你github的用户名. 布局（Layout） Hexo 有三种默认布局：post、page 和 draft。在创建者三种不同类型的文件时，它们将会被保存到不同的路径；而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。 布局 路径 post source/_posts page source draft source/_drafts hexo init 1hexo init [ folder] 新建一个网站。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站。 hexo new 1hexo new [layout] &lt;title&gt; 新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。 hexo new “post title with whitespace” 参数 描述 -p, –path 自定义新文章的路径 -r, –replace 如果存在同名文章，将其替换 -s, –slug 文章的 Slug，作为新文章的文件名和发布后的 URL 默认情况下，Hexo 会使用文章的标题来决定文章文件的路径。对于独立页面来说，Hexo 会创建一个以标题为名字的目录，并在目录中放置一个 index.md 文件。你可以使用 –path 参数来覆盖上述行为、自行决定文件的目录： 1hexo new page --path about&#x2F;me &quot;About me&quot; 以上命令会创建一个 source/about/me.md 文件，同时 Front Matter 中的 title 为 “About me” 注意！title 是必须指定的！如果你这么做并不能达到你的目的： 1hexo new page --path about&#x2F;me 此时 Hexo 会创建 source/_posts/about/me.md，同时 me.md 的 Front Matter 中的 title 为 “page”。这是因为在上述命令中，hexo-cli 将 page 视为指定文章的标题、并采用默认的 layout。","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]}]}