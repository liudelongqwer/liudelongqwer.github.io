{"meta":{"title":"拒绝再玩","subtitle":"","description":"","author":"duoyu","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"K8s容器组 Pod","slug":"K8s容器组","date":"2020-03-31T07:06:33.000Z","updated":"2020-04-01T06:07:02.600Z","comments":true,"path":"2020/03/31/K8s容器组/","link":"","permalink":"http://yoursite.com/2020/03/31/K8s%E5%AE%B9%E5%99%A8%E7%BB%84/","excerpt":"Pod的相关理解","text":"Pod的相关理解 一、Pod 容器组_概述什么是 Pod 容器组Pod（容器组）是 Kubernetes 中最小的可部署单元。 一个 Pod（容器组）包含了一个应用程序容器（某些情况下是多个容器）、存储资源、一个唯一的网络 IP 地址、以及一些确定容器该如何运行的选项。 Pod 容器组代表了 Kubernetes 中一个独立的应用程序运行实例，该实例可能由单个容器或者几个紧耦合在一起的容器组成。 Kubernetes 集群中的 Pod 存在如下两种使用途径： 一个 Pod 中只运行一个容器。”one-container-per-pod” 是 Kubernetes 中最常见的使用方式。Kubernetes 通过 Pod 管理容器，而不是直接管理容器。 一个 Pod 中运行多个需要互相协作的容器。可以将多个紧密耦合、共享资源且始终在一起运行的容器编排在同一个 Pod 中。 每一个 Pod 都是用来运行某一特定应用程序的一个实例。如果想要水平扩展应用程序（运行多个实例），运行多个 Pod 容器组，每一个代表应用程序的一个实例。 Kubernetes 中，称其为 replication（复制副本）。 Kubernetes 中 Controller（控制器）负责为应用程序创建和管理这些复制的副本。 Pod 如何管理多个容器Pod 的设计目的是用来支持多个互相协同的容器，使得形成一个有意义的服务单元。一个 Pod 中的多个容器很自然就可以随 Pod 被一起调度到集群中的同一个物理机或虚拟机上。Pod 中的容器可以： 共享资源、依赖 互相通信 相互协商何时以何种方式结束运行 Pod 为其成员容器提供了两种类型的共享资源：网络和存储 网络 Networking 每一个 Pod 被分配一个独立的 IP 地址。Pod 中的所有容器共享一个网络名称空间： ​ 同一个 Pod 中的所有容器 IP 地址都相同 ​ 同一个 Pod 中的不同容器不能使用相同的端口，否则会导致端口冲突 ​ 同一个 Pod 中的不同容器可以通过 localhost:port 进行通信 ​ 同一个 Pod 中的不同容器可以通过使用常规的进程间通信手段，例如 SystemV semaphores 或者 POSIX 共享内存 存储 Storage Pod 中可以定义一组共享的数据卷。Pod 中所有的容器都可以访问这些共享数据卷，以便共享数据。Pod 中数据卷的数据也可以存储持久化的数据，使得容器在重启后仍然可以访问到之前存入到数据卷中的数据。 不同 Pod 上的两个容器如果要通信，必须使用对方 Pod 的 IP 地址 + 对方容器的端口号进行网络通信 使用 Pod 在 Pod 被创建后（直接创建，或者间接通过 Controller 创建），将被调度到集群中的一个节点上运行。Pod 将一直保留在该节点上，直到 Pod 以下情况发生： Pod 中的容器全部结束运行 Pod 被删除 由于节点资源不够，Pod 被驱逐 节点出现故障（例如死机） Pod 本身并不会运行，Pod 仅仅是容器运行的一个环境 Pod 本身并不能自愈（self-healing）。如果一个 Pod 所在的 Node （节点）出现故障，或者调度程序自身出现故障，Pod 将被删除；同理，当因为节点资源不够或节点维护而驱逐 Pod 时，Pod 也将被删除。Kubernetes 通过引入 Controller（控制器）的概念来管理 Pod 实例。在 Kubernetes 中，更为推荐的做法是使用 Controller 来管理 Pod，而不是直接创建 Pod。 容器组和控制器用户应该始终使用控制器来创建 Pod，而不是直接创建 Pod，控制器可以提供如下特性： 水平扩展（运行 Pod 的多个副本） rollout（版本更新） self-healing（故障恢复） 例如：当一个节点出现故障，控制器可以自动地在另一个节点调度一个配置完全一样的 Pod，以替换故障节点上的 Pod。 在 Kubernetes 中，广泛使用的控制器有： Deployment StatefulSet DaemonSet Termination of Pods（终止Pod）Pod 代表了运行在集群节点上的进程，而进程的终止有两种方式： gracefully terminate （优雅地终止） 直接 kill，此时进程没有机会执行清理动作 当用户发起删除 Pod 的指令时，Kubernetes 需要： 让用户知道 Pod 何时被删除 确保删除 Pod 的指令最终能够完成 Kubernetes 收到用户删除 Pod 的指令后： 记录强制终止前的等待时长（grace period） 向 Pod 中所有容器的主进程发送 TERM 信号 一旦等待超时，向超时的容器主进程发送 KILL 信号 删除 Pod 在 API Server 中的记录 默认情况下，删除 Pod 的 grace period（等待时长）是 30 秒。 可以通过 kubectl delete 命令的选项 --grace-period= 自己指定 grace period（等待时长）。 强制删除 Pod，必须为 kubectl delete 命令同时指定两个选项 --grace-period=0 和 --force 二、Pod 容器组_声明周期Pod phase Phase 描述 Pending Kubernetes 已经创建并确认该 Pod。此时可能有两种情况：Pod 还未完成调度（例如没有合适的节点）正在从 docker registry 下载镜像 Running 该 Pod 已经被绑定到一个节点，并且该 Pod 所有的容器都已经成功创建。其中至少有一个容器正在运行，或者正在启动/重启 Succeeded Pod 中的所有容器都已经成功终止，并且不会再被重启 Failed Pod 中的所有容器都已经终止，至少一个容器终止于失败状态：容器的进程退出码不是 0，或者被系统 kill Unknown 因为某些未知原因，不能确定 Pod 的状态，通常的原因是 master 与 Pod 所在节点之间的通信故障 容器的检查Probe 是指 kubelet 周期性地检查容器的状况。 有三种类型的 Probe： ExecAction： 在容器内执行一个指定的命令。如果该命令的退出状态码为 0，则成功 TCPSocketAction： 探测容器的指定 TCP 端口，如果该端口处于 open 状态，则成功 HTTPGetAction： 探测容器指定端口/路径上的 HTTP Get 请求，如果 HTTP 响应状态码在 200 到 400（不包含400）之间，则成功 Probe 有三种可能的结果： Success： 容器通过检测 Failure： 容器未通过检测 Unknown： 检测执行失败，此时 kubelet 不做任何处理 Kubelet 可以在两种情况下对运行中的容器执行 Probe： 就绪检查 readinessProbe： 确定容器是否已经就绪并接收服务请求。如果就绪检查失败，kubernetes 将该 Pod 的 IP 地址从所有匹配的 Service 的资源池中移除掉。 健康检查 livenessProbe： 确定容器是否正在运行。如果健康检查失败，kubelete 将结束该容器，并根据 restart policy（重启策略）确定是否重启该容器。 何时使用 健康检查/就绪检查？ 如果容器中的进程在碰到问题时可以自己 crash，并不需要执行健康检查；kubelet 可以自动的根据 Pod 的 restart policy（重启策略）执行对应的动作 如果希望在容器的进程无响应后，将容器 kill 掉并重启，则指定一个健康检查 liveness probe，并同时指定 restart policy（重启策略）为 Always 或者 OnFailure 如果想在探测 Pod 确实就绪之后才向其分发服务请求，请指定一个就绪检查 readiness probe。此时，就绪检查的内容可能和健康检查相同。就绪检查适合如下几类容器： 初始化时需要加载大量的数据、配置文件 启动时需要执行迁移任务 容器的状态一旦 Pod 被调度到节点上，kubelet 便开始使用容器引擎（通常是 docker）创建容器。容器有三种可能的状态：Waiting / Running / Terminated： Waiting： 容器的初始状态。处于 Waiting 状态的容器，仍然有对应的操作在执行，例如：拉取镜像、应用 Secrets等。 Running： 容器处于正常运行的状态。容器进入 Running 状态之后，如果指定了 postStart hook，该钩子将被执行。 Terminated： 容器处于结束运行的状态。容器进入 Terminated 状态之前，如果指定了 preStop hook，该钩子将被执行。 重启策略定义 Pod 或工作负载时，可以指定 restartPolicy，可选的值有： Always （默认值） OnFailure Never restartPolicy 将作用于 Pod 中的所有容器。kubelete 将在五分钟内，按照递延的时间间隔（10s, 20s, 40s ……）尝试重启已退出的容器，并在十分钟后再次启动这个循环，直到容器成功启动，或者 Pod 被删除。 容器组的存活期通常，如果没有人或者控制器删除 Pod，Pod 不会自己消失。 只有一种例外，那就是 Pod 处于 Scucceeded 或 Failed 的 phase，并超过了垃圾回收的时长（在 kubernetes master 中通过 terminated-pod-gc-threshold 参数指定），kubelet 自动将其删除。 三、Pod 容器组_初始化容器初始化容器的行为 Pod 的启动时，首先初始化网络和数据卷，然后按顺序执行每一个初始化容器。任何一个初始化容器都必须成功退出，才能开始下一个初始化容器。如果某一个容器启动失败或者执行失败，kubelet 将根据 Pod 的 restartPolicy 决定是否重新启动 Pod。 只有所有的初始化容器全都执行成功，Pod 才能进入 ready 状态。初始化容器的端口是不能够通过 kubernetes Service 访问的。Pod 在初始化过程中处于 Pending 状态，并且同时有一个 type 为 initializing status 为 True 的 Condition 如果 Pod 重启，所有的初始化容器也将被重新执行。 可以组合使用就绪检查和 activeDeadlineSeconds ，以防止初始化容器始终失败。 Pod 中不能包含两个同名的容器（初始化容器和工作容器也不能同名）。 Pod 重启的原因Pod 重启时，所有的初始化容器都会重新执行，Pod 重启的原因可能有： 用户更新了 Pod 的定义，并改变了初始化容器的镜像 改变任何一个初始化容器的镜像，将导致整个 Pod 重启 改变工作容器的镜像，将只重启该工作容器，而不重启 Pod Pod 容器基础设施被重启（例如 docker engine），这种情况不常见，通常只有 node 节点的 root 用户才可以执行此操作 Pod 中所有容器都已经结束，restartPolicy 是 Always，且初始化容器执行的记录已经被垃圾回收，此时将重启整个 Pod 配置初始化容器Pod的配置文件如下 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: name: init-demospec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html # These containers are run during pod initialization initContainers: - name: install image: busybox command: - wget - \"-O\" - \"/work-dir/index.html\" - https://kuboard.cn volumeMounts: - name: workdir mountPath: \"/work-dir\" dnsPolicy: Default volumes: - name: workdir emptyDir: &#123;&#125; 从配置文件可以看出，Pod 中初始化容器和应用程序共享了同一个数据卷。初始化容器将该共享数据卷挂载到 /work-dir 路径，应用程序容器将共享数据卷挂载到 /usr/share/nginx/html 路径。初始化容器执行如下命令后，就退出执行： wget -O /work-dir/index.html https://kuboard.cn 验证： 12345[root@k8s-master k8s-yamls]# kubectl get pod init-demoNAME READY STATUS RESTARTS AGEinit-demo 1&#x2F;1 Running 0 22s[root@k8s-master k8s-yamls]# kubectl exec -it init-demo -- &#x2F;bin&#x2F;bashroot@init-demo:&#x2F;# apt-get update 四、Pod 容器组_Debug初始化容器查看 Pod 的状态： 12345kubectl get pod &lt;pod-name&gt;NAME READY STATUS RESTARTS AGE&lt;pod-name&gt; 0/1 Init:1/2 0 7s例如，状态如果是 Init:1/2，则表明了两个初始化容器当中的一个已经成功执行： 查看初始化容器的详情 1kubectl describe pod &lt;pod-name&gt; 理解 Pod 状态如果 Pod 的状态以 Init: 开头，表示该 Pod 正在执行初始化容器。下表描述了 Debug 初始化容器的过程中，一些可能出现的 Pod 状态： 状态 描述 Init:N/M Pod 中包含 M 个初始化容器，其中 N 个初始化容器已经成功执行 Init:Error Pod 中有一个初始化容器执行失败 Init:CrashLoopBackOff Pod 中有一个初始化容器反复执行失败 Pending Pod 还未开始执行初始化容器 PodInitializing or Running Pod 已经完成初始化容器的执行 五、Pod 容器组_配置PodDisruptionBudget​ 在Kubernetes中，为了保证业务不中断或业务SLA不降级，需要将应用进行集群化部署。通过PodDisruptionBudget控制器可以设置应用 Pod 集群处于运行状态最低个数，也可以设置应用Pod 集群处于运行状态的最低百分比，这样可以保证在主动销毁应用Pod的时候，不会一次性销毁太多的应用Pod，从而保证业务不中断或业务SLA不降级。 在Kubernetes 1.5中，kubectl drain命令已经支持了PodDisruptionBudget控制器，在进行kubectl drain操作时会根据PodDisruptionBudget控制器判断应用Pod集群数量，进而保证在业务不中断或业务SLA不降级的情况下进行应用Pod销毁。 确定需要PDB保护的应用通常如下几种 Kubernetes 控制器创建的应用程序可以使用 PDB： Deployment ReplicationController ReplicaSet StatefulSet PDB 中 .spec.selector 字段的内容必须与控制器中 .spec.selector 字段的内容相同。 当毁坏发生时，在短时间内，应用程序最多可以容许多少个实例被终止？ 无状态的前端： 关注点：不能让服务能力（serving capacity）降低超过 10% 解决方案：在 PDB 中配置 minAvailable 90% 单实例有状态应用： 关注点：未经同意不能关闭此应用程序 解决方案1： 不使用 PDB，并且容忍偶尔的停机 解决方案2： 在 PDB 中设置 maxUnavailable=0。与集群管理员达成一致（不是通过Kubernetes，而是邮件、电话或面对面），请集群管理员在终止应用之前与你沟通。当集群管理员联系你时，准备好停机时间，删除 PDB 以表示已准备好应对毁坏。并做后续处理 多实例有状态应用，例如 consul、zookeeper、etcd： 关注点：不能将实例数降低到某个数值，否则写入会失败 解决方案1： 在 PDB 中设置 maxUnavailable 为 1 （如果副本数会发生变化，可以使用此设置） 解决方案2： 在 PDB 中设置 minAvailable 为最低数量（例如，当总副本数为 5 时，设置为3）（可以同时容忍更多的毁坏数） 可以重新开始的批处理任务： 关注点：当发生自愿毁坏时，Job仍然需要完成其执行任务 解决方案： 不创建 PDB。Job 控制器将会创建一个 Pod 用于替换被毁坏的 Pod 指定百分比时的舍入逻辑minAvailable 或 maxUnavailable 可以指定为整数或者百分比。 当指定一个整数时，代表 Pod 的数量。例如，设置 minAvailable 为 10，则至少 10 个 Pod 必须始终可用，即便是在毁坏发生时 当指定一个百分比时（例如，50%），代表总 Pod 数量的一个百分比。例如，设置 maxUnavailable 为 50%，则最多可以有 50% 的 Pod 可以被毁坏 如果指定这些值为一个百分数，其计算结果可能不会正好是一个整数。例如，假设有 7 个 Pod，minAvailable 设置为 50%，你将很难判断，到底是 3 个还是 4 个 Pod 必须始终保持可用。Kubernetes 将向上舍入（round up to the nearest integer），因此，此处必须有 4 个 Pod 始终可用。 定义PodDisruptionBudgetPodDisruptionBudget 包含三个字段： 标签选择器 .spec.selector 用于指定 PDB 适用的 Pod。此字段为必填 .spec.minAvailable：当完成驱逐时，最少仍然要保留多少个 Pod 可用。该字段可以是一个整数，也可以是一个百分比 .spec.maxUnavailable： 当完成驱逐时，最多可以有多少个 Pod 被终止。该字段可以是一个整数，也可以是一个百分比 在一个 PodDisruptionBudget 中，只能指定 maxUnavailable 和 minAvailable 中的一个。 maxUnavailable 只能应用到那些有控制器的 Pod 上。下面的例子中，“期望的副本数” 是 PodDisruptionBudget 对应 Pod 的控制器的 .spec.replicas 字段： 例子1： minAvailable 为 5 时，只要 PodDisruptionBudget 的 selector 匹配的 Pod 中有超过 5 个仍然可用，就可以继续驱逐 Pod 例子2： minAvailable 为 30% 时，至少保证期望副本数的 30% 可用 例子3： maxUnavailable 为 5 时，最多可以有 5 个副本不可用（unthealthy） 例子4： maxUnavailable 为 30% 时，最多可以有期望副本数的 30% 不可用 通常，一个 PDB 对应一个控制器创建的 Pod，例如，Deployment、ReplicaSet或StatefulSet。 使用 minAvailable12345678910111213141516apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: zk-pdbspec: minAvailable: 2 selector: matchLabels: app: zookeeper [root@k8s-master k8s-yamls]# kubectl apply -f zk-pdb.yaml poddisruptionbudget.policy/zk-pdb created[root@k8s-master k8s-yamls]# kubectl get poddisruptionbudgetsNAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGEzk-pdb 2 N/A 0 26s 使用 maxUnavailable12345678910111213141516apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: zk-pdbspec: maxUnavailable: 1 selector: matchLabels: app: zookeeper [root@k8s-master k8s-yamls]# kubectl apply -f max_zk-pdb.yaml poddisruptionbudget.policy/zk-pdb configured[root@k8s-master k8s-yamls]# kubectl get poddisruptionbudgetsNAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGEzk-pdb N/A 1 0 98s","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":" Kubernetes容器","slug":"Kubernetes容器","date":"2020-03-28T06:58:54.000Z","updated":"2020-03-28T07:35:24.859Z","comments":true,"path":"2020/03/28/Kubernetes容器/","link":"","permalink":"http://yoursite.com/2020/03/28/Kubernetes%E5%AE%B9%E5%99%A8/","excerpt":"Kubernetes中容器的相关","text":"Kubernetes中容器的相关 一、容器容器镜像在 Kubernetes 的 Pod 中使用容器镜像之前，您必须将其推送到一个镜像仓库（或者使用仓库中已经有的容器镜像）。在 Kubernetes 的 Pod 定义中定义容器时，必须指定容器所使用的镜像，容器中的 image 字段支持与 docker 命令一样的语法，包括私有镜像仓库和标签。 例如：my-registry.example.com:5000/example/web-example:v1.0.1 由如下几个部分组成： my-registry.example.com:5000/example/web-example:v1.0.1 my-registry.example.com：registry 地址 :5000：registry 端口 example：repository 名字 web-example：image 名字 v1.0.1：image 标签 如果使用 hub.dokcer.com Registry 中的镜像，可以省略 registry 地址和 registry 端口。 例如：nginx:latest，eipwork/kuboard 更新镜像Kubernetes中，默认的镜像抓取策略是 IfNotPresent，使用此策略，kubelet在发现本机有镜像的情况下，不会向镜像仓库抓取镜像。如果想每次启动 Pod 时，都强制从镜像仓库抓取镜像，可以尝试如下方式： 设置 container 中的 imagePullPolicy 为 Always 省略 imagePullPolicy 字段，并使用 :latest tag 的镜像 省略 imagePullPolicy 字段和镜像的 tag 激活 AlwaysPullImages 管理控制器 imagePullPolicy 字段和 image tag的可能取值将影响到 kubelet 如何抓取镜像： imagePullPolicy: IfNotPresent 仅在节点上没有该镜像时，从镜像仓库抓取 imagePullPolicy: Always 每次启动 Pod 时，从镜像仓库抓取 imagePullPolicy 未填写，镜像 tag 为 :latest 或者未填写，则同 Always 每次启动 Pod 时，从镜像仓库抓取 imagePullPolicy 未填写，镜像 tag 已填写但不是 :latest，则同 IfNotPresent 仅在节点上没有该镜像时，从镜像仓库抓取 imagePullPolicy: Never，Kubernetes 假设本地存在该镜像，并且不会尝试从镜像仓库抓取镜像 二、容器的环境变量Kubernetes为容器提供了一系列重要的资源： 由镜像、一个或多个数据卷合并组成的文件系统 容器自身的信息 集群中其他重要对象的信息 集群的信息在容器创建时，集群中所有的 Service 的连接信息将以环境变量的形式注入到容器中。例如，已创建了一个名为 Foo 的 Service，此时再创建任何容器时，该容器将包含如下环境变量： 12FOO_SERVICE_HOST &#x3D; &lt;Service的ClusterIP&gt;FOO_SERVICE_PORT &#x3D; &lt;Service的端口&gt; 三、容器生命周期容器钩子Kubernetes中为容器提供了两个 hook（钩子函数）： PostStart 此钩子函数在容器创建后将立刻执行。但是，并不能保证该钩子函数在容器的 ENTRYPOINT 之前执行。该钩子函数没有输入参数。 PreStop 此钩子函数在容器被 terminate（终止）之前执行，例如： 通过接口调用删除容器所在 Pod 某些管理事件的发生：健康检查失败、资源紧缺等 如果容器已经被关闭或者进入了 completed 状态，preStop 钩子函数的调用将失败。该函数的执行是同步的，即，kubernetes 将在该函数完成执行之后才删除容器。该钩子函数没有输入参数。 Hook handler的实现容器只要实现并注册 hook handler 便可以使用钩子函数。Kubernetes 中，容器可以实现两种类型的 hook handler： Exec - 在容器的名称空进和 cgroups 中执行一个指定的命令，例如 pre-stop.sh。该命令所消耗的 CPU、内存等资源，将计入容器可以使用的资源限制。 HTTP - 向容器的指定端口发送一个 HTTP 请求 Hook handler的执行当容器的生命周期事件发生时，Kubernetes 在容器中执行该钩子函数注册的 handler。 对于 Pod 而言，hook handler 的调用是同步的。即，如果是 PostStart hook，容器的 ENTRYPOINT 和 hook 是同时出发的，然而如果 hook 执行的时间过长或者挂起了，容器将不能进入到 Running 状态。 PreStop hook 的行为与此相似。如果 hook 在执行过程中挂起了，Pod phase 将停留在 Terminating 的状态，并且在 terminationGracePeriodSeconds 超时之后，Pod被删除。如果 PostStart 或者 PreStop hook 执行失败，则 Kubernetes 将 kill（杀掉）该容器。 用户应该使其 hook handler 越轻量级越好。例如，对于长时间运行的任务，在停止容器前，调用 PreStop 钩子函数，以保存当时的计算状态和数据。 Hook触发的保证Hook 将至少被触发一次，即，当指定事件 PostStart 或 PreStop 发生时，hook 有可能被多次触发。hook handler 的实现需要保证即使多次触发，执行也不会出错。 通常来说，hook 实际值被触发一次。例如：如果 HTTP hook 的服务端已经停机，或者因为网络的问题不能接收到请求，请求将不会被再次发送。在极少数的情况下， 触发两次 hook 的事情会发生。例如，如果 kueblet 在触发 hook 的过程中重启了，该 hook 将在 Kubelet 重启后被再次触发。 调试 hook handlerHook handler 的日志并没有在 Pod 的 events 中发布。如果 handler 因为某些原因失败了，kubernetes 将广播一个事件 PostStart hook 发送 FailedPreStopHook 事件。 可以执行命令 kubectl describe pod $(pod_name) 以查看这些事件。 四、容器生命周期事件处理Kubernetes 中支持容器的 postStart 和 preStop 事件，本文阐述了如何向容器添加生命周期事件处理程序（handler）。 postStart 容器启动时，Kubernetes 立刻发送 postStart 事件，但不确保对应的 handler 是否能在容器的 EntryPoint 之前执行 preStop 容器停止前，Kubernetes 发送 preStop 事件 定义postStart和preStop处理程序创建一个包含单一容器的 Pod，并为该容器关联 postStart 和 preStop 处理程序（handler）。Pod 的yaml文件定义如下： 123456789101112131415apiVersion: v1kind: Podmetadata: name: lifecycle-demospec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler &gt; /usr/share/message\"] preStop: exec: command: [\"/bin/sh\",\"-c\",\"nginx -s quit; while killall -0 nginx; do sleep 1; done\"] 在该例子中，请注意： postStart 命令向 usr/share/message 文件写入了一行文字 preStop 命令优雅地关闭了 nginx 如果容器碰到问题，被 Kubernetes 关闭，这个操作是非常有帮助的，可以使得您的程序在关闭前执行必要的清理任务 创建pod 12345[root@k8s-master k8s-yamls]# kubectl apply -f lifecycle-demo.yaml pod/lifecycle-demo created[root@k8s-master k8s-yamls]# kubectl get pod -ANAMESPACE NAME READY STATUS RESTARTS AGEdefault lifecycle-demo 1/1 Running 0 50s 进入容器的命令行终端： 123[root@k8s-master k8s-yamls]# kubectl exec -it lifecycle-demo -- /bin/bashroot@lifecycle-demo:/# cat /usr/share/messageHello from the postStart handler 总结Kubernetes 在容器启动后立刻发送 postStart 事件，但是并不能确保 postStart 事件处理程序在容器的 EntryPoint 之前执行。 postStart 事件处理程序相对于容器中的进程来说是异步的（同时执行），然而，Kubernetes 在管理容器时，将一直等到 postStart 事件处理程序结束之后，才会将容器的状态标记为 Running。 Kubernetes 在决定关闭容器时，立刻发送 preStop 事件，并且，将一直等到 preStop 事件处理程序结束或者 Pod 的 --grace-period 超时，才删除容器。 注意： Kubernetes 只在 Pod Teminated 状态时才发送 preStop 事件，这意味着，如果 Pod 已经进入了 Completed 状态， preStop 事件处理程序将不会被调用","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s标签和选择器","slug":"k8s标签和选择器","date":"2020-03-28T06:26:59.000Z","updated":"2020-03-28T06:56:51.605Z","comments":true,"path":"2020/03/28/k8s标签和选择器/","link":"","permalink":"http://yoursite.com/2020/03/28/k8s%E6%A0%87%E7%AD%BE%E5%92%8C%E9%80%89%E6%8B%A9%E5%99%A8/","excerpt":"标签和标签选择器的作用","text":"标签和标签选择器的作用 标签和选择器标签（Label）是附加在Kubernetes对象上的一组名值对，其意图是按照对用户有意义的方式来标识Kubernetes对象，同时，又不对Kubernetes的核心逻辑产生影响。 标签可以用来组织和选择一组Kubernetes对象。 可以在创建Kubernetes对象时为其添加标签，也可以在创建以后再为其添加标签。 使用标签（Label）可以高效地查询和监听Kubernetes对象 每个Kubernetes对象可以有多个标签，同一个对象的标签的 Key 必须唯一，例如： 1234metadata: labels: key1: value1 key2: value2 为什么要使用标签使用标签，用户可以按照自己期望的形式组织 Kubernetes 对象之间的结构，而无需对 Kubernetes 有任何修改。 应用程序的部署或者批处理程序的部署通常都是多维度的（例如，多个高可用分区、多个程序版本、多个微服务分层）。管理这些对象时，很多时候要针对某一个维度的条件做整体操作，例如，将某个版本的程序整体删除，这种情况下，如果用户能够事先规划好标签的使用，再通过标签进行选择，就会非常地便捷。 标签的例子有： release: stable、release: canary environment: dev、environment: qa、environment: production tier: frontend、tier: backend、tier: cache partition: customerA、partition: customerB track: daily、track: weekly 上面只是一些使用比较普遍的标签，可以根据您自己的情况建立合适的使用标签 句法和字符集 标签是一组名值对（key/value pair）。标签的 key 可以有两个部分：可选的前缀和标签名，通过 / 分隔。 例如，下面的例子中的Pod包含两个标签 environment: production 和 app:nginx 12345678910111213apiVersion: v1kind: Podmetadata: name: label-demo labels: environment: production app: nginxspec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 标签选择器与 name 和 UID 不同，标签不一定是唯一的。通常来讲，会有多个Kubernetes对象包含相同的标签。通过使用标签选择器（label selector），用户/客户端可以选择一组对象。标签选择器（label selector）是 Kubernetes 中最主要的分类和筛选手段。 Kubernetes api server支持两种形式的标签选择器，equality-based 基于等式的 和 set-based 基于集合的。标签选择器可以包含多个条件，并使用逗号分隔，此时只有满足所有条件的 Kubernetes 对象才会被选中。 如果使用空的标签选择器或者不指定选择器，其含义由具体的 API 接口决定。 基于等式的选择方式 Equality- 或者 inequality-based 选择器可以使用标签的名和值来执行过滤选择。只有匹配所有条件的对象才被选中（被选中的对象可以包含未指定的标签）。可以使用三种操作符 =、==、!=。前两个操作符含义是一样的，都代表相等，后一个操作符代表不相等。例如： 1234# 选择了标签名为 `environment` 且 标签值为 `production` 的Kubernetes对象environment = production# 选择了标签名为 `tier` 且标签值不等于 `frontend` 的对象，以及不包含标签 `tier` 的对象tier != frontend 也可以使用逗号分隔的两个等式 environment=production,tier!=frontend，此时将选中所有 environment 为 production 且 tier 不为 frontend 的对象。 以Pod 的节点选择器为例，下面的 Pod 可以被调度到包含标签 accelerator=nvidia-tesla-p100 的节点上： 12345678910111213apiVersion: v1kind: Podmetadata: name: cuda-testspec: containers: - name: cuda-test image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 基于集合的选择方式 Set-based 标签选择器可以根据标签名的一组值进行筛选。支持的操作符有三种：in、notin、exists。例如： 12345678# 选择所有的包含 `environment` 标签且值为 `production` 或 `qa` 的对象environment in (production, qa)# 选择所有的 `tier` 标签不为 `frontend` 和 `backend`的对象，或不含 `tier` 标签的对象tier notin (frontend, backend)# 选择所有包含 `partition` 标签的对象partition# 选择所有不包含 `partition` 标签的对象!partition 可以组合多个选择器，用 , 分隔，, 相当于 AND 操作符。例如： 12# 选择包含 `partition` 标签（不检查标签值）且 `environment` 不是 `qa` 的对象partition,environment notin (qa) 基于集合的选择方式是一个更宽泛的基于等式的选择方式，例如，environment=production 等价于 environment in (production)；environment!=production 等价于 environment notin (production)。 基于集合的选择方式可以和基于等式的选择方式可以混合使用，例如： partition in (customerA, customerB),environment!=qa API查询条件LIST 和 WATCH 操作时，可指定标签选择器作为查询条件，以筛选指定的对象集合。两种选择方式都可以使用，但是要符合 URL 编码，例如： 基于等式的选择方式： ?labelSelector=environment%3Dproduction,tier%3Dfrontend 基于集合的选择方式： ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29 两种选择方式都可以在 kubectl 的 list 和 watch 命令中使用，例如： 使用基于等式的选择方式 1kubectl get pods -l environment=production,tier=frontend 使用基于集合的选择方式 1kubectl get pods -l 'environment in (production),tier in (frontend)' Kubernetes对象引用某些 Kubernetes 对象中（例如，Service和Deployment），使用标签选择器指定一组其他类型的 Kubernetes 对象（例如，Pod） ServiceService 中通过 spec.selector 字段来选择一组 Pod，并将服务请求转发到选中的 Pod 上。 在 yaml 或 json 文件中，标签选择器用一个 map 来定义，且支持基于等式的选择方式，例如： 12345678\"selector\": &#123; \"component\" : \"redis\",&#125;或selector: component: redis 上面的例子中定义的标签选择器等价于 component=redis 或 component in (redis) 有些对象支持基于集合的选择方式Job、Deployment、ReplicaSet 和 DaemonSet 同时支持基于等式的选择方式和基于集合的选择方式。例如： 123456selector: matchLabels: component: redis matchExpressions: - &#123;key: tier, operator: In, values: [cache]&#125; - &#123;key: environment, operator: NotIn, values: [dev]&#125; matchLabels 是一个 {key,value} 组成的 map。map 中的一个 {key,value} 条目相当于 matchExpressions 中的一个元素，其 key 为 map 的 key，operator 为 In， values 数组则只包含 value 一个元素。matchExpression 等价于基于集合的选择方式，支持的 operator 有 In、NotIn、Exists 和 DoesNotExist。当 operator 为 In 或 NotIn 时，values 数组不能为空。所有的选择条件都以 AND 的形式合并计算，即所有的条件都满足才可以算是匹配 。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"使用名称空间共享集群","slug":"使用名称空间共享集群","date":"2020-03-28T05:42:22.000Z","updated":"2020-03-28T06:23:37.173Z","comments":true,"path":"2020/03/28/使用名称空间共享集群/","link":"","permalink":"http://yoursite.com/2020/03/28/%E4%BD%BF%E7%94%A8%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E5%85%B1%E4%BA%AB%E9%9B%86%E7%BE%A4/","excerpt":"查看、创建、删除、使用namespace","text":"查看、创建、删除、使用namespace 使用名称空间共享集群一、查看名称空间查看集群中的名称空间列表： 123456[root@k8s-master k8s-yamls]# kubectl get namespacesNAME STATUS AGEdefault Active 23hkube-node-lease Active 23hkube-public Active 23hkube-system Active 23h Kubernetes 安装成功后，默认有初始化了三个名称空间： default 默认名称空间，如果 Kubernetes 对象中不定义 metadata.namespace 字段，该对象将放在此名称空间下 kube-system Kubernetes系统创建的对象放在此名称空间下 kube-public 此名称空间自动在安装集群是自动创建，并且所有用户都是可以读取的（即使是那些未登录的用户）。主要是为集群预留的，例如，某些情况下，某些Kubernetes对象应该被所有集群用户看到。 查看名称空间详细信息： 123456789[root@k8s-master k8s-yamls]# kubectl describe namespaces kube-systemName: kube-systemLabels: &lt;none&gt;Annotations: &lt;none&gt;Status: ActiveNo resource quota.No resource limits. Resource quota 汇总了名称空间中使用的资源总量，并指定了集群管理员定义该名称空间最多可以使用的资源量 Limit range 定义了名称空间中某种具体的资源类型的最大、最小值 名称空间可能有两种状态（phase）： Active 名称空间正在使用中 Termining 名称空间正在被删除，不能再向其中创建新的对象 二、创建名称空间使用kubectl有两种方式创建名称空间 1.通过 yaml 文件，创建文件 my-namespace.yaml 1234567apiVersion: v1kind: Namespacemetadata: name: &lt;名称空间的名字&gt;#执行命令kubectl create -f ./my-namespace.yaml 直接使用命令创建名称空间： 1kubectl create namespace &lt;名称空间的名字&gt; 注意： 名称空间可以定义一个可选项字段 finalizers，在名称空间被删除时，用来清理相关的资源。 如果定义了一个不存在的 finalizer，仍然可以成功创建名称空间，但是当删除该名称空间时，将卡在 Terminating 状态。 三、删除名称空间1kubectl delete namespaces &lt;名称空间的名字&gt; 注意： 该操作将删除名称空间中的所有内容（ 此删除操作是异步的，名称空间会停留在 Terminating 状态一段时间。 ） 四、使用名称空间切分集群理解 default 名称空间默认情况下，安装Kubernetes集群时，会初始化一个 default 名称空间，用来将承载那些未指定名称空间的 Pod、Service、Deployment等对象 创建新的名称空间假设企业使用同一个集群作为开发环境和生产环境（注意：通常开发环境和生产环境是物理隔绝的）： 开发团队期望有一个集群中的空间，以便他们可以查看查看和使用他们创建的 Pod、Service、Deployment等。在此空间中，Kubernetes对象被创建又被删除，为了适应敏捷开发的过程，团队中的许多人都可以在此空间内做他们想做的事情。 运维团队也期望有一个集群中的空间，在这里，将有严格的流程控制谁可以操作 Pod、Service、Deployment等对象，因为这些对象都直接服务于生产环境。 此时，可以将一个Kubernetes集群切分成两个名称空间：development 和 production。创建名称空间的 yaml 文件如下所示： 123456apiVersion: v1kind: Namespacemetadata: name: development labels: name: development 执行命令以创建 development 名称空间： 12345678910[root@k8s-master k8s-yamls]# kubectl create -f dev.yaml namespace/development created[root@k8s-master k8s-yamls]# kubectl get namespaces --show-labelsNAME STATUS AGE LABELSdefault Active 23h &lt;none&gt;development Active 2m10s name=developmentkube-node-lease Active 23h &lt;none&gt;kube-public Active 23h &lt;none&gt;kube-system Active 23h &lt;none&gt;production Active 67s name=production 在每个名称空间中创建 PodKubernetes名称空间为集群中的 Pod、Service、Deployment 提供了一个作用域。可以限定使用某个名称空间的用户不能看到另外一个名称空间中的内容。我们可以在 development 名称空间中创建一个简单的 Deployment 和 Pod 来演示这个特性。 1.执行命令以检查当前的 kubectl 上下文 1234567891011121314151617181920[root@k8s-master k8s-yamls]# kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority-data: DATA+OMITTED server: https://192.168.154.144:6443 name: kubernetescontexts:- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 2.执行命令 12[root@k8s-master k8s-yamls]# kubectl config current-contextkubernetes-admin@kubernetes 3.接下来，为 kubectl 定义一个上下文，以便在不同的名称空间中工作。cluster 和 user 字段的取值从前面的 current context 复制过来： 1234[root@k8s-master k8s-yamls]# kubectl config set-context dev --namespace=development --cluster=kubernetes-admin@kubernetes --user=kubernetes-admin@kubernetesContext \"dev\" created.[root@k8s-master k8s-yamls]# kubectl config set-context prod --namespace=production --cluster=kubernetes-admin@kubernetes --user=kubernetes-admin@kubernetesContext \"prod\" created. 上面的命令创建了两个 kubectl 的上下文，可以在两个不同的名称空间中工作： 切换到 development 名称空间： 123456[root@k8s-master k8s-yamls]# kubectl config use-context devSwitched to context \"dev\".#验证[root@k8s-master k8s-yamls]# kubectl config current-contextdev 此时，通过 kubectl 向 Kubernetes 集群发出的所有指令都限定在名称空间 development 里 在不同的namespace里工作创建一个 nginx 1234567891011kubectl run snowflake --image=nginx:1.7.9 --replicas=2#刚刚创建的 Deployment 副本数为 2，运行了一个 nginx 容器。kubectl get deploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEsnowflake 2 2 2 2 2mkubectl get pods -l run=snowflakeNAME READY STATUS RESTARTS AGEsnowflake-3968820950-9dgr8 1/1 Running 0 2msnowflake-3968820950-vgc4n 1/1 Running 0 2m 此时，开发人员可以做任何他想要做的操作，所有操作都限定在名称空间 development 里，而无需担心影响到 production 名称空间中的内容 用户在一个名称空间创建的内容对于另外一个名称空间来说是不可见的。同时也可以为不同的名称空间定义不同的访问权限控制。 为什么需要名称空间一个Kubernetes集群应该可以满足多组用户的不同需要。Kubernetes名称空间可以使不同的项目、团队或客户共享同一个 Kubernetes 集群。实现的方式是，提供： namespace的作用域 为不同的名称空间定义不同的授权方式和资源分配策略 Resource Quota 和 resource limit range 每一个用户组都期望独立于其他用户组进行工作。通过名称空间，每个用户组拥有自己的： Kubernetes 对象（Pod、Service、Deployment等） 授权（谁可以在该名称空间中执行操作） 资源分配（该用户组或名称空间可以使用集群中的多少计算资源） 可能的使用情况有： 集群管理员通过一个Kubernetes集群支持多个用户组 集群管理员将集群中某个名称空间的权限分配给用户组中的受信任的成员 集群管理员可以限定某一个用户组可以消耗的资源数量，以避免其他用户组受到影响 集群用户可以使用自己的Kubernetes对象，而不会与集群中的其他用户组相互干扰","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"操作Kubernetes","slug":"操作Kubernetes","date":"2020-03-28T02:24:30.000Z","updated":"2020-03-28T03:52:42.373Z","comments":true,"path":"2020/03/28/操作Kubernetes/","link":"","permalink":"http://yoursite.com/2020/03/28/%E6%93%8D%E4%BD%9CKubernetes/","excerpt":"什么是Kubernetes对象，管理Kubernetes对象，名称（name），名称空间（namespace）的概念","text":"什么是Kubernetes对象，管理Kubernetes对象，名称（name），名称空间（namespace）的概念 一、什么是Kubernetes对象​ Kubernetes对象指的是Kubernetes系统的持久化实体，所有这些对象合起来，代表了集群的实际情况。常规的应用里，应用程序的数据存储在数据库中，Kubernetes将其数据以Kubernetes对象的形式通过 api server存储在 etcd 中。具体来说，这些数据（Kubernetes对象）描述了： 集群中运行了哪些容器化应用程序（以及在哪个节点上运行） 集群中对应用程序可用的资源 应用程序相关的策略定义，例如，重启策略、升级策略、容错策略 其他Kubernetes管理应用程序时所需要的信息 操作 Kubernetes 对象（创建、修改、删除）的方法主要有： kubectl 命令行工具 kuboard 图形界面工具 kubectl、kuboard 最终都通过调用 kubernetes API 来实现对 Kubernetes 对象的操作。 对象的spec和status每一个 Kubernetes 对象都包含了两个重要的字段： spec 必须由您来提供，描述了您对该对象所期望的 目标状态 status 只能由 Kubernetes 系统来修改，描述了该对象在 Kubernetes 系统中的 实际状态 Kubernetes通过对应的控制器，不断地使实际状态趋向于期望的目标状态。 ​ 例如，一个 Kubernetes Deployment 对象可以代表一个应用程序在集群中的运行状态。当创建 Deployment 对象时，可以通过 Deployment 的 spec 字段指定需要运行应用程序副本数（假设为3）。Kubernetes 从 Deployment 的 spec 中读取这些信息，并创建指定容器化应用程序的 3 个副本，再将实际的状态更新到 Deployment 的 status 字段。Kubernetes 系统将不断地比较 实际状态 staus 和 目标状态 spec 之间的差异，并根据差异做出对应的调整。例如，如果任何一个副本运行失败了，Kubernetes 将启动一个新的副本，以替代失败的副本 描述Kubernetes对象在 Kubernetes 中创建一个对象时，必须提供 该对象的 spec 字段，通过该字段描述您期望的 目标状态 该对象的一些基本信息，例如名字 如果使用 kubectl 创建对象，必须编写 .yaml 格式的文件下面是一个 kubectl 可以使用的 .yaml 文件： 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 # 运行 2 个容器化应用程序副本 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 使用 kube apply 命令可以创建该 .yaml 文件中的 Deployment 对象： 12345678910[root@k8s-master k8s-yamls]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-deployment created[root@k8s-master k8s-yamls]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-deployment-54f57cf6bf-528xn 1/1 Running 0 4m35snginx-deployment-54f57cf6bf-bgclh 1/1 Running 0 4m35s#删除[root@k8s-master k8s-yamls]# kubectl delete -f nginx-deployment.yaml deployment.apps \"nginx-deployment\" deleted 必填字段在上述的 .yaml 文件中，如下字段是必须填写的： apiVersion 用来创建对象时所使用的Kubernetes API版本 kind 被创建对象的类型 metadata 用于唯一确定该对象的元数据：包括 name 和 namespace，如果 namespace 为空，则默认值为 default spec 描述您对该对象的期望状态 二、管理Kubernetes对象三种管理方式 管理方式 操作对象 推荐的环境 参与编辑的人数 学习曲线 指令性的命令行 Kubernetes对象 开发环境 1+ 最低 指令性的对象配置 单个 yaml 文件 生产环境 1 适中 声明式的对象配置 包含多个 yaml 文件的多个目录 生产环境 1+ 最高 指令性的命令行当使用指令性的命令行（imperative commands）时，用户通过向 kubectl 命令提供参数的方式，直接操作集群中的 Kubernetes 对象。此时，用户无需编写或修改 .yaml 文件。 这是在 Kubernetes 集群中执行一次性任务的一个简便的办法。由于这种方式直接修改 Kubernetes 对象，也就无法提供历史配置查看的功能。 例子创建一个 Deployment 对象，以运行一个 nginx 实例： 1kubectl run nginx --image nginx 下面的命令完成了相同的任务，但是命令格式不同： 1kubectl create deployment nginx --image nginx 优缺点与编写 .yaml 文件进行配置的方式相比的优势： 命令简单，易学易记，只需要一个步骤，就可以对集群执行变更 缺点： 使用命令，无法进行变更review的管理；不提供日志审计；没有创建新对象的模板 指令性的对象配置使用指令性的对象配置（imperative object configuration）时，需要向 kubectl 命令指定具体的操作（create,replace,apply,delete等），可选参数以及至少一个配置文件的名字。配置文件中必须包括一个完整的对象的定义，可以是 yaml 格式，也可以是 json 格式。 通过配置文件创建对象 1kubectl create -f nginx.yaml 删除两个配置文件中的对象 1kubectl delete -f nginx.yaml -f redis.yaml 直接使用配置文件中的对象定义，替换Kubernetes中对应的对象： 1kubectl replace -f nginx.yaml 声明式的对象配置当使用声明式的对象配置时，用户操作本地存储的Kubernetes对象配置文件，然而，在将文件传递给 kubectl 命令时，并不指定具体的操作，由 kubectl 自动检查每一个对象的状态并自行决定是创建、更新、还是删除该对象。使用这种方法时，可以直接针对一个或多个文件目录进行操作（对不同的对象可能需要执行不同的操作）。 处理 configs 目录中所有配置文件中的Kubernetes对象，根据情况创建对象、或更新Kubernetes中已经存在的对象。可以先执行 diff 指令查看具体的变更，然后执行 apply 指令执行变更： 123456kubectl diff -f configs/kubectl apply -f configs/#递归处理目录中的内容kubectl diff -R -f configs/kubectl apply -R -f configs/ 三、名称 Kubernetes REST API 中，所有的对象都是通过 name 和 UID 唯一性确定。 Names 同一个名称空间下，同一个类型的对象，可以通过 name 唯一性确定。如果删除该对象之后，可以再重新创建一个同名对象。 例如，下面的配置文件定义了一个 name 为 nginx-demo 的 Pod，该 Pod 包含一个 name 为 nginx 的容器： 12345678910apiVersion: v1kind: Podmetadata: name: nginx-demospec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 UIDsUID 是由 Kubernetes 系统生成的，唯一标识某个 Kubernetes 对象的字符串。 Kubernetes集群中，每创建一个对象，都有一个唯一的 UID。用于区分多次创建的同名对象（如前所述，按照名字删除对象后，重新再创建同名对象时，两次创建的对象 name 相同，但是 UID 不同。） 四、名称空间何时使用名称空间 名称空间的用途是，为不同团队的用户（或项目）提供虚拟的集群空间（划分集群的资源），也可以用来区分开发环境/测试环境、准上线环境/生产环境。 名称空间为名称提供了作用域。名称空间内部的同类型对象不能重名，但是跨名称空间可以有同名同类型对象。名称空间不可以嵌套，任何一个Kubernetes对象只能在一个名称空间中。 在 Kubernetes 将来的版本中，同名称空间下的对象将默认使用相同的访问控制策略。 当Kubernetes对象之间的差异不大时，无需使用名称空间来区分，例如，同一个软件的不同版本，只需要使用 labels 来区分即可。 如何使用名称空间查看名称空间执行命令 kubectl get namespaces 可以查看名称空间 Kubernetes 安装成功后，默认有初始化了三个名称空间： default 默认名称空间，如果 Kubernetes 对象中不定义 metadata.namespace 字段，该对象将放在此名称空间下 kube-system Kubernetes系统创建的对象放在此名称空间下 kube-public 此名称空间自动在安装集群是自动创建，并且所有用户都是可以读取的（即使是那些未登录的用户）。主要是为集群预留的，例如，某些情况下，某些Kubernetes对象应该被所有集群用户看到。 在执行请求时设定namespace执行 kubectl 命令时，可以使用 --namespace 参数指定名称空间，例如： 12kubectl run nginx --image=nginx --namespace=&lt;your_namespace&gt;kubectl get pods --namespace=&lt;your_namespace&gt; 设置名称空间偏好可以通过 set-context 命令改变当前kubectl 上下文的名称空间，后续所有命令都默认在此名称空间下执行。 123kubectl config set-context --current --namespace=&lt;your_namespace&gt;# 验证结果kubectl config view --minify | grep namespace: 名称空间与DNS当创建一个 Service 时，Kubernetes 为其创建一个对应的DNS 条目。该 DNS 记录的格式为 &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local ，也就是说，如果在容器中只使用 &lt;service-name&gt; ，其DNS将解析到同名称空间下的 Service。这个特点在多环境的情况下非常有用，例如将开发环境、测试环境、生产环境部署在不同的名称空间下，应用程序只需要使用&lt;service-name&gt;即可进行服务发现，无需为不同的环境修改配置。如果跨名称空间访问服务，则必须使用完整的域名（fully qualified domain name，FQDN）。 并非所有对象都在名称空间里大部分的 Kubernetes 对象（例如，Pod、Service、Deployment、StatefulSet等）都必须在名称空间里。但是某些更低层级的对象，是不在任何名称空间中的，例如nodes、persistentVolumes、storageClass等 执行一下命令可查看哪些 Kubernetes 对象在名称空间里，哪些不在： 12345# 在名称空间里kubectl api-resources --namespaced=true# 不在名称空间里kubectl api-resources --namespaced=false","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"kubeadm安装Kubernetes1.16.3","slug":"kubeadm安装Kubernetes1-16-3","date":"2020-03-27T07:11:48.000Z","updated":"2020-03-27T07:39:50.190Z","comments":true,"path":"2020/03/27/kubeadm安装Kubernetes1-16-3/","link":"","permalink":"http://yoursite.com/2020/03/27/kubeadm%E5%AE%89%E8%A3%85Kubernetes1-16-3/","excerpt":"系统环境： CentOS 版本：7.7 Docker 版本：18.09.9-3 Calico 版本：v3.10 Kubernetes 版本：1.16.3 Kubernetes Newwork 模式：IPVS Kubernetes Dashboard 版本：dashboard:v2.0.0-beta6","text":"系统环境： CentOS 版本：7.7 Docker 版本：18.09.9-3 Calico 版本：v3.10 Kubernetes 版本：1.16.3 Kubernetes Newwork 模式：IPVS Kubernetes Dashboard 版本：dashboard:v2.0.0-beta6 一、更新系统内核（全部节点）由于 Docker 对系统内核有一定的要求，所以我们最好使用 yum 来更新系统软件及其内核。 1234567891011#备份本地 yum 源$ mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo_bak # 获取阿里 yum 源配置文件$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #清理 yum$ yum clean all#更新软件版本并且更新现有软件$ yum -y update 二、基础环境设置（全部节点）Kubernetes 需要一定的环境来保证正常运行，如各个节点时间同步，主机名称解析，关闭防火墙等等。 1、修改 Host分布式系统环境中的多主机通信通常基于主机名称进行，这在 IP 地址存在变化的可能 性时为主机提供了固定的访问人口，因此一般需要有专用的 DNS 服务负责解析各节点主机。考虑到此处部署的是测试集群，因此为了降低系复杂度，这里将基于 hosts 的文件进行主机名称解析。 1$ vim /etc/hosts 加入下面内容： 123192.168.2.11 k8s-master192.168.2.12 k8s-node-01192.168.2.13 k8s-node-02 2、修改 Hostnamekubernetes 中会以各个服务的 hostname 为其节点命名，所以需要进入不同的服务器修改 hostname 名称。 1234567891011#修改 192.168.2.11 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-master$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts#修改 192.168.2.12 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-node-01$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts#修改 192.168.2.13 服务器，设置 hostname，然后将 hostname 写入 hosts$ hostnamectl set-hostname k8s-node-02$ echo \"127.0.0.1 $(hostname)\" &gt;&gt; /etc/hosts 3、主机时间同步将各个服务器的时间同步，并设置开机启动同步时间服务。 1$ systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service 4、关闭防火墙服务关闭防火墙，并禁止开启启动。 1$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld 5、关闭并禁用SELinux关闭SELinux，并编辑／etc/sysconfig selinux 文件，以彻底禁用 SELinux 12$ setenforce 0$ sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config 查看selinux状态 1$ getenforce 6、禁用 Swap 设备关闭当前已启用的所有 Swap 设备： 1$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0 编辑 fstab 配置文件，注释掉标识为 Swap 设备的所有行： 123$ vi /etc/fstab#/dev/mapper/centos-swap swap swap defaults 0 0 7、设置内核参数配置内核参数： 123456789$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10EOF 使配置生效: 12345678#挂载 br_netfilter$ modprobe br_netfilter#使配置生效$ sysctl -p /etc/sysctl.d/k8s.conf#查看是否生成相关文件$ ls /proc/sys/net/bridge 8、配置 IPVS 模块由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块： ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4 12345678910$ cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF 执行脚本并查看是否正常加载内核模块： 12345678#修改脚本权限$ chmod 755 /etc/sysconfig/modules/ipvs.modules #执行脚本$ bash /etc/sysconfig/modules/ipvs.modules #查看是否已经正确加载所需的内核模块$ lsmod | grep -e ip_vs -e nf_conntrack_ipv4 安装 ipset 1$ yum install -y ipset 9、配置资源限制123456echo \"* soft nofile 65536\" &gt;&gt; /etc/security/limits.confecho \"* hard nofile 65536\" &gt;&gt; /etc/security/limits.confecho \"* soft nproc 65536\" &gt;&gt; /etc/security/limits.confecho \"* hard nproc 65536\" &gt;&gt; /etc/security/limits.confecho \"* soft memlock unlimited\" &gt;&gt; /etc/security/limits.confecho \"* hard memlock unlimited\" &gt;&gt; /etc/security/limits.conf 10、安装依赖包以及相关工具12$ yum install -y epel-release$ yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim ntpdate libseccomp libtool-ltdl 三、安装Docker（全部节点）1、移除之前安装过的Docker1234567891011$ sudo yum -y remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-ce-cli \\ docker-engine 查看还有没有存在的 Docker 组件，一定要确保删除干净： 1$ rpm -qa | grep docker 有则通过命令 yum -y remove XXX 来删除，比如： 1$ yum remove docker-ce-cli 2、更换 Docker 的 yum 源由于官方下载速度比较慢，所以需要更改 Docker 安装的 yum 源，这里推荐用阿里镜像源： 1$ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3、显示 docker 所有可安装版本：1$ yum list docker-ce --showduplicates | sort -r 、安装指定版本 docker 注意：安装前一定要提前查询将要安装的 Kubernetes 版本是否和 Docker 版本对应。 1$ yum install -y docker-ce-18.09.9-3.el7 设置镜像存储目录，找到大点的挂载的目录进行存储 1234$ vi /lib/systemd/system/docker.service#找到这行，往后面加上存储目录，例如这里是 --graph /apps/dockerExecStart=/usr/bin/docker --graph /apps/docker 5、配置 Docker 参数和镜像加速器Kubernetes 推荐的一些 Docker 配置参数，这里配置一下。还有就是由于国内访问 Docker 仓库速度很慢，所以国内几家云厂商推出镜像加速下载的代理加速器，这里也需要配置一下。 创建 Docker 配置文件的目录： 1$ mkdir -p /etc/docker 添加配置文件： 123456789101112131415161718192021$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [ \"https://dockerhub.azk8s.cn\", \"http://hub-mirror.c.163.com\", \"https://registry.docker-cn.com\" ], \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\":\"5\" &#125;&#125;EOF 6、启动 docker 并设置 docker 开机启动启动 Docker： 1$ systemctl start docker &amp;&amp; systemctl enable docker 如果 Docker 已经启动，则需要重启 Docker： 12$ systemctl daemon-reload$ systemctl restart docker 四、安装 kubelet、kubectl、kubeadm（全部节点）1、配置可用的国内 yum 源1234567891011$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 2、安装 kubelet、kubectl、kubeadm kubelet: 在集群中的每个节点上用来启动 pod 和 container 等。 kubectl: 用来与集群通信的命令行工具。 kubeadm: 用来初始化集群的指令。 注意安装顺序，一定不要先安装 kubeadm，因为 kubeadm 会自动安装最新版本的 kubelet 与 kubectl，导致版本不一致问题。 12345678#安装 kubelet$ yum install -y kubelet-1.16.3-0#安装 kubectl$ yum install -y kubectl-1.16.3-0#安装 kubeadm$ yum install -y kubeadm-1.16.3-0 3、启动 kubelet 并配置开机启动1$ systemctl start kubelet &amp;&amp; systemctl enable kubelet 检查状态时会发现 kubelet 是 failed 状态，等初 master 节点初始化完成后即可显示正常。 五、重启服务器（全部节点）为了防止发生某些未知错误，这里我们重启下服务器，方便进行后续操作 1$ reboot 六、kubeadm 安装 kubernetes（Master 节点）创建 kubeadm 配置文件 kubeadm-config.yaml，然后需要配置一些参数： 配置 localAPIEndpoint.advertiseAddress 参数，调整为你的 Master 服务器地址。 配置 imageRepository 参数，调整 kubernetes 镜像下载地址为阿里云。 配置 networking.podSubnet 参数，调整为你要设置的网络范围。 kubeadm-config.yaml 123456789101112131415161718192021222324$ cat &gt; kubeadm-config.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta2kind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.2.11 bindPort: 6443nodeRegistration: taints: - effect: PreferNoSchedule key: node-role.kubernetes.io/master---apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationimageRepository: registry.aliyuncs.com/google_containerskubernetesVersion: v1.16.3networking: podSubnet: 10.244.0.0/16---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvsEOF kubeadm 初始化 kubernetes 集群 1$ kubeadm init --config kubeadm-config.yaml 部署日志信息： 1234567891011121314151617181920......[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.2.11:6443 --token 4udy8a.f77ai0zun477kx0p \\ --discovery-token-ca-cert-hash sha256:4645472f24b438e0ecf5964b6dcd64913f68e0f9f7458768cfb96a9ab16b4212 上面记录了完成的初始化输出的内容，根据输出的内容基本上可以看出手动初始化安装一个Kubernetes集群所需要的关键步骤。 其中有以下关键内容： [kubelet] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml” [certificates]生成相关的各种证书 [kubeconfig]生成相关的kubeconfig文件 [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到 在此处看日志可以知道，可以通过下面命令，添加 kubernetes 相关环境变量： 123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 或者直接只用命令 12345kubeadm init \\ --kubernetes-version=v1.16.3 \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.2.11 \\ --ignore-preflight-errors=Swap 提前拉取镜像 1kubeadm config images pull --config kubeadm-master.config 安装过程中遇到异常： 12Copy[preflight] Some fatal errors occurred: [ERROR DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty 直接删除/var/lib/etcd文件夹 如果初始化过程出现问题，使用如下命令重置： 1kubeadm reset 七、工作节点加入集群（Work Node 节点）根据上面 Master 节点创建 Kubernetes 集群时的日志信息，可以知道在各个节点上执行下面命令来让工作节点加入主节点： 12$ kubeadm join 192.168.2.11:6443 --token 4udy8a.f77ai0zun477kx0p \\ --discovery-token-ca-cert-hash sha256:4645472f24b438e0ecf5964b6dcd64913f68e0f9f7458768cfb96a9ab16b4212 八、部署网络插件（Master 节点）Kubernetes 中可以部署很多种网络插件，不过比较流行也推荐的有两种： Flannel： Flannel 是基于 Overlay 网络模型的网络插件，能够方便部署，一般部署后只要不出问题，一般不需要管它。 Calico： 与 Flannel 不同，Calico 是一个三层的数据中心网络方案，使用 BGP 路由协议在主机之间路由数据包，可以灵活配置网络策略。 这两种网络根据环境任选其一即可，这里使用的是 Calico，可以按下面步骤部署： 1、部署 Calico 网络插件下载 Calico 部署文件，并替换里面的网络范围为上面 kubeadm 中 networking.podSubnet 配置的值。 12345678#下载 calico 部署文件$ wget https://docs.projectcalico.org/v3.10/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml #替换 calico 部署文件的 IP 为 kubeadm 中的 networking.podSubnet 参数 10.244.0.0。$ sed -i 's/192.168.0.0/10.244.0.0/g' calico.yaml#部署 Calico 插件$ kubectl apply -f calico.yaml 2、查看 Pod 是否成功启动12345678910111213141516$ kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcalico-kube-controllers-6b64bcd855-jn8pz 1/1 Running 0 2m40scalico-node-5wssd 1/1 Running 0 2m40scalico-node-7tw94 1/1 Running 0 2m40scalico-node-xzfp4 1/1 Running 0 2m40scoredns-58cc8c89f4-hv4fn 1/1 Running 0 21mcoredns-58cc8c89f4-k97x6 1/1 Running 0 21metcd-k8s-master 1/1 Running 0 20mkube-apiserver-k8s-master 1/1 Running 0 20mkube-controller-manager-k8s-master 1/1 Running 0 20mkube-proxy-9dlpz 1/1 Running 0 14mkube-proxy-krd5n 1/1 Running 0 14mkube-proxy-tntpr 1/1 Running 0 21mkube-scheduler-k8s-master 1/1 Running 0 20m 可以看到所以 Pod 都已经成功启动。 九、配置 Kubectl 命令自动补全（Master 节点）1$ yum install -y bash-completion 添加补全配置 123$ source &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion$ source &lt;(kubectl completion bash)$ echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~&#x2F;.bashrc 添加完成就可与通过输入 kubectl 后，按补全键（一般为 tab）会自动补全对应的命令。 十、查看是否开启 IPVS（Master 节点）上面全部组件都已经部署完成，不过还需要确认是否成功将网络模式设置为 IPVS，可以查看 kube-proxy 日志，在日志信息中查找是否存在 IPVS 关键字信息来确认。 12345$ kubectl get pod -n kube-system | grep kube-proxykube-proxy-9dlpz 1/1 Running 0 42mkube-proxy-krd5n 1/1 Running 0 42mkube-proxy-tntpr 1/1 Running 0 49m 选择其中一个 Pod ，查看该 Pod 中的日志信息中是否存在 ipvs 信息： 12345678910111213141516$ kubectl logs kube-proxy-9dlpz -n kube-systemI1120 18:13:46.357178 1 node.go:135] Successfully retrieved node IP: 192.168.2.13I1120 18:13:46.357265 1 server_others.go:176] Using ipvs Proxier.W1120 18:13:46.358005 1 proxier.go:420] IPVS scheduler not specified, use rr by defaultI1120 18:13:46.358919 1 server.go:529] Version: v1.16.3I1120 18:13:46.359327 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_max&#39; to 131072I1120 18:13:46.359379 1 conntrack.go:52] Setting nf_conntrack_max to 131072I1120 18:13:46.359426 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_tcp_timeout_established&#39; to 86400I1120 18:13:46.359452 1 conntrack.go:100] Set sysctl &#39;net&#x2F;netfilter&#x2F;nf_conntrack_tcp_timeout_close_wait&#39; to 3600I1120 18:13:46.359626 1 config.go:313] Starting service config controllerI1120 18:13:46.359685 1 shared_informer.go:197] Waiting for caches to sync for service configI1120 18:13:46.359833 1 config.go:131] Starting endpoints config controllerI1120 18:13:46.359889 1 shared_informer.go:197] Waiting for caches to sync for endpoints configI1120 18:13:46.460013 1 shared_informer.go:204] Caches are synced for service config I1120 18:13:46.460062 1 shared_informer.go:204] Caches are synced for endpoints config 如上，在日志中查到了 IPVS 字样，则代表使用了 IPVS 模式。 十一、集群中移除Node 如果需要从集群中移除node2这个Node执行下面的命令： 在master节点上执行： 123kubectl drain node2 --delete-local-data --force --ignore-daemonsets kubectl delete node node2 在node2上执行： 123456kubeadm reset ifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf &#x2F;var&#x2F;lib&#x2F;cni&#x2F; 十二、部署 Kubernetes Dashboard接下来我们将部署 Kubernetes 的控制看板，由于集群为 1.16.3，而 Dashboard 的稳定版本还是基于 Kubernetes 1.10 版本，所以我们直接使用 Kubernetes Dashboard 2.0.0 Bate 版本，虽然为测试版本，不过它比较新，对新版本的兼容还是比旧版本好些，坐等它稳定，这里先部署尝尝鲜。 1、创建 Dashboard 部署文件k8s-dashboard-deploy.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184$ cat &gt; k8s-dashboard-deploy.yaml &lt;&lt; EOFapiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemrules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\", \"kubernetes-dashboard-csrf\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\", \"dashboard-metrics-scraper\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\", \"dashboard-metrics-scraper\", \"http:dashboard-metrics-scraper\"] verbs: [\"get\"]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboardrules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kube-systemtype: Opaquedata: csrf: \"\"---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kube-systemtype: Opaque---kind: ConfigMapapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kube-system---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.0.0-beta6 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kube-system #设置为当前namespace volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule EOF 部署 Kubernetes Dashboard 1$ kubectl apply -f k8s-dashboard-deploy.yaml 2、创建监控信息 kubernetes-metrics-scraperk8s-dashboard-metrics.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ cat &gt; k8s-dashboard-metrics.yaml &lt;&lt; EOFkind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-metrics-scraper name: dashboard-metrics-scraper namespace: kube-systemspec: ports: - port: 8000 targetPort: 8000 selector: k8s-app: kubernetes-metrics-scraper---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-metrics-scraper name: kubernetes-metrics-scraper namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-metrics-scraper template: metadata: labels: k8s-app: kubernetes-metrics-scraper spec: containers: - name: kubernetes-metrics-scraper image: kubernetesui/metrics-scraper:v1.0.1 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 serviceAccountName: kubernetes-dashboard tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule EOF 部署 Dashboard Metrics 1$ kubectl apply -f k8s-dashboard-metrics.yaml 3、创建 Dashboard ServiceAccount访问 Kubernetes Dashboard 时会验证身份，需要提前在 Kubernetes 中创建一个一定权限的 ServiceAccount，然后获取其 Token 串，这里为了简单方便，直接创建一个与管理员绑定的服务账户，获取其 Token 串。 k8s-dashboard-rbac.yaml 123456789101112131415161718192021222324252627$ cat &gt; k8s-dashboard-token.yaml &lt;&lt; EOFkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\"roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: admin namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile EOF 部署访问的 ServiceAccount： 1$ kubectl apply -f k8s-dashboard-rbac.yaml 获取 Token： 1$ kubectl describe secret/$(kubectl get secret -n kube-system |grep admin|awk '&#123;print $1&#125;') -n kube-system 然后输入 Kubernetes 集群任意节点地址配置上面配置的 Service 的 NodePort 30001 来访问看板。输入地址https://192.168.2.11:30001进入看板页面，输入上面获取的 Token 串进行验证登录。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-控制器","slug":"k8s进阶-架构-控制器","date":"2020-03-26T08:49:58.000Z","updated":"2020-03-26T09:17:26.418Z","comments":true,"path":"2020/03/26/k8s进阶-架构-控制器/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84-%E6%8E%A7%E5%88%B6%E5%99%A8/","excerpt":"控制器不断地尝试着将当前的状态调整到目标状态","text":"控制器不断地尝试着将当前的状态调整到目标状态 控制器在 Kubernetes 中，控制器 就是上面所说的 控制循环，它不断监控着集群的状态，并对集群做出对应的变更调整。每一个控制器都不断地尝试着将 当前状态 调整到 目标状态。 控制器模式在 Kubernetes 中，每个控制器至少追踪一种类型的资源。这些资源对象中有一个 spec 字段代表了目标状态。资源对象对应的控制器负责不断地将当前状态调整到目标状态。 理论上，控制器可以自己直接执行调整动作，然而，在Kubernetes 中，更普遍的做法是，控制器发送消息到 API Server，而不是直接自己执行调整动作。 通过APIServer进行控制以 Kubernetes 中自带的一个控制器 Job Controller 为例。Kubernetes 自带的控制器都是通过与集群中 API Server 交互来达到调整状态的目的。 Job 是一种 Kubernetes API 对象，一个 Job 将运行一个（或多个）Pod，执行一项任务，然后停止。当新的 Job 对象被创建时，Job Controller 将确保集群中有合适数量的节点上的 kubelet 启动了指定个数的 Pod，以完成 Job 的执行任务。Job Controller 自己并不执行任何 Pod 或容器，而是发消息给 API Server，由其他的控制组件配合 API Server，以执行创建或删除 Pod 的实际动作。 当新的 Job 对象被创建时，目标状态是指定的任务被执行完成。Job Controller 调整集群的当前状态以达到目标状态：创建 Pod 以执行 Job 中指定的任务 控制器同样也会更新其关注的 API 对象。例如：一旦 Job 的任务执行结束，Job Controller 将更新 Job 的 API 对象，将其标注为 Finished。 直接控制某些特殊的控制器需要对集群外部的东西做调整。例如，想用一个控制器确保集群中有足够的节点，此时控制器需要调用云供应商的接口以创建新的节点或移除旧的节点。这类控制器将从 API Server 中读取关于目标状态的信息，并直接调用外部接口以实现调整目标。 目标状态 vs 当前状态Kubernetes 使用了 云原生（cloud-native）的视角来看待系统，并且可以持续应对变化。集群在运行的过程中，任何时候都有可能发生突发事件，而控制器则自动地修正这些问题。这就意味着，集群永远不会达到一个稳定不变的状态。 这种通过控制器监控集群状态并利用负反馈原理不断接近目标状态的系统，相较于那种完成安装后就不再改变的系统，是一种更高级的系统形态。 运行控制器的方式Kubernetes 在 kube-controller-manager 中运行了大量的内建控制器（例如，Deployment Controller、Job Controller、StatefulSet Controller、DaemonSet Controller 等）。这些内建控制器提供了 Kubernetes 非常重要的核心功能。Kubernetes 可以运行一个 master 集群，以实现内建控制器的高可用。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-集群内的通信","slug":"k8s进阶-架构-集群内的通信","date":"2020-03-26T08:24:42.000Z","updated":"2020-03-27T07:24:09.675Z","comments":true,"path":"2020/03/26/k8s进阶-架构-集群内的通信/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84-%E9%9B%86%E7%BE%A4%E5%86%85%E7%9A%84%E9%80%9A%E4%BF%A1/","excerpt":"Master-Node之间的通信","text":"Master-Node之间的通信 Master-Node之间的通信Master-Node 之间的通信可以分为如下两类： Cluster to Master Master to Cluster Cluster to Master所有从集群访问 Master 节点的通信，都是针对 apiserver 的（没有任何其他 master 组件发布远程调用接口）。通常安装 Kubernetes 时，apiserver 监听 HTTPS 端口（443），并且配置了一种或多种客户端认证方式 authentication。至少需要配置一种形式的授权方式 authorization，尤其是匿名访问 anonymous requests或 Service Account Tokens被启用的情况下。 节点上必须配置集群（apiserver）的公钥根证书（public root certificate），此时，在提供有效的客户端身份认证的情况下，节点可以安全地访问 APIServer。例如，在 Google Kubernetes Engine 的一个默认 Kubernetes 安装里，通过客户端证书为 kubelet 提供客户端身份认证。 对于需要调用 APIServer 接口的 Pod，应该为其关联 Service Account，此时，Kubernetes将在创建Pod时自动为其注入公钥根证书（public root certificate）以及一个有效的 bearer token（放在HTTP请求头Authorization字段）。所有名称空间中，都默认配置了名为 kubernetes Kubernetes Service，该 Service对应一个虚拟 IP（默认为 10.96.0.1），发送到该地址的请求将由 kube-proxy 转发到 apiserver 的 HTTPS 端口上。 默认情况下，从集群（节点以及节点上运行的 Pod）访问 master 的连接是安全的，因此，可以通过不受信的网络或公网连接 Kubernetes 集群 Master to Cluster从 master（apiserver）到Cluster存在着两条主要的通信路径： apiserver 访问集群中每个节点上的 kubelet 进程 使用 apiserver 的 proxy 功能，从 apiserver 访问集群中的任意节点、Pod、Service apiserver to kubeletapiserver 在如下情况下访问 kubelet： 抓取 Pod 的日志 通过 kubectl exec -it 指令（或 kuboard 的终端界面）获得容器的命令行终端 提供 kubectl port-forward 功能 这些连接的访问端点是 kubelet 的 HTTPS 端口。默认情况下，apiserver 不校验 kubelet 的 HTTPS 证书，这种情况下，连接可能会收到 man-in-the-middle 攻击，因此该连接如果在不受信网络或者公网上运行时，是 不安全 的。 如果要校验 kubelet 的 HTTPS 证书，可以通过 --kubelet-certificate-authority 参数为 apiserver 提供校验 kubelet 证书的根证书。 如果不能完成这个配置，又需要通过不受信网络或公网将节点加入集群，则需要使用SSH隧道连接 apiserver 和 kubelet。 同时，Kubelet authentication/authorization需要激活，以保护 kubelet API apiserver to nodes, pods, services从 apiserver 到 节点/Pod/Service 的连接使用的是 HTTP 连接，没有进行身份认证，也没有进行加密传输。您也可以通过增加 https 作为 节点/Pod/Service 请求 URL 的前缀，但是 HTTPS 证书并不会被校验，也无需客户端身份认证，因此该连接是无法保证一致性的。目前，此类连接如果运行在非受信网络或公网上时，是 不安全 的 SSH隧道Kubernetes 支持 SSH隧道（tunnel）来保护 Master –&gt; Cluster 访问路径。此时，apiserver 将向集群中的每一个节点建立一个 SSH隧道（连接到端口22的ssh服务）并通过隧道传递所有发向 kubelet、node、pod、service 的请求。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s进阶-架构-节点","slug":"k8s进阶-架构","date":"2020-03-26T02:21:12.000Z","updated":"2020-03-26T09:17:36.969Z","comments":true,"path":"2020/03/26/k8s进阶-架构/","link":"","permalink":"http://yoursite.com/2020/03/26/k8s%E8%BF%9B%E9%98%B6-%E6%9E%B6%E6%9E%84/","excerpt":"节点 节点管理","text":"节点 节点管理 节点节点状态节点的状态包含如下信息： Addresses Conditions Capacity and Allocatable Info 执行以下命令可查看所有节点的列表 1kubectl get nodes -o wide 执行以下命令可查看节点状态以及节点的其他详细信息： 1kubectl describe node &lt;your-node-name&gt; Addresses HostName： 在节点命令行界面上执行 hostname 命令所获得的值。启动 kubelet 时，可以通过参数 --hostname-override 覆盖 ExternalIP：通常是节点的外部IP（可以从集群外访问的内网IP地址；上面的例子中，此字段为空） InternalIP：通常是从节点内部可以访问的 IP 地址 ConditionsConditions 描述了节点的状态。Condition的例子有： Node Condition 描述 OutOfDisk 如果节点上的空白磁盘空间不够，不能够再添加新的节点时，该字段为 True，其他情况为 False Ready 如果节点是健康的且已经就绪可以接受新的 Pod。则节点Ready字段为 True。False表明了该节点不健康，不能够接受新的 Pod。 MemoryPressure 如果节点内存紧张，则该字段为 True，否则为False PIDPressure 如果节点上进程过多，则该字段为 True，否则为 False DiskPressure 如果节点磁盘空间紧张，则该字段为 True，否则为 False NetworkUnvailable 如果节点的网络配置有问题，则该字段为 True，否则为 False Capacity and Allocatable（容量和可分配量）容量和可分配量（Capacity and Allocatable）描述了节点上的可用资源的情况： CPU 内存 该节点可调度的最大 pod 数量 Capacity 中的字段表示节点上的资源总数，Allocatable 中的字段表示该节点上可分配给普通 Pod 的资源总数。 Info描述了节点的基本信息，例如： Linux 内核版本 Kubernetes 版本（kubelet 和 kube-proxy 的版本） Docker 版本 操作系统名称 这些信息由节点上的 kubelet 收集。 节点管理与 Pod 和 Service 不一样，节点并不是由 Kubernetes 创建的，节点由云供应商（例如，Google Compute Engine、阿里云等）创建，或者节点已经存在于您的物理机/虚拟机的资源池。向 Kubernetes 中创建节点时，仅仅是创建了一个描述该节点的 API 对象。节点 API 对象创建成功后，Kubernetes将检查该节点是否有效。 节点控制器（Node Controller）节点控制器是一个负责管理节点的 Kubernetes master 组件。在节点的生命周期中，节点控制器起到了许多作用。 节点控制器在注册节点时为节点分配 CIDR 地址块 节点控制器通过云供应商（cloud-controller-manager）接口检查节点列表中每一个节点对象对应的虚拟机是否可用。在云环境中，只要节点状态异常，节点控制器检查其虚拟机在云供应商的状态，如果虚拟机不可用，自动将节点对象从 APIServer 中删除。 节点控制器监控节点的健康状况。当节点变得不可触达时（例如，由于节点已停机，节点控制器不再收到来自节点的心跳信号），节点控制器将节点API对象的 NodeStatus Condition 取值从 NodeReady 更新为 Unknown；然后在等待 pod-eviction-timeout 时间后，将节点上的所有 Pod 从节点驱逐。 默认40秒未收到心跳，修改 NodeStatus Condition 为 Unknown； 默认 pod-eviction-timeout 为 5分钟 节点控制器每隔 --node-monitor-period 秒检查一次节点的状态 节点自注册（Self-Registration）如果 kubelet 的启动参数 --register-node为 true（默认为 true），kubelet 会尝试将自己注册到 API Server。kubelet自行注册时，将使用如下选项： --kubeconfig：向 apiserver 进行认证时所用身份信息的路径 --cloud-provider：向云供应商读取节点自身元数据 --register-node：自动向 API Server 注册节点 --register-with-taints：注册节点时，为节点添加污点（逗号分隔，格式为 =: --node-ip：节点的 IP 地址 --node-labels：注册节点时，为节点添加标签 --node-status-update-frequency：向 master 节点发送心跳信息的时间间隔 如果 Node authorization mode 和 NodeRestriction admission plugin 被启用，kubelet 只拥有创建/修改其自身所对应的节点 API 对象的权限。 手动管理节点如果想要手工创建节点API对象，可以将 kubelet 的启动参数 --register-node 设置为 false。 管理员可以修改节点API对象（不管是否设置了 --register-node 参数）。可以修改的内容有： 增加/减少标签 标记节点为不可调度（unschedulable） 节点的标签与 Pod 上的节点选择器（node selector）配合，可以控制调度方式，例如，限定 Pod 只能在某一组节点上运行。 执行如下命令可将节点标记为不可调度（unschedulable），此时将阻止新的 Pod 被调度到该节点上，但是不影响任何已经在该节点上运行的 Pod。这在准备重启节点之前非常有用。 1kubectl cordon $NODENAME 节点容量（Node Capacity）Kubernetes 调度器在调度 Pod 到节点上时，将确保节点上有足够的资源。具体来说，调度器检查节点上所有容器的资源请求之和不大于节点的容量。此时，只能检查由 kubelet 启动的容器，不包括直接由容器引擎启动的容器，更不包括不在容器里运行的进程","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"伸缩服务和滚动更新","slug":"伸缩服务","date":"2020-03-25T08:43:30.000Z","updated":"2020-03-25T09:09:44.425Z","comments":true,"path":"2020/03/25/伸缩服务/","link":"","permalink":"http://yoursite.com/2020/03/25/%E4%BC%B8%E7%BC%A9%E6%9C%8D%E5%8A%A1/","excerpt":"通过更改部署中的 replicas（副本数）来完成伸缩","text":"通过更改部署中的 replicas（副本数）来完成伸缩 Scaling（伸缩）应用程序 伸缩 的实现可以通过更改 nginx-deployment.yaml 文件中部署的 replicas（副本数）来完成 12spec: replicas: 4 #使用该Deployment创建两个应用程序实例 修改了 Deployment 的 replicas 为 4 后，Kubernetes 又为该 Deployment 创建了 3 新的 Pod，这 4 个 Pod 有相同的标签。因此Service A通过标签选择器与新的 Pod建立了对应关系，将访问流量通过负载均衡在 4 个 Pod 之间进行转发。 将 nginx Deployment 扩容到 4 个副本修改 nginx-deployment.yaml 文件，将 replicas 修改为 4 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 4 #通过更改部署中的 replicas（副本数）来完成扩展 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 执行命令： 1234kubectl apply -f nginx-deployment.yaml#查看结果watch kubectl get pods -o wide 滚动更新用户期望应用程序始终可用，为此开发者/运维者在更新应用程序时要分多次完成。在 Kubernetes 中，这是通过 Rolling Update 滚动更新完成的。Rolling Update滚动更新 通过使用新版本的 Pod 逐步替代旧版本的 Pod 来实现 Deployment 的更新，从而实现零停机。新的 Pod 将在具有可用资源的 Node（节点）上进行调度。 Kubernetes 更新多副本的 Deployment 的版本时，会逐步的创建新版本的 Pod，逐步的停止旧版本的 Pod，以便使应用一直处于可用状态。这个过程中，Service 能够监视 Pod 的状态，将流量始终转发到可用的 Pod 上。 滚动更新步骤1.原本 Service A 将流量负载均衡到 4 个旧版本的 Pod （当中的容器为 绿色）上 1.原本 Service A 将流量负载均衡到 4 个旧版本的 Pod （当中的容器为 绿色）上 2.更新完 Deployment 部署文件中的镜像版本后，master 节点选择了一个 worker 节点，并根据新的镜像版本创建 Pod（紫色容器）。新 Pod 拥有唯一的新的 IP。同时，master 节点选择一个旧版本的 Pod 将其移除。 此时，Service A 将新 Pod 纳入到负载均衡中，将旧Pod移除 同步骤2，再创建一个新的 Pod 替换一个原有的 Pod 如此 Rolling Update 滚动更新，直到所有旧版本 Pod 均移除，新版本 Pod 也达到 Deployment 部署文件中定义的副本数，则滚动更新完成 更新 nginx Deployment 修改 nginx-deployment.yaml 文件123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 4 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.8 #使用镜像nginx:1.8替换原来的nginx:1.7.9 ports: - containerPort: 80 执行命令 1234kubectl apply -f nginx-deployment.yaml#查看过程及结果，可观察到 pod 逐个被替换的过程。watch kubectl get pods -l app=nginx","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"Service服务","slug":"Service服务","date":"2020-03-25T07:09:33.000Z","updated":"2020-03-25T08:29:26.546Z","comments":true,"path":"2020/03/25/Service服务/","link":"","permalink":"http://yoursite.com/2020/03/25/Service%E6%9C%8D%E5%8A%A1/","excerpt":"Kubernetes Service（服务）概述","text":"Kubernetes Service（服务）概述 Service（服务）概述Pod 有自己的生命周期。当 worker node（节点）故障时，节点上运行的 Pod（容器组）也会消失。然后，Deployment 可以通过创建新的 Pod（容器组）来动态地将群集调整回原来的状态，以使应用程序保持运行。 Service的作用：由于 Kubernetes 集群中每个 Pod（容器组）都有一个唯一的 IP 地址（即使是同一个 Node 上的不同 Pod），service 可以解决为前端系统屏蔽后端系统的 Pod（容器组）在销毁、创建过程中所带来的 IP 地址的变化。 Kubernetes 中的 Service（服务） 提供了这样的一个抽象层，它选择具备某些特征的 Pod（容器组）并为它们定义一个访问方式。Service（服务）使 Pod（容器组）之间的相互依赖解耦（原本从一个 Pod 中访问另外一个 Pod，需要知道对方的 IP 地址）。一个 Service（服务）选定哪些 Pod（容器组） 通常由 LabelSelector(标签选择器) 来决定。 在创建Service的时候，通过设置配置文件中的 spec.type 字段的值，可以以不同方式向外部暴露应用程序： ClusterIP（默认） 在群集中的内部IP上公布服务，这种方式的 Service（服务）只在集群内部可以访问到 NodePort 使用 NAT 在集群中每个的同一端口上公布服务。这种方式下，可以通过访问集群中任意节点+端口号的方式访问服务 :。此时 ClusterIP 的访问方式仍然可用。 LoadBalancer 在云环境中（需要云供应商可以支持）创建一个集群外部的负载均衡器，并为使用该负载均衡器的 IP 地址作为服务的访问地址。此时 ClusterIP 和 NodePort 的访问方式仍然可用。 Service是一个抽象层，它通过 LabelSelector 选择了一组 Pod（容器组），把这些 Pod 的指定端口公布到到集群外部，并支持负载均衡和服务发现。 公布 Pod 的端口以使其可访问 在多个 Pod 间实现负载均衡 使用 Label 和 LabelSelector 服务和标签 下图中有两个服务Service A(黄色虚线)和Service B(蓝色虚线) Service A 将请求转发到 IP 为 10.10.10.1 的Pod上， Service B 将请求转发到 IP 为 10.10.10.2、10.10.10.3、10.10.10.4 的Pod上。 Service 将外部请求路由到一组 Pod 中，它提供了一个抽象层，使得 Kubernetes 可以在不影响服务调用者的情况下，动态调度容器组 Service使用 Labels、LabelSelector (标签和选择器 匹配一组 Pod。Labels（标签）是附加到 Kubernetes 对象的键/值对，其用途有多种： 将 Kubernetes 对象（Node、Deployment、Pod、Service等）指派用于开发环境、测试环境或生产环境 嵌入版本标签，使用标签区别不同应用软件版本 使用标签对 Kubernetes 对象进行分类 Labels（标签）和 LabelSelector（标签选择器）之间的关联关系 Deployment B 含有 LabelSelector 为 app=B 通过此方式声明含有 app=B 标签的 Pod 与之关联 通过 Deployment B 创建的 Pod 包含标签为 app=B Service B 通过标签选择器 app=B 选择可以路由的 Pod Labels（标签）可以在创建 Kubernetes 对象时附加上去，也可以在创建之后再附加上去。任何时候都可以修改一个 Kubernetes 对象的 Labels（标签） nginx Deployment 创建一个 Service创建nginx的Deployment中定义了Labels，如下：1234metadata: #译名为元数据，即Deployment的一些基本属性和信息 name: nginx-deployment #Deployment的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组 app: nginx #为该Deployment设置key为app，value为nginx的标签 创建文件 nginx-service.yaml12345678910111213141516apiVersion: v1kind: Servicemetadata: name: nginx-service #Service 的名称 labels: #Service 自己的标签 app: nginx #为该 Service 设置 key 为 app，value 为 nginx 的标签spec: #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问 selector: #标签选择器 app: nginx #选择包含标签 app:nginx 的 Pod ports: - name: nginx-port #端口的名字 protocol: TCP #协议类型 TCP/UDP port: 80 #集群内的其他容器组可通过 80 端口访问 Service nodePort: 32600 #通过任意节点的 32600 端口访问 Service targetPort: 80 #将请求转发到匹配 Pod 的 80 端口 type: NodePort #Serive的类型，ClusterIP/NodePort/LoaderBalancer 执行命令 1234567kubectl apply -f nginx-service.yaml#检查执行结果kubectl get services -o wide #可查看到名称为 nginx-service 的服务。#访问服务curl &lt;任意节点的 IP&gt;:32600","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"查看Pods/Nodes","slug":"查看Pods-Nodes","date":"2020-03-25T06:05:21.000Z","updated":"2020-03-25T06:21:04.946Z","comments":true,"path":"2020/03/25/查看Pods-Nodes/","link":"","permalink":"http://yoursite.com/2020/03/25/%E6%9F%A5%E7%9C%8BPods-Nodes/","excerpt":"了解 Pod 和 Node","text":"了解 Pod 和 Node Pod​ Pod中的容器共享 IP 地址和端口空间（同一 Pod 中的不同 container 端口不能相互冲突），始终位于同一位置并共同调度，并在同一节点上的共享上下文中运行。（同一个Pod内的容器可以localhost + 端口互相访问）。 ​ 当在 k8s 上创建 Deployment 时，会在集群上创建包含容器的 Pod (而不是直接创建容器)。每个Pod都与运行它的 worker 节点（Node）绑定，并保持在那里直到终止或被删除。如果节点（Node）发生故障，则会在群集中的其他可用节点（Node）上运行相同的 Pod（从同样的镜像创建 Container，使用同样的配置，IP 地址不同，Pod 名字不同）。 Pod（容器组）是 k8s 集群上的最基本的单元。 Pod 是一组容器（可包含一个或多个应用程序容器），以及共享存储（卷 Volumes）、IP 地址和有关如何运行容器的信息。 如果多个容器紧密耦合并且需要共享磁盘等资源，则应该被部署在同一个Pod（容器组）中 Node Pod（容器组）总是在 Node（节点） 上运行。 Node（节点）是 kubernetes 集群中的计算机，可以是虚拟机或物理机。 每个 Node（节点）都由 master 管理。 一个 Node（节点）可以有多个Pod（容器组），kubernetes master 会根据每个 Node（节点）上可用资源的情况，自动调度 Pod（容器组）到最佳的 Node（节点）上。 每个Node（节点）至少运行： Kubelet，负责 master 节点和 worker 节点之间通信的进程；管理 Pod（容器组）和 Pod（容器组）内运行的 Container（容器）。 容器运行环境（如Docker）负责下载镜像、创建和运行容器等。 相关命令操作kubectl get - 显示资源列表1234567891011121314151617# kubectl get 资源类型#获取类型为Deployment的资源列表kubectl get deployments#获取类型为Pod的资源列表kubectl get pods#获取类型为Node的资源列表kubectl get nodes# 查看所有名称空间的 Deploymentkubectl get deployments -Akubectl get deployments --all-namespaces# 查看 kube-system 名称空间的 Deploymentkubectl get deployments -n kube-system kubectl describe - 显示有关资源的详细信息1234567# kubectl describe 资源类型 资源名称#查看名称为nginx-XXXXXX的Pod的信息kubectl describe pod nginx-XXXXXX #查看名称为nginx的Deployment的信息kubectl describe deployment nginx kubectl logs - 查看pod中的容器的打印日志（和命令docker logs 类似）12# kubectl logs Pod名称kubectl logs -f nginx-pod-XXXXXXX kubectl exec - 在pod中的容器环境内执行命令(和命令docker exec 类似)1234# kubectl exec Pod名称 操作命令# 在名称为nginx-pod-xxxxxx的Pod中运行bashkubectl exec -it nginx-pod-xxxxxx /bin/bash","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"k8s部署一个应用程序","slug":"k8s部署一个应用程序","date":"2020-03-25T05:49:00.000Z","updated":"2020-03-25T05:59:21.447Z","comments":true,"path":"2020/03/25/k8s部署一个应用程序/","link":"","permalink":"http://yoursite.com/2020/03/25/k8s%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"使用 kubectl 在 k8s 上部署第一个应用程序。","text":"使用 kubectl 在 k8s 上部署第一个应用程序。 Deployment概念​ 通过发布 Deployment，可以创建应用程序 (docker image) 的实例 (docker container)，这个实例会被包含在称为 Pod 的概念中，Pod 是 k8s 中最小可管理单元。 ​ 在 k8s 集群中发布 Deployment 后，Deployment 将指示 k8s 如何创建和更新应用程序的实例，master 节点将应用程序实例调度到集群中的具体的节点上。 ​ 创建应用程序实例后，Kubernetes Deployment Controller 会持续监控这些实例。如果运行实例的 worker 节点关机或被删除，则 Kubernetes Deployment Controller 将在群集中资源最优的另一个 worker 节点上重新创建一个新的实例。这提供了一种自我修复机制来解决机器故障或维护问题。 ​ 通过创建应用程序实例并确保它们在集群节点中的运行实例个数，Kubernetes Deployment 提供了一种完全不同的方式来管理应用程序。 Deployment 处于 master 节点上，通过发布 Deployment，master 节点会选择合适的 worker 节点创建 Container（即图中的正方体），Container 会被包含在 Pod （即蓝色圆圈）里。 部署 nginx Deployment创建文件 nginx-deployment.yaml12345678910111213141516171819apiVersion: apps/v1 #与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本kind: Deployment #该配置的类型，我们使用的是 Deploymentmetadata: #译名为元数据，即 Deployment 的一些基本属性和信息 name: nginx-deployment #Deployment 的名称 labels: #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解 app: nginx #为该Deployment设置key为app，value为nginx的标签spec: #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用 replicas: 1 #使用该Deployment创建一个应用程序实例 selector: #标签选择器，与上面的标签共同作用，目前不需要理解 matchLabels: #选择包含标签app:nginx的资源 app: nginx template: #这是选择或创建的Pod的模板 metadata: #Pod的元数据 labels: #Pod的标签，上面的selector即选择包含标签app:nginx的Pod app: nginx spec: #期望Pod实现的功能（即在pod中部署） containers: #生成container，与docker中的container是同一种 - name: nginx #container的名称 image: nginx:1.7.9 #使用镜像nginx:1.7.9创建container，该container默认80端口可访问 应用 YAML 文件1kubectl apply -f nginx-deployment.yaml 查看部署结果12345# 查看 Deploymentkubectl get deployments# 查看 Podkubectl get pods","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"kubectl 命令技巧大全","slug":"kubectl 命令技巧大全","date":"2020-03-20T03:13:20.000Z","updated":"2020-03-20T07:20:09.424Z","comments":true,"path":"2020/03/20/kubectl 命令技巧大全/","link":"","permalink":"http://yoursite.com/2020/03/20/kubectl%20%E5%91%BD%E4%BB%A4%E6%8A%80%E5%B7%A7%E5%A4%A7%E5%85%A8/","excerpt":"一些基本的kubernets操作命令","text":"一些基本的kubernets操作命令 kubectl 命令技巧大全Kubectl 自动补全12345yum install -y bash-completionsource &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completionsource &lt;(kubectl completion bash) 创建对象Kubernetes 的清单文件可以使用 json 或 yaml 格式定义。可以以 .yaml、.yml、或者 .json 为扩展名。 1234567891011$ kubectl create -f .&#x2F;my-manifest.yaml # 创建资源$ kubectl create -f .&#x2F;my1.yaml -f .&#x2F;my2.yaml # 使用多个文件创建资源$ kubectl create -f .&#x2F;dir # 使用目录下的所有清单文件来创建资源$ kubectl create -f https:&#x2F;&#x2F;git.io&#x2F;vPieo # 使用 url 来创建资源$ kubectl run nginx --image&#x3D;nginx # 启动一个 nginx 实例$ kubectl explain pods,svc # 获取 pod 和 svc 的文档 显示和查找资源 列出所有 namespace 中的所有service 1$ kubectl get services 列出所有 namespace 中的所有 pod 1$ kubectl get pods --all-namespaces 列出所有 pod 并显示详细信息 1$ kubectl get pods -o wide 列出指定 deployment 1$ kubectl get deployment my-dep 列出该 namespace 中的所有 pod 包括未初始化的 1$ kubectl get pods --include-uninitialized 使用详细输出来描述命令 12345 $ kubectl describe nodes my-node $ kubectl describe pods my-pod $ kubectl get services --sort-by&#x3D;.metadata.name 根据重启次数排序列出 pod 1$ kubectl get pods --sort-by&#x3D;&#39;.status.containerStatuses[0].restartCount&#39; 获取所有具有 app=cassandra 的 pod 中的 version 标签 1$ kubectl get pods --selector&#x3D;app&#x3D;cassandra rc -o \\ jsonpath&#x3D;&#39;&#123;.items[*].metadata.labels.version&#125;&#39; 获取所有节点的 ExternalIP 1$ kubectl get nodes -o jsonpath&#x3D;&#39;&#123;.items[*].status.addresses[?(@.type&#x3D;&#x3D;&quot;ExternalIP&quot;)].address&#125;&#39; 列出属于某个 PC 的 Pod 的名字，“jq”命令用于转换复杂的 jsonpath，参考 https://stedolan.github.io/jq/ 123 $ sel&#x3D;$&#123;$(kubectl get rc my-rc --output&#x3D;json | jq -j &#39;.spec.selector | to_entries | .[] | &quot;\\(.key)&#x3D;\\(.value),&quot;&#39;)%?&#125; $ echo $(kubectl get pods --selector&#x3D;$sel --output&#x3D;jsonpath&#x3D;&#123;.items..metadata.name&#125;) 查看哪些节点已就绪 1$ JSONPATH&#x3D;&#39;&#123;range .items[*]&#125;&#123;@.metadata.name&#125;:&#123;range @.status.conditions[*]&#125;&#123;@.type&#125;&#x3D;&#123;@.status&#125;;&#123;end&#125;&#123;end&#125;&#39; \\ &amp;&amp; kubectl get nodes -o jsonpath&#x3D;&quot;$JSONPATH&quot; | grep &quot;Ready&#x3D;True&quot; 列出当前 Pod 中使用的 Secret 1$ kubectl get pods -o json | jq &#39;.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name&#39; | grep -v null | sort | uniq 更新资源滚动更新 pod frontend-v1 1$ kubectl rolling-update frontend-v1 -f frontend-v2.json 更新资源名称并更新镜像 1$ kubectl rolling-update frontend-v1 frontend-v2 --image&#x3D;image:v2 更新 frontend pod 中的镜像 1$ kubectl rolling-update frontend --image&#x3D;image:v2 退出已存在的进行中的滚动更新 1$ kubectl rolling-update frontend-v1 frontend-v2 --rollback 基于 stdin 输入的 JSON 替换 pod 1$ cat pod.json | kubectl replace -f - 强制替换，删除后重新创建资源。会导致服务中断。 1$ kubectl replace --force -f .&#x2F;pod.json 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口 1$ kubectl expose rc nginx --port&#x3D;80 --target-port&#x3D;8000 更新单容器 pod 的镜像版本（tag）到 v4 1$ kubectl get pod mypod -o yaml | sed &#39;s&#x2F;\\(image: myimage\\):.*$&#x2F;\\1:v4&#x2F;&#39; | kubectl replace -f - 添加标签 1$ kubectl label pods my-pod new-label&#x3D;awesome 添加注解 1$ kubectl annotate pods my-pod icon-url&#x3D;http:&#x2F;&#x2F;goo.gl&#x2F;XXBTWq 自动扩展 deployment “foo” 1$ kubectl autoscale deployment foo --min&#x3D;2 --max&#x3D;10 删除资源# 删除 pod.json 文件中定义的类型和名称的 pod 1$ kubectl delete -f .&#x2F;pod.json 删除名为“baz”的 pod 和名为“foo”的 service 1$ kubectl delete pod,service baz foo 删除具有 name=myLabel 标签的 pod 和 serivce 1$ kubectl delete pods,services -l name&#x3D;myLabel 删除具有 name=myLabel 标签的 pod 和 service，包括尚未初始化的 1$ kubectl delete pods,services -l name&#x3D;myLabel --include-uninitialized 删除 my-ns namespace 下的所有 pod 和 serivce，包括尚未初始化的 1$ kubectl -n my-ns delete po,svc --all 与运行中的Pod交互# dump 输出 pod 的日志（stdout） 1$ kubectl logs my-pod dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） 1$ kubectl logs my-pod -c my-container 流式输出 pod 的日志（stdout） 1$ kubectl logs -f my-pod 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用） 1$ kubectl logs -f my-pod -c my-container 交互式 shell 的方式运行 pod 1$ kubectl run -i --tty busybox --image&#x3D;busybox -- sh 连接到运行中的容器 1$ kubectl attach my-pod -i 转发 pod 中的 6000 端口到本地的 5000 端口 1$ kubectl port-forward my-pod 5000:6000 在已存在的容器中执行命令（只有一个容器的情况下） 1$ kubectl exec my-pod -- ls &#x2F; 在已存在的容器中执行命令（pod 中有多个容器的情况下） 1$ kubectl exec my-pod -c my-container -- ls &#x2F; 显示指定 pod 和容器的指标度量 1$ kubectl top pod POD_NAME --containers 与节点和集群交互# 标记 my-node 不可调度 1$ kubectl cordon my-node 清空 my-node 以待维护 1$ kubectl drain my-node 标记 my-node 可调度 1$ kubectl uncordon my-node 显示 my-node 的指标度量 1$ kubectl top node my-node 显示 master 和服务的地址 1$ kubectl cluster-info 将当前集群状态输出到 stdout 1$ kubectl cluster-info dump 将当前集群状态输出到 /path/to/cluster-state 1$ kubectl cluster-info dump --output-directory&#x3D;&#x2F;path&#x2F;to&#x2F;cluster-state 如果该键和影响的污点（taint）已存在，则使用指定的值替换 1$ kubectl taint nodes foo dedicated&#x3D;special-user:NoSchedule 资源类型 缩写别名 clusters componentstatuses cs configmaps cm daemonsets ds deployments deploy endpoints ep event ev horizontalpodautoscalers hpa ingresses ing jobs limitranges limits namespaces ns networkpolicies nodes no statefulsets persistentvolumeclaims pvc persistentvolumes pv pods po podsecuritypolicies psp podtemplates replicasets rs replicationcontrollers rc resourcequotas quota cronjob secrets serviceaccount sa services svc storageclasses kubectl get - 显示资源列表#获取类型为Deployment的资源列表 1kubectl get deployments #获取类型为Pod的资源列表 1kubectl get pods #获取类型为Node的资源列表 1kubectl get nodes 名称空间在命令后增加 -A 或 –all-namespaces 可查看所有名称空间中的对象，使用参数 -n 可查看指定名称空间的对象，例如 # 查看所有名称空间的 Deployment 12kubectl get deployments -A kubectl get deployments --all-namespaces 查看 kube-system 名称空间的 Deployment 1kubectl get deployments -n kube-system 检查 kubectl 是否知道集群地址及凭证 1$ kubectl config view 通过 kubectl cluster-info 命令获得这些服务列表： 12345[root@ebs ~]# kubectl cluster-info Kubernetes master is running at https:&#x2F;&#x2F;172.16.121.88:6443 KubeDNS is running at https:&#x2F;&#x2F;172.16.121.88:6443&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"Kubernetes核心概念","slug":"Kubernetes核心概念","date":"2020-03-20T03:10:20.000Z","updated":"2020-03-25T09:26:08.403Z","comments":true,"path":"2020/03/20/Kubernetes核心概念/","link":"","permalink":"http://yoursite.com/2020/03/20/Kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","excerpt":"Cluster，Pod，Label， Replication Controller ，Service …","text":"Cluster，Pod，Label， Replication Controller ，Service … 什么是Kubernetes？Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。如果你曾经用过Docker容器技术部署容器，那么可以将Docker看成Kubernetes内部使用的低级别组件。Kubernetes不仅仅支持Docker，还支持Rocket，这是另一种容器技术。 使用Kubernetes可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它，等等… 集群集群是一组节点，这些节点可以是物理服务器或者虚拟机，之上安装了Kubernetes平台。下图展示这样的集群。注意该图为了强调核心概念有所简化。这里可以看到一个典型的Kubernetes架构图。 上图可以看到如下组件，使用特别的图标表示Service和Label： PodContainer（容器） Labe （标签） Replication Controller（复制控制器） Service（服务） Node（节点） Kubernetes Master（Kubernetes主节点） PodPod（上图绿色方框）安排在节点上，包含一组容器和卷。同一个Pod里的容器共享同一个网络命名空间，可以使用localhost互相通信。Pod是短暂的，不是持续性实体。你可能会有这些问题： 如果Pod是短暂的，那么我怎么才能持久化容器数据使其能够跨重启而存在呢？ 是的，Kubernetes支持 卷 的概念，因此可以使用持久化的卷类型。 是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么？可以手动创建单个Pod，但是也可以使用Replication Controller使用Pod模板创建出多份拷贝，下文会详细介绍。 如果Pod是短暂的，那么重启时IP地址可能会改变，那么怎么才能从前端容器正确可靠地指向后台容器呢？这时可以使用Service，下文会详细介绍。 Label正如图所示，一些Pod有Label 。一个Label是attach到Pod的一对键/值对，用来传递用户定义的属性。比如，你可能创建了一个”tier”和“app”标签，通过Label（tier=frontend, app=myapp）来标记前端Pod容器，使用Label（tier=backend, app=myapp）标记后台Pod。然后可以使用 [Selectors] 选择带有特定Label的Pod，并且将Service或者Replication Controller应用到上面。 Replication Controller是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么，能否将Pods划到逻辑组里？ Replication Controller确保任意时间都有指定数量的Pod“副本”在运行。如果为某个Pod创建了Replication Controller并且指定3个副本，它会创建3个Pod，并且持续监控它们。如果某个Pod不响应，那么Replication Controller会替换它，保持总数为3.如下面的动画所示： 如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动 [升级] 时很有用。 当创建Replication Controller时，需要指定两个东西： Pod模板：用来创建Pod副本的模板 Label：Replication Controller需要监控的Pod的标签。现在已经创建了Pod的一些副本，那么在这些副本上如何均衡负载呢？我们需要的是Service。 TIP 最新 Kubernetes 版本里，推荐使用 Deployment Service如果Pods是短暂的，那么重启时IP地址可能会改变，怎么才能从前端容器正确可靠地指向后台容器呢？ [Service] 抽象 现在，假定有2个后台Pod，并且定义后台Service的名称为‘backend-service’，label选择器为(tier=backend, app=myapp) 的Service会完成如下两件重要的事情： 会为Service创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。 现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod的哪一个呢？Service在这2个后台Pod之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个Node上运行的代理（kube-proxy）完成。 下述动画展示了Service的功能。注意该图作了很多简化。如果不进入网络配置，那么达到透明的负载均衡目标所涉及的底层网络和路由相对先进。如果有兴趣，有更深入的介绍。 每个节点都运行如下Kubernetes关键组件： Kubelet：是主节点代理。 Kube-proxy：Service使用其将链接路由到Pod，如上文所述。 Docker或Rocket：Kubernetes使用的容器技术来创建容器。 Kubernetes Master集群拥有一个Kubernetes Master（紫色方框）。Kubernetes Master提供集群的独特视角，并且拥有一系列组件，比如Kubernetes API Server。API Server提供可以用来和集群交互的REST端点。master节点包括用来创建和复制Pod的Replication Controller。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/categories/kubernetes/"}],"tags":[]},{"title":"hexo 使用","slug":"hexo使用","date":"2020-03-19T09:18:36.000Z","updated":"2020-03-23T07:14:57.409Z","comments":true,"path":"2020/03/19/hexo使用/","link":"","permalink":"http://yoursite.com/2020/03/19/hexo%E4%BD%BF%E7%94%A8/","excerpt":"hexo 搭建、部署、操作；","text":"hexo 搭建、部署、操作； _config.yml配置git 1234deploy: type: &#39;git&#39; repo: git@github.com:yourname&#x2F;yourname.github.io.git branch: master 上传github，推送文件步骤：hexo clean hexo c 清除缓存文件 (db.json) 和已生成的静态文件 (public) hexo generate hexo g 生成静态文件 hexo deploy hexo d 部署网站 执行端口启动： 1hexo s -i 0.0.0.0 -p 8080 绑定个人域名： 解析域名注意，博客网址中必须使用你github的用户名. 布局（Layout） Hexo 有三种默认布局：post、page 和 draft。在创建者三种不同类型的文件时，它们将会被保存到不同的路径；而您自定义的其他布局和 post 相同，都将储存到 source/_posts 文件夹。 布局 路径 post source/_posts page source draft source/_drafts hexo init 1hexo init [ folder] 新建一个网站。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站。 hexo new 1hexo new [layout] &lt;title&gt; 新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。 hexo new “post title with whitespace” 参数 描述 -p, –path 自定义新文章的路径 -r, –replace 如果存在同名文章，将其替换 -s, –slug 文章的 Slug，作为新文章的文件名和发布后的 URL 默认情况下，Hexo 会使用文章的标题来决定文章文件的路径。对于独立页面来说，Hexo 会创建一个以标题为名字的目录，并在目录中放置一个 index.md 文件。你可以使用 –path 参数来覆盖上述行为、自行决定文件的目录： 1hexo new page --path about&#x2F;me &quot;About me&quot; 以上命令会创建一个 source/about/me.md 文件，同时 Front Matter 中的 title 为 “About me” 注意！title 是必须指定的！如果你这么做并不能达到你的目的： 1hexo new page --path about&#x2F;me 此时 Hexo 会创建 source/_posts/about/me.md，同时 me.md 的 Front Matter 中的 title 为 “page”。这是因为在上述命令中，hexo-cli 将 page 视为指定文章的标题、并采用默认的 layout。","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]}]}